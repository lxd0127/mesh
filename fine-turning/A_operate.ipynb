{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6992401-380f-438a-928c-1e9b9e8956ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///home/featurize/work/LLaMA-Factory-4-2\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.48.3,>=4.41.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b6/1a/efeecb8d83705f2f4beac98d46f2148c95ecd7babfb31b5c0f1e7017e83d/transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets<=3.2.0,>=2.16.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d7/84/0df6c5981f5fc722381662ff8cfbdf8aad64bec875f75d80b55bfef394ce/datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate<=1.2.1,>=0.34.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c2/60/a585c806d6c0ec5f8149d44eb202714792802f484e6e2b1bf96b23bd2b00/accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting peft<=0.12.0,>=0.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/62/72/fcabddf222ec938c3cbd5616e5a72796938b5235897e07a1fcc2a8e7735e/peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trl<=0.9.6,>=0.8.6\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a5/c3/6565c2c376a829f99da20d39c2912405195ec1fa6aae068dc45c46793e72/trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<=0.21.0,>=0.19.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/22/06/69d7ce374747edaf1695a4f61b83570d91cc8bbfc51ccfecf76f56ab4aac/tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gradio>=4.38.0 in /environment/miniconda3/lib/python3.11/site-packages (5.26.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /environment/miniconda3/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in /environment/miniconda3/lib/python3.11/site-packages (1.15.2)\n",
      "Collecting einops\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/87/62/9773de14fe6c45c23649e98b83231fffd7b9892b6cf863251dc2afa73643/einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /environment/miniconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Collecting tiktoken\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b1/73/41591c525680cd460a6becf56c9b17468d3711b1df242c53d2c7b2183d16/tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /environment/miniconda3/lib/python3.11/site-packages (4.25.3)\n",
      "Requirement already satisfied: uvicorn in /environment/miniconda3/lib/python3.11/site-packages (0.34.2)\n",
      "Requirement already satisfied: pydantic in /environment/miniconda3/lib/python3.11/site-packages (2.11.4)\n",
      "Requirement already satisfied: fastapi in /environment/miniconda3/lib/python3.11/site-packages (0.115.12)\n",
      "Collecting sse-starlette\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/43/a4/ee4a20f0b5ff34c391f3685eff7cdba1178a487766e31b04efb51bbddd87/sse_starlette-2.3.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /environment/miniconda3/lib/python3.11/site-packages (3.9.0)\n",
      "Collecting fire\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6b/b6/82c7e601d6d3c3278c40b7bd35e17e82aa227f050aa9f66cb7b7fce29471/fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /environment/miniconda3/lib/python3.11/site-packages (25.0)\n",
      "Requirement already satisfied: pyyaml in /environment/miniconda3/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /environment/miniconda3/lib/python3.11/site-packages (1.26.4)\n",
      "Collecting av\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2b/6a/b88bfb2cd832a410690d97c3ba917e4d01782ca635675ca5a93854530e6c/av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting librosa\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b5/ba/c63c5786dfee4c3417094c4b00966e61e4a63efecee22cb7b4c0387dda83/librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tyro<0.9.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/60/ec/e34d546cfd9c5b906d1d534bb75557be9f2b179609d60bb9e97ec07e8ead/tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.13.1 in /environment/miniconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Collecting nltk\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jieba\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge-chinese\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/03/0f/394cf877be7b903881020ef7217f7dc644dad158d52a9353fcab22e3464d/rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: psutil in /environment/miniconda3/lib/python3.11/site-packages (from accelerate<=1.2.1,>=0.34.0) (5.9.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /environment/miniconda3/lib/python3.11/site-packages (from accelerate<=1.2.1,>=0.34.0) (0.31.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /environment/miniconda3/lib/python3.11/site-packages (from accelerate<=1.2.1,>=0.34.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /environment/miniconda3/lib/python3.11/site-packages (from datasets<=3.2.0,>=2.16.0) (3.18.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets<=3.2.0,>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/46/1f/7f02009bc7fc8955c391defee5348f510e589a020e4b40ca05edcb847854/pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0 (from datasets<=3.2.0,>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.32.2 in /environment/miniconda3/lib/python3.11/site-packages (from datasets<=3.2.0,>=2.16.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /environment/miniconda3/lib/python3.11/site-packages (from datasets<=3.2.0,>=2.16.0) (4.67.1)\n",
      "Collecting xxhash (from datasets<=3.2.0,>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d9/72/9256303f10e41ab004799a4aa74b80b3c5977d6383ae4550548b24bd1971/xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.17 (from datasets<=3.2.0,>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/50/15/b56e50e8debaf439f44befec5b2af11db85f6e0f344c3113ae0be0593a91/multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.2.0,>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1d/a0/6aaea0c2fbea2f89bfd5db25fb1e3481896a423002ebe4e55288907a97a3/fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /environment/miniconda3/lib/python3.11/site-packages (from datasets<=3.2.0,>=2.16.0) (3.7.4)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (4.9.0)\n",
      "Requirement already satisfied: ffmpy in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.9.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (1.9.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (3.10.18)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (11.2.1)\n",
      "Requirement already satisfied: pydub in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (0.11.8)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (0.15.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio>=4.38.0) (4.13.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /environment/miniconda3/lib/python3.11/site-packages (from gradio-client==1.9.0->gradio>=4.38.0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /environment/miniconda3/lib/python3.11/site-packages (from pandas>=2.0.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /environment/miniconda3/lib/python3.11/site-packages (from pandas>=2.0.0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /environment/miniconda3/lib/python3.11/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /environment/miniconda3/lib/python3.11/site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /environment/miniconda3/lib/python3.11/site-packages (from pydantic) (0.4.0)\n",
      "Requirement already satisfied: sympy in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=1.13.1) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /environment/miniconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1) (12.4.127)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /environment/miniconda3/lib/python3.11/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.48.3,>=4.41.2) (2024.11.6)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from tyro<0.9.0) (14.0.0)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/74/03/3271b7bb470fbab4adf5bd30b0d32143909d96f3608d815b447357f47f2b/shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: click>=7.0 in /environment/miniconda3/lib/python3.11/site-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /environment/miniconda3/lib/python3.11/site-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: termcolor in /environment/miniconda3/lib/python3.11/site-packages (from fire) (2.4.0)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/57/8d/30aa32745af16af0a9a650115fbe81bde7c610ed5c21b381fca0196f3a7f/audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/97/c8/8740616c8436c86c1b9a62e72cb891177d2c34c2d24ddcde4c390371bf4c/numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=1.1.0 (from librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a8/f3/62fc9a5a659bb58a03cdd7e258956a5824bdc9b4bb3c5d932f55880be569/scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.0 (from librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/da/d3/13ee227a148af1c693654932b8b0b02ed64af5e1f7406d56b088b57574cd/joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /environment/miniconda3/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/57/5e/70bdd9579b35003a489fc850b5047beeda26328053ebadc1fb60f320f7db/soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pooch>=1.1 (from librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a8/87/77cc11c7a9ea9fd05503def69e3d18605852cd0d4b0d3b8f15bbeb3ef1d1/pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9f/e5/400e3bf7f29971abad85cb877e290060e5ec61fccd2fa319e3d85709c1be/soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.5/252.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/83/60/d497a310bde3f01cb805196ac61b7ad6dc5dcf8dce66634dc34364b20b4f/lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a8/a1/ad7b84b91ab5a324e707f4c9761633e357820b011a01e34ce658c1dda7cc/msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m403.7/403.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /environment/miniconda3/lib/python3.11/site-packages (from rouge-chinese) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.8 in /environment/miniconda3/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio>=4.38.0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /environment/miniconda3/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio>=4.38.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (23.2.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0) (1.9.4)\n",
      "Requirement already satisfied: certifi in /environment/miniconda3/lib/python3.11/site-packages (from httpx>=0.24.1->gradio>=4.38.0) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /environment/miniconda3/lib/python3.11/site-packages (from httpx>=0.24.1->gradio>=4.38.0) (1.0.9)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate<=1.2.1,>=0.34.0) (1.1.0)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/99/fe/d030f1849ebb1f394bb3f7adad5e729b634fb100515594aca25c354ffc62/llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=2.5.0 in /environment/miniconda3/lib/python3.11/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0) (2.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /environment/miniconda3/lib/python3.11/site-packages (from rich>=11.1.0->tyro<0.9.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /environment/miniconda3/lib/python3.11/site-packages (from rich>=11.1.0->tyro<0.9.0) (2.19.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /environment/miniconda3/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /environment/miniconda3/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio>=4.38.0) (1.5.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /environment/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /environment/miniconda3/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: mdurl~=0.1 in /environment/miniconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: fire, jieba, llamafactory\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114247 sha256=f11461d7a14ea9e174856f273949fad7cc888074642b8d66ab28e22bc5500b8c\n",
      "  Stored in directory: /home/featurize/.cache/pip/wheels/ca/d7/59/bded1534a72d6b702c835f1f7556c73277850f60578f462984\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=4519e10e31d81e8f8bec1ef661f13502f026620916b0562fce9a8d50622ab9de\n",
      "  Stored in directory: /home/featurize/.cache/pip/wheels/37/08/79/ea7c0d2ca823affa13f89586a5a9eff8dd6ad589640396e1b5\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=25488 sha256=c4a80b8b7bc322a7e175c38c2f0f8a539e016a7d03306f75a4f013c91d6cefff\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dsx5urlf/wheels/be/6b/70/1e08af3a0def55e0b4df5b789a391f1b896bfc0904982490fd\n",
      "Successfully built fire jieba llamafactory\n",
      "Installing collected packages: jieba, xxhash, threadpoolctl, soxr, shtab, rouge-chinese, pyarrow, msgpack, llvmlite, lazy_loader, joblib, fsspec, fire, einops, docstring-parser, dill, av, audioread, tiktoken, soundfile, scikit-learn, pooch, numba, nltk, multiprocess, tyro, tokenizers, sse-starlette, librosa, transformers, datasets, accelerate, trl, peft, llamafactory\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.6.0\n",
      "    Uninstalling accelerate-1.6.0:\n",
      "      Successfully uninstalled accelerate-1.6.0\n",
      "Successfully installed accelerate-1.2.1 audioread-3.0.1 av-14.3.0 datasets-3.2.0 dill-0.3.8 docstring-parser-0.16 einops-0.8.1 fire-0.7.0 fsspec-2024.9.0 jieba-0.42.1 joblib-1.5.0 lazy_loader-0.4 librosa-0.11.0 llamafactory-0.9.2.dev0 llvmlite-0.44.0 msgpack-1.1.0 multiprocess-0.70.16 nltk-3.9.1 numba-0.61.2 peft-0.12.0 pooch-1.8.2 pyarrow-20.0.0 rouge-chinese-1.0.3 scikit-learn-1.6.1 shtab-1.7.2 soundfile-0.13.1 soxr-0.5.0.post1 sse-starlette-2.3.4 threadpoolctl-3.6.0 tiktoken-0.9.0 tokenizers-0.21.0 transformers-4.48.3 trl-0.9.6 tyro-0.8.14 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36bb4c7-63ca-4958-96c7-35b917cda53b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: socksio in /environment/miniconda3/lib/python3.11/site-packages (1.0.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tf-keras\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/45/6b/d245122d108a94df5969ee7408ad343af1627730e91478e01ef098976bfa/tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow<2.20,>=2.19 (from tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ba/1c/370b5546cf7afc29649b2fb74c171ef2493a36f62cf901c1425ead4a56af/tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.62.2)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5d/12/4f70e8e2ba0dbe72ea978429d8530b0333f0ed2140cc571a48802878ef99/tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras>=3.5.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3f/0a/678ebcf4b6dad6ad63dfc2445d190f79a97fa7bc7150f57a6c505459e2bc/keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.2.0,>=1.26.0 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.11.0)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/cc/2a/5421fd3dbe6eef9b844cc9d05f568b9fb568503a2e51cb1eb4443d9fc56b/ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /environment/miniconda3/lib/python3.11/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.36.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /environment/miniconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.41.2)\n",
      "Requirement already satisfied: rich in /environment/miniconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in /environment/miniconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /environment/miniconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /environment/miniconda3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /environment/miniconda3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /environment/miniconda3/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /environment/miniconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /environment/miniconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /environment/miniconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /environment/miniconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Installing collected packages: ml-dtypes, tensorboard, keras, tensorflow, tf-keras\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.3.2\n",
      "    Uninstalling ml-dtypes-0.3.2:\n",
      "      Successfully uninstalled ml-dtypes-0.3.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.3.2\n",
      "    Uninstalling keras-3.3.2:\n",
      "      Successfully uninstalled keras-3.3.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.16.1\n",
      "    Uninstalling tensorflow-2.16.1:\n",
      "      Successfully uninstalled tensorflow-2.16.1\n",
      "Successfully installed keras-3.9.2 ml-dtypes-0.5.1 tensorboard-2.19.0 tensorflow-2.19.0 tf-keras-2.19.0\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting unsloth\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6f/c7/1e762675727596707d327108c8931e94580f9752296da8e7e61fa4f671c2/unsloth-2025.4.7-py3-none-any.whl (218 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.5/218.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo>=2025.4.4 (from unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/90/f1/701393f74a1c9593097123be16544e2df2b464cc058f82880906090d5de2/unsloth_zoo-2025.4.4-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch>=2.4.0 (from unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0e/6b/87fcddd34df9f53880fa1f0c23af7b6b96c935856473faf3914323588c40/torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1e/b3/9a850d949093b15ff283acae58c4f5adaf8776c57386b688c7f241f4dfbf/xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: bitsandbytes in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (0.45.5)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3c/c5/4874a81131cc9e934d88377fbc9d24319ae1fb540f3333b4e9c696ebc607/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (25.0)\n",
      "Requirement already satisfied: tyro in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (0.8.14)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (4.48.3)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (3.2.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (5.9.8)\n",
      "Collecting wheel>=0.42.0 (from unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (1.2.1)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (0.9.6)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (0.12.0)\n",
      "Collecting protobuf<4.0.0 (from unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8d/14/619e24a4c70df2901e1f4dbc50a6291eb63a759172558df326347dce1f0d/protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (0.31.1)\n",
      "Collecting hf_transfer (from unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/d8/f87ea6f42456254b48915970ed98e993110521e9263472840174d32c880d/hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting diffusers (from unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e7/7a/f08f610cea8a3395ad3b4f586db23bedb43c68db6c3261145a15e7b63126/diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /environment/miniconda3/lib/python3.11/site-packages (from unsloth) (0.17.2)\n",
      "Requirement already satisfied: pyyaml in /environment/miniconda3/lib/python3.11/site-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /environment/miniconda3/lib/python3.11/site-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
      "Requirement already satisfied: filelock in /environment/miniconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /environment/miniconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /environment/miniconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /environment/miniconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /environment/miniconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /environment/miniconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /environment/miniconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /environment/miniconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth) (3.7.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub->unsloth) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub->unsloth) (1.1.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a2/09/77d55d46fd61b4a135c444fc97158ef34a095e5681d0a6c10b75bf356191/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /environment/miniconda3/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (3.3)\n",
      "Requirement already satisfied: jinja2 in /environment/miniconda3/lib/python3.11/site-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/75/2e/46030320b5a80661e88039f59060d1790298b4718944a65a7f2aeda3d9e9/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e1/23/e717c5ac26d26cf39a27fbc076240fad2e3b817e5889d671b67f4f9f49c5/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/49/60/7b6497946d74bcf1de852a21824d63baad12cd417db4195fc1bfe59db953/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2a/78/4535c9c7f859a64781e43c969a3a7e84c54634e319a996d43ef32ce46f83/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.6.4.1 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/af/eb/ff4b8c503fa1f1796679dce648854d58751982426e4e4b37d6fce49d259c/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8f/16/73727675941ab8e6ffd86ca3a4b7b47065edcca7a997920b831f8147c99d/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/73/1b/44a01c4e70933637c93e6e1a8063d1e998b50213a6b65ac5a9169c47e98e/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f0/6e/c2cf12c9ff8b872e92b4a5740701e51ff17689c4d726fca91875b07f655d/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/06/1e/b8b7c2f4099a37b96af5c9bb158632ea9e5d9d27d7391d7eb8fc45236674/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3b/9a/72ef35b399b0e183bc2e8f6f558036922d453c4d8237dab26c666a04244b/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/67/ca/f42388aed0fddd64ade7493dbba36e1f534d4e6fdbdd355c6a90030ae028/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/9a/fff8376f8e3d084cd1530e1ef7b879bb7d6d265620c95c1b322725c694f4/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9d/d7/c5383e47c7e9bf1c99d5bd2a8c935af2b6d705ad831a7ec5c97db4d82f4f/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b2/66/cc9876340ac68ae71b15c743ddb13f8b30d5244af344ec8322b449e35426/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.8.0 in /environment/miniconda3/lib/python3.11/site-packages (from triton>=3.0.0->unsloth) (68.2.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /environment/miniconda3/lib/python3.11/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /environment/miniconda3/lib/python3.11/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /environment/miniconda3/lib/python3.11/site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from tyro->unsloth) (14.0.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /environment/miniconda3/lib/python3.11/site-packages (from tyro->unsloth) (1.7.2)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.4.4->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/df/5f/62fdb048f84d19e2123b6bbd722fe09c8c79b4964c50094d1e979db808e2/cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pillow in /environment/miniconda3/lib/python3.11/site-packages (from unsloth_zoo>=2025.4.4->unsloth) (11.2.1)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.4.4->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/85/2e/db7e189b57901955239f7689b5dcd6ae9458637a9c66747326726c650523/msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata (from diffusers->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/20/b0/36bd937216ec521246249be3bf9855081de4c5e06a0c9b4219dbeda50373/importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/09/42/6908bff012a1dcc4fc515e52339652d7f488e208986542765c02ea775c2f/torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (23.2.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /environment/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /environment/miniconda3/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /environment/miniconda3/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth) (2.19.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Collecting zipp>=3.20 (from importlib-metadata->diffusers->unsloth)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b7/1a/7e4798e9339adc931158c9d69ecc34f5e6791489d469f5e50ec15e35f458/zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /environment/miniconda3/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /environment/miniconda3/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /environment/miniconda3/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /environment/miniconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /environment/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, zipp, wheel, triton, sympy, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, hf_transfer, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, importlib-metadata, nvidia-cusolver-cu12, diffusers, torch, xformers, torchvision, cut_cross_entropy, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.2\n",
      "    Uninstalling wheel-0.41.2:\n",
      "      Successfully uninstalled wheel-0.41.2\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.2\n",
      "    Uninstalling torchvision-0.17.2:\n",
      "      Successfully uninstalled torchvision-0.17.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.2 requires torch==2.2.2, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cut_cross_entropy-25.1.1 diffusers-0.33.1 hf_transfer-0.1.9 importlib-metadata-8.7.0 msgspec-0.19.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 protobuf-3.20.3 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0 unsloth-2025.4.7 unsloth_zoo-2025.4.4 wheel-0.45.1 xformers-0.0.30 zipp-3.21.0\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: bitsandbytes in /environment/miniconda3/lib/python3.11/site-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from bitsandbytes) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /environment/miniconda3/lib/python3.11/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /environment/miniconda3/lib/python3.11/site-packages (from torch<3,>=2.0->bitsandbytes) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /environment/miniconda3/lib/python3.11/site-packages (from triton==3.3.0->torch<3,>=2.0->bitsandbytes) (68.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: ipywidgets in /environment/miniconda3/lib/python3.11/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /environment/miniconda3/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from ipywidgets) (8.23.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /environment/miniconda3/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /environment/miniconda3/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /environment/miniconda3/lib/python3.11/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /environment/miniconda3/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /environment/miniconda3/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /environment/miniconda3/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /environment/miniconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /environment/miniconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /environment/miniconda3/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.17.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: jupyter in /environment/miniconda3/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: notebook in /environment/miniconda3/lib/python3.11/site-packages (from jupyter) (6.4.13)\n",
      "Requirement already satisfied: jupyter-console in /environment/miniconda3/lib/python3.11/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /environment/miniconda3/lib/python3.11/site-packages (from jupyter) (7.16.3)\n",
      "Requirement already satisfied: ipykernel in /environment/miniconda3/lib/python3.11/site-packages (from jupyter) (6.29.4)\n",
      "Requirement already satisfied: ipywidgets in /environment/miniconda3/lib/python3.11/site-packages (from jupyter) (8.1.7)\n",
      "Requirement already satisfied: jupyterlab in /environment/miniconda3/lib/python3.11/site-packages (from jupyter) (4.2.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (8.23.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (25.0)\n",
      "Requirement already satisfied: psutil in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (26.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /environment/miniconda3/lib/python3.11/site-packages (from ipykernel->jupyter) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /environment/miniconda3/lib/python3.11/site-packages (from ipywidgets->jupyter) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /environment/miniconda3/lib/python3.11/site-packages (from ipywidgets->jupyter) (3.0.15)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-console->jupyter) (3.0.43)\n",
      "Requirement already satisfied: pygments in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-console->jupyter) (2.19.1)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.14.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab->jupyter) (2.27.1)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /environment/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /environment/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /environment/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /environment/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /environment/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /environment/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /environment/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /environment/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /environment/miniconda3/lib/python3.11/site-packages (from nbconvert->jupyter) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /environment/miniconda3/lib/python3.11/site-packages (from notebook->jupyter) (23.1.0)\n",
      "Requirement already satisfied: ipython-genutils in /environment/miniconda3/lib/python3.11/site-packages (from notebook->jupyter) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /environment/miniconda3/lib/python3.11/site-packages (from notebook->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /environment/miniconda3/lib/python3.11/site-packages (from notebook->jupyter) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /environment/miniconda3/lib/python3.11/site-packages (from notebook->jupyter) (0.20.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /environment/miniconda3/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /environment/miniconda3/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: anyio in /environment/miniconda3/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.9.0)\n",
      "Requirement already satisfied: certifi in /environment/miniconda3/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /environment/miniconda3/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: idna in /environment/miniconda3/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /environment/miniconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Requirement already satisfied: decorator in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.13.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /environment/miniconda3/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (3.10.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /environment/miniconda3/lib/python3.11/site-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.14.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.21.1)\n",
      "Requirement already satisfied: requests>=2.31 in /environment/miniconda3/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /environment/miniconda3/lib/python3.11/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.19.1)\n",
      "Requirement already satisfied: wcwidth in /environment/miniconda3/lib/python3.11/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.13)\n",
      "Requirement already satisfied: ptyprocess in /environment/miniconda3/lib/python3.11/site-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /environment/miniconda3/lib/python3.11/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /environment/miniconda3/lib/python3.11/site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /environment/miniconda3/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /environment/miniconda3/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /environment/miniconda3/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /environment/miniconda3/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.35.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /environment/miniconda3/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.18.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /environment/miniconda3/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /environment/miniconda3/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /environment/miniconda3/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /environment/miniconda3/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: pycparser in /environment/miniconda3/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: fqdn in /environment/miniconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /environment/miniconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /environment/miniconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.1)\n",
      "Requirement already satisfied: uri-template in /environment/miniconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /environment/miniconda3/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.13)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /environment/miniconda3/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /environment/miniconda3/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20240316)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: huggingface_hub in /environment/miniconda3/lib/python3.11/site-packages (0.31.1)\n",
      "Requirement already satisfied: filelock in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (2025.4.26)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting swanlab\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/43/95/52d0c012eb941bb6df31f31a4b3ee27536eb73191dce4adec2ded7d27c23/swanlab-0.5.7-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting boto3>=1.35.49 (from swanlab)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2f/fe/2b69dcdd433c32ba80b36eabfe799e8c3e0b08ff3e0fc06bc2e1cc065a19/boto3-1.38.10-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore (from swanlab)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/92/2c522e277c95d35b4b83bff6a3839875d91b0d835a93545828a7046013c4/botocore-1.38.10-py3-none-any.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /environment/miniconda3/lib/python3.11/site-packages (from swanlab) (8.1.8)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /environment/miniconda3/lib/python3.11/site-packages (from swanlab) (5.9.8)\n",
      "Requirement already satisfied: pydantic>=2.9.0 in /environment/miniconda3/lib/python3.11/site-packages (from swanlab) (2.11.4)\n",
      "Collecting pynvml (from swanlab)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ed/df/f7cf07a65a96dd11d71f346f9c2863accdd4784da83af7181b067d556cbc/pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyyaml in /environment/miniconda3/lib/python3.11/site-packages (from swanlab) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.25.0 in /environment/miniconda3/lib/python3.11/site-packages (from swanlab) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /environment/miniconda3/lib/python3.11/site-packages (from swanlab) (68.2.2)\n",
      "Collecting swankit==0.1.7 (from swanlab)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/74/75/8152b1f1a3034c3ba2369133264ddf9850df62b3ba2b75fd7c104d80090f/swankit-0.1.7-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /environment/miniconda3/lib/python3.11/site-packages (from swanlab) (2.4.0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.35.49->swanlab)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.13.0,>=0.12.0 (from boto3>=1.35.49->swanlab)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/89/64/d2b49620039b82688aeebd510bd62ff4cdcdb86cbf650cc72ae42c5254a3/s3transfer-0.12.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /environment/miniconda3/lib/python3.11/site-packages (from botocore->swanlab) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /environment/miniconda3/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /environment/miniconda3/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /environment/miniconda3/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /environment/miniconda3/lib/python3.11/site-packages (from pydantic>=2.9.0->swanlab) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.11/site-packages (from requests>=2.25.0->swanlab) (2025.4.26)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->swanlab)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/24/552ebea28f0570b9e65e62b50287a273804c9f997cc1c2dcd4e2d64b9e7d/nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /environment/miniconda3/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->swanlab) (1.17.0)\n",
      "Installing collected packages: nvidia-ml-py, swankit, pynvml, jmespath, botocore, s3transfer, boto3, swanlab\n",
      "Successfully installed boto3-1.38.10 botocore-1.38.10 jmespath-1.0.1 nvidia-ml-py-12.575.51 pynvml-12.0.0 s3transfer-0.12.0 swankit-0.1.7 swanlab-0.5.7\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting optree>=0.13.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/aa/3d/52a75740d6c449073d4bb54da382f6368553f285fb5a680b27dd198dd839/optree-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (410 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.4/410.4 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /environment/miniconda3/lib/python3.11/site-packages (from optree>=0.13.0) (4.13.2)\n",
      "Installing collected packages: optree\n",
      "  Attempting uninstall: optree\n",
      "    Found existing installation: optree 0.11.0\n",
      "    Uninstalling optree-0.11.0:\n",
      "      Successfully uninstalled optree-0.11.0\n",
      "Successfully installed optree-0.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install socksio\n",
    "!pip install tf-keras\n",
    "# !pip install flash-attn --no-build-isolation\n",
    "# !pip install flash-attn==2.7.3 --no-build-isolation\n",
    "# !pip install liger-kernel\n",
    "!pip install unsloth\n",
    "!pip install bitsandbytes\n",
    "!pip install ipywidgets\n",
    "!pip install jupyter\n",
    "!pip install huggingface_hub\n",
    "!pip install swanlab\n",
    "!pip install --upgrade 'optree>=0.13.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c7932ed-a14c-4a70-ad26-bbf75e9b82c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting torch==2.6\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/a9/97cbbc97002fff0de394a2da2cdfa859481fdca36996d7bd845d50aa9d8d/torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchaudio in /environment/miniconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /environment/miniconda3/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: filelock in /environment/miniconda3/lib/python3.11/site-packages (from torch==2.6) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /environment/miniconda3/lib/python3.11/site-packages (from torch==2.6) (4.13.2)\n",
      "Requirement already satisfied: networkx in /environment/miniconda3/lib/python3.11/site-packages (from torch==2.6) (3.3)\n",
      "Requirement already satisfied: jinja2 in /environment/miniconda3/lib/python3.11/site-packages (from torch==2.6) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /environment/miniconda3/lib/python3.11/site-packages (from torch==2.6) (2024.3.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2c/14/91ae57cd4db3f9ef7aa99f4019cfa8d54cb4caa7e00975df6467e9725a9f/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ea/27/1795d86fe88ef397885f2e580ac37628ed058a92ed2c39dc8eac3adf0619/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/67/42/f4f60238e8194a3106d06a058d494b18e006c10bb2b915655bd9f6ea4cb1/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ae/71/1c91302526c45ab494c23f61c7a84aa568b8c1f9d196efa5993957faf906/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/27/94/3266821f65b92b3138631e9c8e7fe1fb513804ac934485a8d05776e1dd43/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8a/6d/44ad094874c6f1b9c654f8ed939590bdc408349f137f9b98a3a23ccec411/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3a/e1/5b9089a4b2a4790dfdea8b3a006052cfecff58139d5a4e34cb1a51df8d6f/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/f7/97a9ea26ed4bbbfc2d470994b8b4f338ef663be97b8f677519ac195e113d/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/a8/bcbb63b53a4b1234feeafb65544ee55495e1bb37ec31b999b963cbccfd1d/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/df/99/12cd266d6233f47d00daf3a72739872bdc10267d0383508b0b9c84a18bb6/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/87/20/199b8713428322a2f22b722c62b8cc278cc53dffa9705d744484b5035ee9/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ff/ff/847841bacfbefc97a00036e0fce5a0f086b640756dc38caea5e1bb002655/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.2.0 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a7/2e/757d2280d4fefe7d33af7615124e7e298ae7b8e3bc4446cdb8e88b0f9bab/triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.13.1 (from torch==2.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b2/fe/81695a1aa331a842b582453b605175f419fe8540355886031328089d840a/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.6) (1.3.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ab/20/1873a49df9f1778c241543eaca14d613d657b9f9351c254952114251cb86/torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3e/00/2c69d436c613043f3051210d2f84a4c9062a815fa609c5f54d25ea8bfd07/torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /environment/miniconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/be/a2/b0cedf0a411f1a5d75cfc0b87cde56dd1ddc1878be46a42c905cd8580220/torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /environment/miniconda3/lib/python3.11/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from jinja2->torch==2.6) (2.1.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.0\n",
      "    Uninstalling triton-3.3.0:\n",
      "      Successfully uninstalled triton-3.3.0\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: nvidia-nccl-cu12 2.26.2\n",
      "    Uninstalling nvidia-nccl-cu12-2.26.2:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
      "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.0\n",
      "    Uninstalling torch-2.7.0:\n",
      "      Successfully uninstalled torch-2.7.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.22.0\n",
      "    Uninstalling torchvision-0.22.0:\n",
      "      Successfully uninstalled torchvision-0.22.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.2.2\n",
      "    Uninstalling torchaudio-2.2.2:\n",
      "      Successfully uninstalled torchaudio-2.2.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xformers 0.0.30 requires torch==2.7.0, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0 triton-3.2.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip uninstall torch torchaudio torchvision -y\n",
    "!pip install torch==2.6 torchaudio torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7ce005-bc2e-45d1-8c3f-0ede83c7a724",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Processing ./flash_attn-2.7.2.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
      "Requirement already satisfied: torch in /environment/miniconda3/lib/python3.11/site-packages (from flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (2.6.0)\n",
      "Requirement already satisfied: einops in /environment/miniconda3/lib/python3.11/site-packages (from flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (0.8.1)\n",
      "Requirement already satisfied: filelock in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (4.13.2)\n",
      "Requirement already satisfied: networkx in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (3.3)\n",
      "Requirement already satisfied: jinja2 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /environment/miniconda3/lib/python3.11/site-packages (from torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /environment/miniconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from jinja2->torch->flash-attn==2.7.2.post1+cu12torch2.6cxx11abiFALSE) (2.1.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: flash-attn\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed flash-attn-2.7.2.post1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~eras (/environment/miniconda3/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.2.post1/flash_attn-2.7.2.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl\n",
    "!pip install flash_attn-2.7.2.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2cb319-1ae9-4a70-9bdc-29d9b73f9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Login successfully. Hi, \u001b[1m\u001b[39mlixidong\u001b[0m\u001b[0m!                        \n"
     ]
    }
   ],
   "source": [
    "# 登录huggingface\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"\")\n",
    "\n",
    "!swanlab login --relogin --api-key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ae443-f788-43b7-b09d-17dc587ea794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-30 19:05:44.997887: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746039945.016495   11233 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746039945.022373   11233 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746039945.035873   11233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746039945.035906   11233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746039945.035910   11233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746039945.035913   11233 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-30 19:05:45.040458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://e0062cd008f6961588.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "[WARNING|2025-04-30 19:08:12] llamafactory.webui.common:148 >> Found complex path, some features may be not available.\n",
      "[WARNING|2025-04-30 19:08:12] llamafactory.webui.common:148 >> Found complex path, some features may be not available.\n",
      "[WARNING|2025-04-30 19:08:12] llamafactory.webui.common:148 >> Found complex path, some features may be not available.\n",
      "2025-04-30 19:08:16.781093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746040096.799465   11376 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746040096.805275   11376 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746040096.818802   11376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746040096.818839   11376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746040096.818842   11376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746040096.818845   11376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-30 19:08:16.822988: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[INFO|2025-04-30 19:08:22] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:22,609 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:22,610 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:22,610 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:22,610 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:22,610 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:22,610 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:22,610 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-30 19:08:23,042 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-04-30 19:08:23,045 >> loading configuration file /home/featurize/work/LLaMA-Factory-4-2/A_myModels/Qwen2.5-1.5B-Instruct-bin/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-30 19:08:23,048 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:23,050 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:23,050 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:23,051 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:23,051 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:23,051 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:23,051 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:08:23,051 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-30 19:08:23,528 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-04-30 19:08:23] llamafactory.data.loader:143 >> Loading dataset /home/featurize/data/data_sharegpt/data_sharegp_special_chair_2k.json...\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 510/510 [00:02<00:00, 196.60\n",
      "training example:\n",
      "input_ids:\n",
      "[33975, 25, 4230, 264, 220, 18, 35, 1614, 315, 264, 6176, 26148, 11, 22360, 10496, 11, 323, 4845, 448, 264, 6176, 44638, 13, 151645, 198, 71703, 25, 85, 220, 16, 22, 220, 18, 22, 220, 22, 198, 85, 220, 19, 20, 220, 18, 22, 220, 22, 198, 85, 220, 16, 22, 220, 18, 23, 220, 22, 198, 85, 220, 19, 20, 220, 18, 23, 220, 22, 198, 85, 220, 16, 24, 220, 16, 21, 220, 23, 198, 85, 220, 17, 16, 220, 16, 21, 220, 23, 198, 85, 220, 19, 16, 220, 16, 21, 220, 23, 198, 85, 220, 19, 18, 220, 16, 21, 220, 23, 198, 85, 220, 17, 16, 220, 17, 20, 220, 23, 198, 85, 220, 19, 16, 220, 17, 20, 220, 23, 198, 85, 220, 17, 16, 220, 17, 22, 220, 23, 198, 85, 220, 19, 16, 220, 17, 22, 220, 23, 198, 85, 220, 16, 24, 220, 18, 22, 220, 23, 198, 85, 220, 17, 16, 220, 18, 22, 220, 23, 198, 85, 220, 19, 16, 220, 18, 22, 220, 23, 198, 85, 220, 19, 18, 220, 18, 22, 220, 23, 198, 85, 220, 16, 24, 220, 16, 21, 220, 24, 198, 85, 220, 17, 16, 220, 16, 21, 220, 24, 198, 85, 220, 19, 16, 220, 16, 21, 220, 24, 198, 85, 220, 19, 18, 220, 16, 21, 220, 24, 198, 85, 220, 17, 15, 220, 17, 17, 220, 24, 198, 85, 220, 17, 16, 220, 17, 17, 220, 24, 198, 85, 220, 19, 16, 220, 17, 17, 220, 24, 198, 85, 220, 19, 17, 220, 17, 17, 220, 24, 198, 85, 220, 16, 22, 220, 17, 20, 220, 24, 198, 85, 220, 17, 15, 220, 17, 20, 220, 24, 198, 85, 220, 17, 16, 220, 17, 20, 220, 24, 198, 85, 220, 19, 16, 220, 17, 20, 220, 24, 198, 85, 220, 19, 17, 220, 17, 20, 220, 24, 198, 85, 220, 19, 20, 220, 17, 20, 220, 24, 198, 85, 220, 17, 16, 220, 17, 22, 220, 24, 198, 85, 220, 19, 16, 220, 17, 22, 220, 24, 198, 85, 220, 16, 22, 220, 18, 15, 220, 24, 198, 85, 220, 17, 18, 220, 18, 15, 220, 24, 198, 85, 220, 19, 20, 220, 18, 15, 220, 24, 198, 85, 220, 16, 22, 220, 18, 22, 220, 24, 198, 85, 220, 16, 24, 220, 18, 22, 220, 24, 198, 85, 220, 17, 16, 220, 18, 22, 220, 24, 198, 85, 220, 19, 16, 220, 18, 22, 220, 24, 198, 85, 220, 19, 18, 220, 18, 22, 220, 24, 198, 85, 220, 19, 20, 220, 18, 22, 220, 24, 198, 85, 220, 16, 22, 220, 18, 23, 220, 24, 198, 85, 220, 19, 20, 220, 18, 23, 220, 24, 198, 85, 220, 16, 22, 220, 19, 21, 220, 24, 198, 85, 220, 17, 18, 220, 19, 21, 220, 24, 198, 85, 220, 16, 24, 220, 16, 21, 220, 20, 18, 198, 85, 220, 17, 16, 220, 16, 21, 220, 20, 18, 198, 85, 220, 19, 16, 220, 16, 21, 220, 20, 18, 198, 85, 220, 19, 18, 220, 16, 21, 220, 20, 18, 198, 85, 220, 17, 15, 220, 17, 17, 220, 20, 18, 198, 85, 220, 17, 16, 220, 17, 17, 220, 20, 18, 198, 85, 220, 19, 16, 220, 17, 17, 220, 20, 18, 198, 85, 220, 19, 17, 220, 17, 17, 220, 20, 18, 198, 85, 220, 16, 22, 220, 17, 20, 220, 20, 18, 198, 85, 220, 17, 15, 220, 17, 20, 220, 20, 18, 198, 85, 220, 17, 16, 220, 17, 20, 220, 20, 18, 198, 85, 220, 19, 16, 220, 17, 20, 220, 20, 18, 198, 85, 220, 19, 17, 220, 17, 20, 220, 20, 18, 198, 85, 220, 19, 20, 220, 17, 20, 220, 20, 18, 198, 85, 220, 17, 16, 220, 17, 22, 220, 20, 18, 198, 85, 220, 19, 16, 220, 17, 22, 220, 20, 18, 198, 85, 220, 16, 22, 220, 18, 15, 220, 20, 18, 198, 85, 220, 17, 18, 220, 18, 15, 220, 20, 18, 198, 85, 220, 19, 20, 220, 18, 15, 220, 20, 18, 198, 85, 220, 16, 22, 220, 18, 22, 220, 20, 18, 198, 85, 220, 16, 24, 220, 18, 22, 220, 20, 18, 198, 85, 220, 17, 16, 220, 18, 22, 220, 20, 18, 198, 85, 220, 19, 16, 220, 18, 22, 220, 20, 18, 198, 85, 220, 19, 18, 220, 18, 22, 220, 20, 18, 198, 85, 220, 19, 20, 220, 18, 22, 220, 20, 18, 198, 85, 220, 16, 22, 220, 18, 23, 220, 20, 18, 198, 85, 220, 19, 20, 220, 18, 23, 220, 20, 18, 198, 85, 220, 16, 22, 220, 19, 21, 220, 20, 18, 198, 85, 220, 17, 18, 220, 19, 21, 220, 20, 18, 198, 85, 220, 16, 24, 220, 16, 21, 220, 20, 19, 198, 85, 220, 17, 16, 220, 16, 21, 220, 20, 19, 198, 85, 220, 19, 16, 220, 16, 21, 220, 20, 19, 198, 85, 220, 19, 18, 220, 16, 21, 220, 20, 19, 198, 85, 220, 17, 16, 220, 17, 20, 220, 20, 19, 198, 85, 220, 19, 16, 220, 17, 20, 220, 20, 19, 198, 85, 220, 17, 16, 220, 17, 22, 220, 20, 19, 198, 85, 220, 19, 16, 220, 17, 22, 220, 20, 19, 198, 85, 220, 16, 24, 220, 18, 22, 220, 20, 19, 198, 85, 220, 17, 16, 220, 18, 22, 220, 20, 19, 198, 85, 220, 19, 16, 220, 18, 22, 220, 20, 19, 198, 85, 220, 19, 18, 220, 18, 22, 220, 20, 19, 198, 85, 220, 16, 22, 220, 18, 22, 220, 20, 20, 198, 85, 220, 19, 20, 220, 18, 22, 220, 20, 20, 198, 85, 220, 16, 22, 220, 18, 23, 220, 20, 20, 198, 85, 220, 19, 20, 220, 18, 23, 220, 20, 20, 198, 69, 220, 16, 220, 17, 220, 19, 198, 69, 220, 16, 220, 18, 21, 220, 17, 198, 69, 220, 16, 220, 19, 220, 18, 198, 69, 220, 16, 220, 18, 220, 18, 21, 198, 69, 220, 17, 220, 19, 18, 220, 19, 198, 69, 220, 17, 220, 18, 21, 220, 19, 16, 198, 69, 220, 17, 220, 19, 16, 220, 19, 18, 198, 69, 220, 18, 220, 19, 220, 19, 17, 198, 69, 220, 18, 220, 19, 17, 220, 18, 21, 198, 69, 220, 19, 220, 19, 18, 220, 19, 17, 198, 69, 220, 20, 220, 21, 220, 16, 18, 198, 69, 220, 20, 220, 16, 22, 220, 21, 198, 69, 220, 20, 220, 16, 18, 220, 18, 22, 198, 69, 220, 20, 220, 18, 22, 220, 16, 22, 198, 69, 220, 21, 220, 16, 19, 220, 16, 18, 198, 69, 220, 21, 220, 16, 23, 220, 16, 19, 198, 69, 220, 21, 220, 16, 22, 220, 16, 23, 198, 69, 220, 22, 220, 23, 220, 16, 20, 198, 69, 220, 22, 220, 16, 24, 220, 23, 198, 69, 220, 22, 220, 16, 20, 220, 18, 24, 198, 69, 220, 22, 220, 18, 24, 220, 16, 24, 198, 69, 220, 23, 220, 16, 21, 220, 16, 20, 198, 69, 220, 23, 220, 17, 15, 220, 16, 21, 198, 69, 220, 23, 220, 16, 24, 220, 17, 15, 198, 69, 220, 24, 220, 16, 15, 220, 16, 16, 198, 69, 220, 24, 220, 17, 22, 220, 16, 15, 198, 69, 220, 24, 220, 16, 16, 220, 18, 16, 198, 69, 220, 24, 220, 18, 16, 220, 17, 22, 198, 69, 220, 16, 15, 220, 16, 17, 220, 16, 16, 198, 69, 220, 16, 15, 220, 17, 23, 220, 16, 17, 198, 69, 220, 16, 15, 220, 17, 22, 220, 17, 23, 198, 69, 220, 16, 16, 220, 16, 17, 220, 18, 17, 198, 69, 220, 16, 16, 220, 18, 17, 220, 18, 16, 198, 69, 220, 16, 17, 220, 17, 23, 220, 18, 17, 198, 69, 220, 16, 18, 220, 16, 19, 220, 18, 23, 198, 69, 220, 16, 18, 220, 18, 23, 220, 18, 22, 198, 69, 220, 16, 19, 220, 16, 23, 220, 18, 23, 198, 69, 220, 16, 20, 220, 16, 21, 220, 19, 15, 198, 69, 220, 16, 20, 220, 19, 15, 220, 18, 24, 198, 69, 220, 16, 21, 220, 17, 15, 220, 19, 15, 198, 69, 220, 16, 22, 220, 18, 22, 220, 16, 23, 198, 69, 220, 16, 23, 220, 18, 22, 220, 18, 23, 198, 69, 220, 16, 24, 220, 18, 24, 220, 17, 15, 198, 69, 220, 17, 15, 220, 18, 24, 220, 19, 15, 198, 69, 220, 17, 16, 220, 17, 17, 220, 17, 21, 198, 69, 220, 17, 16, 220, 20, 16, 220, 17, 17, 198, 69, 220, 17, 16, 220, 17, 21, 220, 20, 15, 198, 69, 220, 17, 16, 220, 20, 15, 220, 20, 16, 198, 69, 220, 17, 17, 220, 17, 22, 220, 17, 21, 198, 69, 220, 17, 17, 220, 20, 16, 220, 17, 22, 198, 69, 220, 17, 18, 220, 17, 19, 220, 17, 23, 198, 69, 220, 17, 18, 220, 20, 18, 220, 17, 19, 198, 69, 220, 17, 18, 220, 17, 23, 220, 20, 17, 198, 69, 220, 17, 18, 220, 20, 17, 220, 20, 18, 198, 69, 220, 17, 19, 220, 17, 24, 220, 17, 23, 198, 69, 220, 17, 19, 220, 20, 18, 220, 17, 24, 198, 69, 220, 17, 20, 220, 18, 15, 220, 18, 20, 198, 69, 220, 17, 20, 220, 20, 19, 220, 18, 15, 198, 69, 220, 17, 20, 220, 18, 20, 220, 18, 18, 198, 69, 220, 17, 20, 220, 18, 18, 220, 20, 19, 198, 69, 220, 17, 21, 220, 17, 22, 220, 20, 20, 198, 69, 220, 17, 21, 220, 20, 20, 220, 20, 15, 198, 69, 220, 17, 22, 220, 18, 16, 220, 17, 23, 198, 69, 220, 17, 22, 220, 20, 16, 220, 20, 21, 198, 69, 220, 17, 22, 220, 20, 21, 220, 20, 20, 198, 69, 220, 17, 23, 220, 17, 24, 220, 20, 22, 198, 69, 220, 17, 23, 220, 18, 16, 220, 18, 17, 198, 69, 220, 17, 23, 220, 20, 22, 220, 20, 17, 198, 69, 220, 17, 24, 220, 20, 18, 220, 20, 23, 198, 69, 220, 17, 24, 220, 20, 23, 220, 20, 22, 198, 69, 220, 18, 15, 220, 21, 19, 220, 18, 20, 198, 69, 220, 18, 15, 220, 20, 19, 220, 20, 24, 198, 69, 220, 18, 15, 220, 20, 24, 220, 21, 19, 198, 69, 220, 18, 18, 220, 18, 19, 220, 19, 20, 198, 69, 220, 18, 18, 220, 21, 17, 220, 18, 19, 198, 69, 220, 18, 18, 220, 18, 20, 220, 21, 17, 198, 69, 220, 18, 18, 220, 19, 20, 220, 19, 19, 198, 69, 220, 18, 18, 220, 19, 19, 220, 21, 17, 198, 69, 220, 18, 18, 220, 21, 17, 220, 20, 19, 198, 69, 220, 18, 19, 220, 22, 19, 220, 19, 20, 198, 69, 220, 18, 19, 220, 21, 17, 220, 21, 18, 198, 69, 220, 18, 19, 220, 21, 18, 220, 22, 19, 198, 69, 220, 18, 20, 220, 21, 19, 220, 21, 17, 198, 69, 220, 18, 21, 220, 19, 17, 220, 19, 16, 198, 69, 220, 19, 16, 220, 19, 17, 220, 19, 18, 198, 69, 220, 19, 19, 220, 19, 20, 220, 22, 18, 198, 69, 220, 19, 19, 220, 22, 18, 220, 21, 17, 198, 69, 220, 19, 20, 220, 22, 19, 220, 22, 18, 198, 69, 220, 19, 21, 220, 19, 22, 220, 21, 21, 198, 69, 220, 19, 21, 220, 22, 20, 220, 19, 22, 198, 69, 220, 19, 21, 220, 21, 21, 220, 23, 18, 198, 69, 220, 19, 21, 220, 23, 18, 220, 22, 20, 198, 69, 220, 19, 22, 220, 21, 22, 220, 21, 21, 198, 69, 220, 19, 22, 220, 22, 21, 220, 21, 22, 198, 69, 220, 19, 22, 220, 22, 20, 220, 22, 21, 198, 69, 220, 19, 23, 220, 19, 24, 220, 21, 23, 198, 69, 220, 19, 23, 220, 22, 22, 220, 19, 24, 198, 69, 220, 19, 23, 220, 21, 23, 220, 23, 20, 198, 69, 220, 19, 23, 220, 23, 20, 220, 22, 22, 198, 69, 220, 19, 24, 220, 21, 24, 220, 21, 23, 198, 69, 220, 19, 24, 220, 22, 23, 220, 21, 24, 198, 69, 220, 19, 24, 220, 22, 22, 220, 22, 23, 198, 69, 220, 20, 15, 220, 20, 21, 220, 20, 16, 198, 69, 220, 20, 15, 220, 20, 20, 220, 20, 21, 198, 69, 220, 20, 17, 220, 20, 23, 220, 20, 18, 198, 69, 220, 20, 17, 220, 20, 22, 220, 20, 23, 198, 69, 220, 20, 19, 220, 21, 17, 220, 20, 24, 198, 69, 220, 20, 21, 220, 20, 22, 220, 21, 15, 198, 69, 220, 20, 21, 220, 22, 24, 220, 20, 22, 198, 69, 220, 20, 21, 220, 21, 15, 220, 23, 16, 198, 69, 220, 20, 21, 220, 23, 16, 220, 22, 24, 198, 69, 220, 20, 22, 220, 21, 16, 220, 21, 15, 198, 69, 220, 20, 22, 220, 23, 15, 220, 21, 16, 198, 69, 220, 20, 22, 220, 22, 24, 220, 23, 15, 198, 69, 220, 20, 24, 220, 21, 17, 220, 21, 19, 198, 69, 220, 21, 15, 220, 21, 16, 220, 23, 17, 198, 69, 220, 21, 15, 220, 23, 17, 220, 23, 16, 198, 69, 220, 21, 16, 220, 23, 15, 220, 23, 17, 198, 69, 220, 21, 17, 220, 22, 18, 220, 21, 18, 198, 69, 220, 21, 18, 220, 22, 18, 220, 22, 19, 198, 69, 220, 21, 20, 220, 22, 15, 220, 22, 17, 198, 69, 220, 21, 20, 220, 23, 22, 220, 22, 15, 198, 69, 220, 21, 20, 220, 22, 17, 220, 22, 16, 198, 69, 220, 21, 20, 220, 22, 16, 220, 23, 22, 198, 69, 220, 21, 21, 220, 21, 22, 220, 23, 19, 198, 69, 220, 21, 21, 220, 23, 19, 220, 23, 18, 198, 69, 220, 21, 22, 220, 22, 21, 220, 23, 19, 198, 69, 220, 21, 23, 220, 21, 24, 220, 23, 21, 198, 69, 220, 21, 23, 220, 23, 21, 220, 23, 20, 198, 69, 220, 21, 24, 220, 22, 23, 220, 23, 21, 198, 69, 220, 22, 15, 220, 24, 15, 220, 22, 17, 198, 69, 220, 22, 15, 220, 23, 22, 220, 23, 23, 198, 69, 220, 22, 15, 220, 23, 23, 220, 24, 15, 198, 69, 220, 22, 16, 220, 22, 17, 220, 23, 24, 198, 69, 220, 22, 16, 220, 23, 24, 220, 23, 22, 198, 69, 220, 22, 17, 220, 24, 15, 220, 23, 24, 198, 69, 220, 22, 20, 220, 23, 18, 220, 22, 21, 198, 69, 220, 22, 21, 220, 23, 18, 220, 23, 19, 198, 69, 220, 22, 22, 220, 23, 20, 220, 22, 23, 198, 69, 220, 22, 23, 220, 23, 20, 220, 23, 21, 198, 69, 220, 22, 24, 220, 23, 16, 220, 23, 15, 198, 69, 220, 23, 15, 220, 23, 16, 220, 23, 17, 198, 69, 220, 23, 22, 220, 23, 24, 220, 23, 23, 198, 69, 220, 23, 23, 220, 23, 24, 220, 24, 15, 198, 151645, 198]\n",
      "inputs:\n",
      "Human: Create a 3D model of a green couch, wooden chair, and bed with a green cushion.<|im_end|>\n",
      "Assistant:v 17 37 7\n",
      "v 45 37 7\n",
      "v 17 38 7\n",
      "v 45 38 7\n",
      "v 19 16 8\n",
      "v 21 16 8\n",
      "v 41 16 8\n",
      "v 43 16 8\n",
      "v 21 25 8\n",
      "v 41 25 8\n",
      "v 21 27 8\n",
      "v 41 27 8\n",
      "v 19 37 8\n",
      "v 21 37 8\n",
      "v 41 37 8\n",
      "v 43 37 8\n",
      "v 19 16 9\n",
      "v 21 16 9\n",
      "v 41 16 9\n",
      "v 43 16 9\n",
      "v 20 22 9\n",
      "v 21 22 9\n",
      "v 41 22 9\n",
      "v 42 22 9\n",
      "v 17 25 9\n",
      "v 20 25 9\n",
      "v 21 25 9\n",
      "v 41 25 9\n",
      "v 42 25 9\n",
      "v 45 25 9\n",
      "v 21 27 9\n",
      "v 41 27 9\n",
      "v 17 30 9\n",
      "v 23 30 9\n",
      "v 45 30 9\n",
      "v 17 37 9\n",
      "v 19 37 9\n",
      "v 21 37 9\n",
      "v 41 37 9\n",
      "v 43 37 9\n",
      "v 45 37 9\n",
      "v 17 38 9\n",
      "v 45 38 9\n",
      "v 17 46 9\n",
      "v 23 46 9\n",
      "v 19 16 53\n",
      "v 21 16 53\n",
      "v 41 16 53\n",
      "v 43 16 53\n",
      "v 20 22 53\n",
      "v 21 22 53\n",
      "v 41 22 53\n",
      "v 42 22 53\n",
      "v 17 25 53\n",
      "v 20 25 53\n",
      "v 21 25 53\n",
      "v 41 25 53\n",
      "v 42 25 53\n",
      "v 45 25 53\n",
      "v 21 27 53\n",
      "v 41 27 53\n",
      "v 17 30 53\n",
      "v 23 30 53\n",
      "v 45 30 53\n",
      "v 17 37 53\n",
      "v 19 37 53\n",
      "v 21 37 53\n",
      "v 41 37 53\n",
      "v 43 37 53\n",
      "v 45 37 53\n",
      "v 17 38 53\n",
      "v 45 38 53\n",
      "v 17 46 53\n",
      "v 23 46 53\n",
      "v 19 16 54\n",
      "v 21 16 54\n",
      "v 41 16 54\n",
      "v 43 16 54\n",
      "v 21 25 54\n",
      "v 41 25 54\n",
      "v 21 27 54\n",
      "v 41 27 54\n",
      "v 19 37 54\n",
      "v 21 37 54\n",
      "v 41 37 54\n",
      "v 43 37 54\n",
      "v 17 37 55\n",
      "v 45 37 55\n",
      "v 17 38 55\n",
      "v 45 38 55\n",
      "f 1 2 4\n",
      "f 1 36 2\n",
      "f 1 4 3\n",
      "f 1 3 36\n",
      "f 2 43 4\n",
      "f 2 36 41\n",
      "f 2 41 43\n",
      "f 3 4 42\n",
      "f 3 42 36\n",
      "f 4 43 42\n",
      "f 5 6 13\n",
      "f 5 17 6\n",
      "f 5 13 37\n",
      "f 5 37 17\n",
      "f 6 14 13\n",
      "f 6 18 14\n",
      "f 6 17 18\n",
      "f 7 8 15\n",
      "f 7 19 8\n",
      "f 7 15 39\n",
      "f 7 39 19\n",
      "f 8 16 15\n",
      "f 8 20 16\n",
      "f 8 19 20\n",
      "f 9 10 11\n",
      "f 9 27 10\n",
      "f 9 11 31\n",
      "f 9 31 27\n",
      "f 10 12 11\n",
      "f 10 28 12\n",
      "f 10 27 28\n",
      "f 11 12 32\n",
      "f 11 32 31\n",
      "f 12 28 32\n",
      "f 13 14 38\n",
      "f 13 38 37\n",
      "f 14 18 38\n",
      "f 15 16 40\n",
      "f 15 40 39\n",
      "f 16 20 40\n",
      "f 17 37 18\n",
      "f 18 37 38\n",
      "f 19 39 20\n",
      "f 20 39 40\n",
      "f 21 22 26\n",
      "f 21 51 22\n",
      "f 21 26 50\n",
      "f 21 50 51\n",
      "f 22 27 26\n",
      "f 22 51 27\n",
      "f 23 24 28\n",
      "f 23 53 24\n",
      "f 23 28 52\n",
      "f 23 52 53\n",
      "f 24 29 28\n",
      "f 24 53 29\n",
      "f 25 30 35\n",
      "f 25 54 30\n",
      "f 25 35 33\n",
      "f 25 33 54\n",
      "f 26 27 55\n",
      "f 26 55 50\n",
      "f 27 31 28\n",
      "f 27 51 56\n",
      "f 27 56 55\n",
      "f 28 29 57\n",
      "f 28 31 32\n",
      "f 28 57 52\n",
      "f 29 53 58\n",
      "f 29 58 57\n",
      "f 30 64 35\n",
      "f 30 54 59\n",
      "f 30 59 64\n",
      "f 33 34 45\n",
      "f 33 62 34\n",
      "f 33 35 62\n",
      "f 33 45 44\n",
      "f 33 44 62\n",
      "f 33 62 54\n",
      "f 34 74 45\n",
      "f 34 62 63\n",
      "f 34 63 74\n",
      "f 35 64 62\n",
      "f 36 42 41\n",
      "f 41 42 43\n",
      "f 44 45 73\n",
      "f 44 73 62\n",
      "f 45 74 73\n",
      "f 46 47 66\n",
      "f 46 75 47\n",
      "f 46 66 83\n",
      "f 46 83 75\n",
      "f 47 67 66\n",
      "f 47 76 67\n",
      "f 47 75 76\n",
      "f 48 49 68\n",
      "f 48 77 49\n",
      "f 48 68 85\n",
      "f 48 85 77\n",
      "f 49 69 68\n",
      "f 49 78 69\n",
      "f 49 77 78\n",
      "f 50 56 51\n",
      "f 50 55 56\n",
      "f 52 58 53\n",
      "f 52 57 58\n",
      "f 54 62 59\n",
      "f 56 57 60\n",
      "f 56 79 57\n",
      "f 56 60 81\n",
      "f 56 81 79\n",
      "f 57 61 60\n",
      "f 57 80 61\n",
      "f 57 79 80\n",
      "f 59 62 64\n",
      "f 60 61 82\n",
      "f 60 82 81\n",
      "f 61 80 82\n",
      "f 62 73 63\n",
      "f 63 73 74\n",
      "f 65 70 72\n",
      "f 65 87 70\n",
      "f 65 72 71\n",
      "f 65 71 87\n",
      "f 66 67 84\n",
      "f 66 84 83\n",
      "f 67 76 84\n",
      "f 68 69 86\n",
      "f 68 86 85\n",
      "f 69 78 86\n",
      "f 70 90 72\n",
      "f 70 87 88\n",
      "f 70 88 90\n",
      "f 71 72 89\n",
      "f 71 89 87\n",
      "f 72 90 89\n",
      "f 75 83 76\n",
      "f 76 83 84\n",
      "f 77 85 78\n",
      "f 78 85 86\n",
      "f 79 81 80\n",
      "f 80 81 82\n",
      "f 87 89 88\n",
      "f 88 89 90\n",
      "<|im_end|>\n",
      "\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 85, 220, 16, 22, 220, 18, 22, 220, 22, 198, 85, 220, 19, 20, 220, 18, 22, 220, 22, 198, 85, 220, 16, 22, 220, 18, 23, 220, 22, 198, 85, 220, 19, 20, 220, 18, 23, 220, 22, 198, 85, 220, 16, 24, 220, 16, 21, 220, 23, 198, 85, 220, 17, 16, 220, 16, 21, 220, 23, 198, 85, 220, 19, 16, 220, 16, 21, 220, 23, 198, 85, 220, 19, 18, 220, 16, 21, 220, 23, 198, 85, 220, 17, 16, 220, 17, 20, 220, 23, 198, 85, 220, 19, 16, 220, 17, 20, 220, 23, 198, 85, 220, 17, 16, 220, 17, 22, 220, 23, 198, 85, 220, 19, 16, 220, 17, 22, 220, 23, 198, 85, 220, 16, 24, 220, 18, 22, 220, 23, 198, 85, 220, 17, 16, 220, 18, 22, 220, 23, 198, 85, 220, 19, 16, 220, 18, 22, 220, 23, 198, 85, 220, 19, 18, 220, 18, 22, 220, 23, 198, 85, 220, 16, 24, 220, 16, 21, 220, 24, 198, 85, 220, 17, 16, 220, 16, 21, 220, 24, 198, 85, 220, 19, 16, 220, 16, 21, 220, 24, 198, 85, 220, 19, 18, 220, 16, 21, 220, 24, 198, 85, 220, 17, 15, 220, 17, 17, 220, 24, 198, 85, 220, 17, 16, 220, 17, 17, 220, 24, 198, 85, 220, 19, 16, 220, 17, 17, 220, 24, 198, 85, 220, 19, 17, 220, 17, 17, 220, 24, 198, 85, 220, 16, 22, 220, 17, 20, 220, 24, 198, 85, 220, 17, 15, 220, 17, 20, 220, 24, 198, 85, 220, 17, 16, 220, 17, 20, 220, 24, 198, 85, 220, 19, 16, 220, 17, 20, 220, 24, 198, 85, 220, 19, 17, 220, 17, 20, 220, 24, 198, 85, 220, 19, 20, 220, 17, 20, 220, 24, 198, 85, 220, 17, 16, 220, 17, 22, 220, 24, 198, 85, 220, 19, 16, 220, 17, 22, 220, 24, 198, 85, 220, 16, 22, 220, 18, 15, 220, 24, 198, 85, 220, 17, 18, 220, 18, 15, 220, 24, 198, 85, 220, 19, 20, 220, 18, 15, 220, 24, 198, 85, 220, 16, 22, 220, 18, 22, 220, 24, 198, 85, 220, 16, 24, 220, 18, 22, 220, 24, 198, 85, 220, 17, 16, 220, 18, 22, 220, 24, 198, 85, 220, 19, 16, 220, 18, 22, 220, 24, 198, 85, 220, 19, 18, 220, 18, 22, 220, 24, 198, 85, 220, 19, 20, 220, 18, 22, 220, 24, 198, 85, 220, 16, 22, 220, 18, 23, 220, 24, 198, 85, 220, 19, 20, 220, 18, 23, 220, 24, 198, 85, 220, 16, 22, 220, 19, 21, 220, 24, 198, 85, 220, 17, 18, 220, 19, 21, 220, 24, 198, 85, 220, 16, 24, 220, 16, 21, 220, 20, 18, 198, 85, 220, 17, 16, 220, 16, 21, 220, 20, 18, 198, 85, 220, 19, 16, 220, 16, 21, 220, 20, 18, 198, 85, 220, 19, 18, 220, 16, 21, 220, 20, 18, 198, 85, 220, 17, 15, 220, 17, 17, 220, 20, 18, 198, 85, 220, 17, 16, 220, 17, 17, 220, 20, 18, 198, 85, 220, 19, 16, 220, 17, 17, 220, 20, 18, 198, 85, 220, 19, 17, 220, 17, 17, 220, 20, 18, 198, 85, 220, 16, 22, 220, 17, 20, 220, 20, 18, 198, 85, 220, 17, 15, 220, 17, 20, 220, 20, 18, 198, 85, 220, 17, 16, 220, 17, 20, 220, 20, 18, 198, 85, 220, 19, 16, 220, 17, 20, 220, 20, 18, 198, 85, 220, 19, 17, 220, 17, 20, 220, 20, 18, 198, 85, 220, 19, 20, 220, 17, 20, 220, 20, 18, 198, 85, 220, 17, 16, 220, 17, 22, 220, 20, 18, 198, 85, 220, 19, 16, 220, 17, 22, 220, 20, 18, 198, 85, 220, 16, 22, 220, 18, 15, 220, 20, 18, 198, 85, 220, 17, 18, 220, 18, 15, 220, 20, 18, 198, 85, 220, 19, 20, 220, 18, 15, 220, 20, 18, 198, 85, 220, 16, 22, 220, 18, 22, 220, 20, 18, 198, 85, 220, 16, 24, 220, 18, 22, 220, 20, 18, 198, 85, 220, 17, 16, 220, 18, 22, 220, 20, 18, 198, 85, 220, 19, 16, 220, 18, 22, 220, 20, 18, 198, 85, 220, 19, 18, 220, 18, 22, 220, 20, 18, 198, 85, 220, 19, 20, 220, 18, 22, 220, 20, 18, 198, 85, 220, 16, 22, 220, 18, 23, 220, 20, 18, 198, 85, 220, 19, 20, 220, 18, 23, 220, 20, 18, 198, 85, 220, 16, 22, 220, 19, 21, 220, 20, 18, 198, 85, 220, 17, 18, 220, 19, 21, 220, 20, 18, 198, 85, 220, 16, 24, 220, 16, 21, 220, 20, 19, 198, 85, 220, 17, 16, 220, 16, 21, 220, 20, 19, 198, 85, 220, 19, 16, 220, 16, 21, 220, 20, 19, 198, 85, 220, 19, 18, 220, 16, 21, 220, 20, 19, 198, 85, 220, 17, 16, 220, 17, 20, 220, 20, 19, 198, 85, 220, 19, 16, 220, 17, 20, 220, 20, 19, 198, 85, 220, 17, 16, 220, 17, 22, 220, 20, 19, 198, 85, 220, 19, 16, 220, 17, 22, 220, 20, 19, 198, 85, 220, 16, 24, 220, 18, 22, 220, 20, 19, 198, 85, 220, 17, 16, 220, 18, 22, 220, 20, 19, 198, 85, 220, 19, 16, 220, 18, 22, 220, 20, 19, 198, 85, 220, 19, 18, 220, 18, 22, 220, 20, 19, 198, 85, 220, 16, 22, 220, 18, 22, 220, 20, 20, 198, 85, 220, 19, 20, 220, 18, 22, 220, 20, 20, 198, 85, 220, 16, 22, 220, 18, 23, 220, 20, 20, 198, 85, 220, 19, 20, 220, 18, 23, 220, 20, 20, 198, 69, 220, 16, 220, 17, 220, 19, 198, 69, 220, 16, 220, 18, 21, 220, 17, 198, 69, 220, 16, 220, 19, 220, 18, 198, 69, 220, 16, 220, 18, 220, 18, 21, 198, 69, 220, 17, 220, 19, 18, 220, 19, 198, 69, 220, 17, 220, 18, 21, 220, 19, 16, 198, 69, 220, 17, 220, 19, 16, 220, 19, 18, 198, 69, 220, 18, 220, 19, 220, 19, 17, 198, 69, 220, 18, 220, 19, 17, 220, 18, 21, 198, 69, 220, 19, 220, 19, 18, 220, 19, 17, 198, 69, 220, 20, 220, 21, 220, 16, 18, 198, 69, 220, 20, 220, 16, 22, 220, 21, 198, 69, 220, 20, 220, 16, 18, 220, 18, 22, 198, 69, 220, 20, 220, 18, 22, 220, 16, 22, 198, 69, 220, 21, 220, 16, 19, 220, 16, 18, 198, 69, 220, 21, 220, 16, 23, 220, 16, 19, 198, 69, 220, 21, 220, 16, 22, 220, 16, 23, 198, 69, 220, 22, 220, 23, 220, 16, 20, 198, 69, 220, 22, 220, 16, 24, 220, 23, 198, 69, 220, 22, 220, 16, 20, 220, 18, 24, 198, 69, 220, 22, 220, 18, 24, 220, 16, 24, 198, 69, 220, 23, 220, 16, 21, 220, 16, 20, 198, 69, 220, 23, 220, 17, 15, 220, 16, 21, 198, 69, 220, 23, 220, 16, 24, 220, 17, 15, 198, 69, 220, 24, 220, 16, 15, 220, 16, 16, 198, 69, 220, 24, 220, 17, 22, 220, 16, 15, 198, 69, 220, 24, 220, 16, 16, 220, 18, 16, 198, 69, 220, 24, 220, 18, 16, 220, 17, 22, 198, 69, 220, 16, 15, 220, 16, 17, 220, 16, 16, 198, 69, 220, 16, 15, 220, 17, 23, 220, 16, 17, 198, 69, 220, 16, 15, 220, 17, 22, 220, 17, 23, 198, 69, 220, 16, 16, 220, 16, 17, 220, 18, 17, 198, 69, 220, 16, 16, 220, 18, 17, 220, 18, 16, 198, 69, 220, 16, 17, 220, 17, 23, 220, 18, 17, 198, 69, 220, 16, 18, 220, 16, 19, 220, 18, 23, 198, 69, 220, 16, 18, 220, 18, 23, 220, 18, 22, 198, 69, 220, 16, 19, 220, 16, 23, 220, 18, 23, 198, 69, 220, 16, 20, 220, 16, 21, 220, 19, 15, 198, 69, 220, 16, 20, 220, 19, 15, 220, 18, 24, 198, 69, 220, 16, 21, 220, 17, 15, 220, 19, 15, 198, 69, 220, 16, 22, 220, 18, 22, 220, 16, 23, 198, 69, 220, 16, 23, 220, 18, 22, 220, 18, 23, 198, 69, 220, 16, 24, 220, 18, 24, 220, 17, 15, 198, 69, 220, 17, 15, 220, 18, 24, 220, 19, 15, 198, 69, 220, 17, 16, 220, 17, 17, 220, 17, 21, 198, 69, 220, 17, 16, 220, 20, 16, 220, 17, 17, 198, 69, 220, 17, 16, 220, 17, 21, 220, 20, 15, 198, 69, 220, 17, 16, 220, 20, 15, 220, 20, 16, 198, 69, 220, 17, 17, 220, 17, 22, 220, 17, 21, 198, 69, 220, 17, 17, 220, 20, 16, 220, 17, 22, 198, 69, 220, 17, 18, 220, 17, 19, 220, 17, 23, 198, 69, 220, 17, 18, 220, 20, 18, 220, 17, 19, 198, 69, 220, 17, 18, 220, 17, 23, 220, 20, 17, 198, 69, 220, 17, 18, 220, 20, 17, 220, 20, 18, 198, 69, 220, 17, 19, 220, 17, 24, 220, 17, 23, 198, 69, 220, 17, 19, 220, 20, 18, 220, 17, 24, 198, 69, 220, 17, 20, 220, 18, 15, 220, 18, 20, 198, 69, 220, 17, 20, 220, 20, 19, 220, 18, 15, 198, 69, 220, 17, 20, 220, 18, 20, 220, 18, 18, 198, 69, 220, 17, 20, 220, 18, 18, 220, 20, 19, 198, 69, 220, 17, 21, 220, 17, 22, 220, 20, 20, 198, 69, 220, 17, 21, 220, 20, 20, 220, 20, 15, 198, 69, 220, 17, 22, 220, 18, 16, 220, 17, 23, 198, 69, 220, 17, 22, 220, 20, 16, 220, 20, 21, 198, 69, 220, 17, 22, 220, 20, 21, 220, 20, 20, 198, 69, 220, 17, 23, 220, 17, 24, 220, 20, 22, 198, 69, 220, 17, 23, 220, 18, 16, 220, 18, 17, 198, 69, 220, 17, 23, 220, 20, 22, 220, 20, 17, 198, 69, 220, 17, 24, 220, 20, 18, 220, 20, 23, 198, 69, 220, 17, 24, 220, 20, 23, 220, 20, 22, 198, 69, 220, 18, 15, 220, 21, 19, 220, 18, 20, 198, 69, 220, 18, 15, 220, 20, 19, 220, 20, 24, 198, 69, 220, 18, 15, 220, 20, 24, 220, 21, 19, 198, 69, 220, 18, 18, 220, 18, 19, 220, 19, 20, 198, 69, 220, 18, 18, 220, 21, 17, 220, 18, 19, 198, 69, 220, 18, 18, 220, 18, 20, 220, 21, 17, 198, 69, 220, 18, 18, 220, 19, 20, 220, 19, 19, 198, 69, 220, 18, 18, 220, 19, 19, 220, 21, 17, 198, 69, 220, 18, 18, 220, 21, 17, 220, 20, 19, 198, 69, 220, 18, 19, 220, 22, 19, 220, 19, 20, 198, 69, 220, 18, 19, 220, 21, 17, 220, 21, 18, 198, 69, 220, 18, 19, 220, 21, 18, 220, 22, 19, 198, 69, 220, 18, 20, 220, 21, 19, 220, 21, 17, 198, 69, 220, 18, 21, 220, 19, 17, 220, 19, 16, 198, 69, 220, 19, 16, 220, 19, 17, 220, 19, 18, 198, 69, 220, 19, 19, 220, 19, 20, 220, 22, 18, 198, 69, 220, 19, 19, 220, 22, 18, 220, 21, 17, 198, 69, 220, 19, 20, 220, 22, 19, 220, 22, 18, 198, 69, 220, 19, 21, 220, 19, 22, 220, 21, 21, 198, 69, 220, 19, 21, 220, 22, 20, 220, 19, 22, 198, 69, 220, 19, 21, 220, 21, 21, 220, 23, 18, 198, 69, 220, 19, 21, 220, 23, 18, 220, 22, 20, 198, 69, 220, 19, 22, 220, 21, 22, 220, 21, 21, 198, 69, 220, 19, 22, 220, 22, 21, 220, 21, 22, 198, 69, 220, 19, 22, 220, 22, 20, 220, 22, 21, 198, 69, 220, 19, 23, 220, 19, 24, 220, 21, 23, 198, 69, 220, 19, 23, 220, 22, 22, 220, 19, 24, 198, 69, 220, 19, 23, 220, 21, 23, 220, 23, 20, 198, 69, 220, 19, 23, 220, 23, 20, 220, 22, 22, 198, 69, 220, 19, 24, 220, 21, 24, 220, 21, 23, 198, 69, 220, 19, 24, 220, 22, 23, 220, 21, 24, 198, 69, 220, 19, 24, 220, 22, 22, 220, 22, 23, 198, 69, 220, 20, 15, 220, 20, 21, 220, 20, 16, 198, 69, 220, 20, 15, 220, 20, 20, 220, 20, 21, 198, 69, 220, 20, 17, 220, 20, 23, 220, 20, 18, 198, 69, 220, 20, 17, 220, 20, 22, 220, 20, 23, 198, 69, 220, 20, 19, 220, 21, 17, 220, 20, 24, 198, 69, 220, 20, 21, 220, 20, 22, 220, 21, 15, 198, 69, 220, 20, 21, 220, 22, 24, 220, 20, 22, 198, 69, 220, 20, 21, 220, 21, 15, 220, 23, 16, 198, 69, 220, 20, 21, 220, 23, 16, 220, 22, 24, 198, 69, 220, 20, 22, 220, 21, 16, 220, 21, 15, 198, 69, 220, 20, 22, 220, 23, 15, 220, 21, 16, 198, 69, 220, 20, 22, 220, 22, 24, 220, 23, 15, 198, 69, 220, 20, 24, 220, 21, 17, 220, 21, 19, 198, 69, 220, 21, 15, 220, 21, 16, 220, 23, 17, 198, 69, 220, 21, 15, 220, 23, 17, 220, 23, 16, 198, 69, 220, 21, 16, 220, 23, 15, 220, 23, 17, 198, 69, 220, 21, 17, 220, 22, 18, 220, 21, 18, 198, 69, 220, 21, 18, 220, 22, 18, 220, 22, 19, 198, 69, 220, 21, 20, 220, 22, 15, 220, 22, 17, 198, 69, 220, 21, 20, 220, 23, 22, 220, 22, 15, 198, 69, 220, 21, 20, 220, 22, 17, 220, 22, 16, 198, 69, 220, 21, 20, 220, 22, 16, 220, 23, 22, 198, 69, 220, 21, 21, 220, 21, 22, 220, 23, 19, 198, 69, 220, 21, 21, 220, 23, 19, 220, 23, 18, 198, 69, 220, 21, 22, 220, 22, 21, 220, 23, 19, 198, 69, 220, 21, 23, 220, 21, 24, 220, 23, 21, 198, 69, 220, 21, 23, 220, 23, 21, 220, 23, 20, 198, 69, 220, 21, 24, 220, 22, 23, 220, 23, 21, 198, 69, 220, 22, 15, 220, 24, 15, 220, 22, 17, 198, 69, 220, 22, 15, 220, 23, 22, 220, 23, 23, 198, 69, 220, 22, 15, 220, 23, 23, 220, 24, 15, 198, 69, 220, 22, 16, 220, 22, 17, 220, 23, 24, 198, 69, 220, 22, 16, 220, 23, 24, 220, 23, 22, 198, 69, 220, 22, 17, 220, 24, 15, 220, 23, 24, 198, 69, 220, 22, 20, 220, 23, 18, 220, 22, 21, 198, 69, 220, 22, 21, 220, 23, 18, 220, 23, 19, 198, 69, 220, 22, 22, 220, 23, 20, 220, 22, 23, 198, 69, 220, 22, 23, 220, 23, 20, 220, 23, 21, 198, 69, 220, 22, 24, 220, 23, 16, 220, 23, 15, 198, 69, 220, 23, 15, 220, 23, 16, 220, 23, 17, 198, 69, 220, 23, 22, 220, 23, 24, 220, 23, 23, 198, 69, 220, 23, 23, 220, 23, 24, 220, 24, 15, 198, 151645, 198]\n",
      "labels:\n",
      "v 17 37 7\n",
      "v 45 37 7\n",
      "v 17 38 7\n",
      "v 45 38 7\n",
      "v 19 16 8\n",
      "v 21 16 8\n",
      "v 41 16 8\n",
      "v 43 16 8\n",
      "v 21 25 8\n",
      "v 41 25 8\n",
      "v 21 27 8\n",
      "v 41 27 8\n",
      "v 19 37 8\n",
      "v 21 37 8\n",
      "v 41 37 8\n",
      "v 43 37 8\n",
      "v 19 16 9\n",
      "v 21 16 9\n",
      "v 41 16 9\n",
      "v 43 16 9\n",
      "v 20 22 9\n",
      "v 21 22 9\n",
      "v 41 22 9\n",
      "v 42 22 9\n",
      "v 17 25 9\n",
      "v 20 25 9\n",
      "v 21 25 9\n",
      "v 41 25 9\n",
      "v 42 25 9\n",
      "v 45 25 9\n",
      "v 21 27 9\n",
      "v 41 27 9\n",
      "v 17 30 9\n",
      "v 23 30 9\n",
      "v 45 30 9\n",
      "v 17 37 9\n",
      "v 19 37 9\n",
      "v 21 37 9\n",
      "v 41 37 9\n",
      "v 43 37 9\n",
      "v 45 37 9\n",
      "v 17 38 9\n",
      "v 45 38 9\n",
      "v 17 46 9\n",
      "v 23 46 9\n",
      "v 19 16 53\n",
      "v 21 16 53\n",
      "v 41 16 53\n",
      "v 43 16 53\n",
      "v 20 22 53\n",
      "v 21 22 53\n",
      "v 41 22 53\n",
      "v 42 22 53\n",
      "v 17 25 53\n",
      "v 20 25 53\n",
      "v 21 25 53\n",
      "v 41 25 53\n",
      "v 42 25 53\n",
      "v 45 25 53\n",
      "v 21 27 53\n",
      "v 41 27 53\n",
      "v 17 30 53\n",
      "v 23 30 53\n",
      "v 45 30 53\n",
      "v 17 37 53\n",
      "v 19 37 53\n",
      "v 21 37 53\n",
      "v 41 37 53\n",
      "v 43 37 53\n",
      "v 45 37 53\n",
      "v 17 38 53\n",
      "v 45 38 53\n",
      "v 17 46 53\n",
      "v 23 46 53\n",
      "v 19 16 54\n",
      "v 21 16 54\n",
      "v 41 16 54\n",
      "v 43 16 54\n",
      "v 21 25 54\n",
      "v 41 25 54\n",
      "v 21 27 54\n",
      "v 41 27 54\n",
      "v 19 37 54\n",
      "v 21 37 54\n",
      "v 41 37 54\n",
      "v 43 37 54\n",
      "v 17 37 55\n",
      "v 45 37 55\n",
      "v 17 38 55\n",
      "v 45 38 55\n",
      "f 1 2 4\n",
      "f 1 36 2\n",
      "f 1 4 3\n",
      "f 1 3 36\n",
      "f 2 43 4\n",
      "f 2 36 41\n",
      "f 2 41 43\n",
      "f 3 4 42\n",
      "f 3 42 36\n",
      "f 4 43 42\n",
      "f 5 6 13\n",
      "f 5 17 6\n",
      "f 5 13 37\n",
      "f 5 37 17\n",
      "f 6 14 13\n",
      "f 6 18 14\n",
      "f 6 17 18\n",
      "f 7 8 15\n",
      "f 7 19 8\n",
      "f 7 15 39\n",
      "f 7 39 19\n",
      "f 8 16 15\n",
      "f 8 20 16\n",
      "f 8 19 20\n",
      "f 9 10 11\n",
      "f 9 27 10\n",
      "f 9 11 31\n",
      "f 9 31 27\n",
      "f 10 12 11\n",
      "f 10 28 12\n",
      "f 10 27 28\n",
      "f 11 12 32\n",
      "f 11 32 31\n",
      "f 12 28 32\n",
      "f 13 14 38\n",
      "f 13 38 37\n",
      "f 14 18 38\n",
      "f 15 16 40\n",
      "f 15 40 39\n",
      "f 16 20 40\n",
      "f 17 37 18\n",
      "f 18 37 38\n",
      "f 19 39 20\n",
      "f 20 39 40\n",
      "f 21 22 26\n",
      "f 21 51 22\n",
      "f 21 26 50\n",
      "f 21 50 51\n",
      "f 22 27 26\n",
      "f 22 51 27\n",
      "f 23 24 28\n",
      "f 23 53 24\n",
      "f 23 28 52\n",
      "f 23 52 53\n",
      "f 24 29 28\n",
      "f 24 53 29\n",
      "f 25 30 35\n",
      "f 25 54 30\n",
      "f 25 35 33\n",
      "f 25 33 54\n",
      "f 26 27 55\n",
      "f 26 55 50\n",
      "f 27 31 28\n",
      "f 27 51 56\n",
      "f 27 56 55\n",
      "f 28 29 57\n",
      "f 28 31 32\n",
      "f 28 57 52\n",
      "f 29 53 58\n",
      "f 29 58 57\n",
      "f 30 64 35\n",
      "f 30 54 59\n",
      "f 30 59 64\n",
      "f 33 34 45\n",
      "f 33 62 34\n",
      "f 33 35 62\n",
      "f 33 45 44\n",
      "f 33 44 62\n",
      "f 33 62 54\n",
      "f 34 74 45\n",
      "f 34 62 63\n",
      "f 34 63 74\n",
      "f 35 64 62\n",
      "f 36 42 41\n",
      "f 41 42 43\n",
      "f 44 45 73\n",
      "f 44 73 62\n",
      "f 45 74 73\n",
      "f 46 47 66\n",
      "f 46 75 47\n",
      "f 46 66 83\n",
      "f 46 83 75\n",
      "f 47 67 66\n",
      "f 47 76 67\n",
      "f 47 75 76\n",
      "f 48 49 68\n",
      "f 48 77 49\n",
      "f 48 68 85\n",
      "f 48 85 77\n",
      "f 49 69 68\n",
      "f 49 78 69\n",
      "f 49 77 78\n",
      "f 50 56 51\n",
      "f 50 55 56\n",
      "f 52 58 53\n",
      "f 52 57 58\n",
      "f 54 62 59\n",
      "f 56 57 60\n",
      "f 56 79 57\n",
      "f 56 60 81\n",
      "f 56 81 79\n",
      "f 57 61 60\n",
      "f 57 80 61\n",
      "f 57 79 80\n",
      "f 59 62 64\n",
      "f 60 61 82\n",
      "f 60 82 81\n",
      "f 61 80 82\n",
      "f 62 73 63\n",
      "f 63 73 74\n",
      "f 65 70 72\n",
      "f 65 87 70\n",
      "f 65 72 71\n",
      "f 65 71 87\n",
      "f 66 67 84\n",
      "f 66 84 83\n",
      "f 67 76 84\n",
      "f 68 69 86\n",
      "f 68 86 85\n",
      "f 69 78 86\n",
      "f 70 90 72\n",
      "f 70 87 88\n",
      "f 70 88 90\n",
      "f 71 72 89\n",
      "f 71 89 87\n",
      "f 72 90 89\n",
      "f 75 83 76\n",
      "f 76 83 84\n",
      "f 77 85 78\n",
      "f 78 85 86\n",
      "f 79 81 80\n",
      "f 80 81 82\n",
      "f 87 89 88\n",
      "f 88 89 90\n",
      "<|im_end|>\n",
      "\n",
      "[INFO|configuration_utils.py:691] 2025-04-30 19:08:27,671 >> loading configuration file /home/featurize/work/LLaMA-Factory-4-2/A_myModels/Qwen2.5-1.5B-Instruct-bin/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-30 19:08:27,672 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|2025-04-30 19:08:27] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "[INFO|modeling_utils.py:1121] 2025-04-30 19:08:27,720 >> loading weights file /home/featurize/work/LLaMA-Factory-4-2/A_myModels/Qwen2.5-1.5B-Instruct-bin/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2167] 2025-04-30 19:08:27,720 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-04-30 19:08:27,722 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4930] 2025-04-30 19:10:51,624 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4938] 2025-04-30 19:10:51,625 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/featurize/work/LLaMA-Factory-4-2/A_myModels/Qwen2.5-1.5B-Instruct-bin.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-04-30 19:10:51,628 >> loading configuration file /home/featurize/work/LLaMA-Factory-4-2/A_myModels/Qwen2.5-1.5B-Instruct-bin/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-04-30 19:10:51,629 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO|2025-04-30 19:10:51] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-04-30 19:10:51] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-04-30 19:10:51] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-04-30 19:10:51] llamafactory.model.adapter:143 >> Fine-tuning method: Full\n",
      "[INFO|2025-04-30 19:10:51] llamafactory.model.loader:143 >> trainable params: 1,543,714,304 || all params: 1,543,714,304 || trainable%: 100.0000\n",
      "[INFO|trainer.py:748] 2025-04-30 19:10:51,659 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2414] 2025-04-30 19:10:51,992 >> ***** Running training *****\n",
      "[INFO|trainer.py:2415] 2025-04-30 19:10:51,993 >>   Num examples = 510\n",
      "[INFO|trainer.py:2416] 2025-04-30 19:10:51,993 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2417] 2025-04-30 19:10:51,993 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2420] 2025-04-30 19:10:51,993 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2421] 2025-04-30 19:10:51,993 >>   Gradient Accumulation steps = 16\n",
      "[INFO|trainer.py:2422] 2025-04-30 19:10:51,993 >>   Total optimization steps = 31\n",
      "[INFO|trainer.py:2423] 2025-04-30 19:10:51,994 >>   Number of trainable parameters = 1,543,714,304\n",
      " 16%|███████                                     | 5/31 [00:40<03:32,  8.17s/it][INFO|2025-04-30 19:11:32] llamafactory.train.callbacks:143 >> {'loss': 0.1305, 'learning_rate': 9.5948e-06, 'epoch': 0.16, 'throughput': 3003.10}\n",
      "{'loss': 0.1305, 'grad_norm': 1.3207899332046509, 'learning_rate': 9.594789058101154e-06, 'epoch': 0.16, 'num_input_tokens_seen': 121736}\n",
      " 32%|█████████████▊                             | 10/31 [01:27<03:18,  9.44s/it][INFO|2025-04-30 19:12:19] llamafactory.train.callbacks:143 >> {'loss': 0.1242, 'learning_rate': 8.0605e-06, 'epoch': 0.31, 'throughput': 2979.68}\n",
      "{'loss': 0.1242, 'grad_norm': 0.4560621678829193, 'learning_rate': 8.060529912738316e-06, 'epoch': 0.31, 'num_input_tokens_seen': 261864}\n",
      " 48%|████████████████████▊                      | 15/31 [02:10<02:16,  8.53s/it][INFO|2025-04-30 19:13:02] llamafactory.train.callbacks:143 >> {'loss': 0.1175, 'learning_rate': 5.7571e-06, 'epoch': 0.47, 'throughput': 2978.50}\n",
      "{'loss': 0.1175, 'grad_norm': 0.7075362801551819, 'learning_rate': 5.757138887522884e-06, 'epoch': 0.47, 'num_input_tokens_seen': 388584}\n",
      " 65%|███████████████████████████▋               | 20/31 [02:56<01:39,  9.02s/it][INFO|2025-04-30 19:13:49] llamafactory.train.callbacks:143 >> {'loss': 0.1361, 'learning_rate': 3.2635e-06, 'epoch': 0.63, 'throughput': 2996.00}\n",
      "{'loss': 0.1361, 'grad_norm': 0.5868704915046692, 'learning_rate': 3.2634737357758994e-06, 'epoch': 0.63, 'num_input_tokens_seen': 530712}\n",
      " 65%|███████████████████████████▋               | 20/31 [02:57<01:39,  9.02s/it][INFO|trainer.py:3984] 2025-04-30 19:13:49,137 >> Saving model checkpoint to saves/temp/checkpoint-20\n",
      "[INFO|configuration_utils.py:419] 2025-04-30 19:13:49,141 >> Configuration saved in saves/temp/checkpoint-20/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-04-30 19:13:49,144 >> Configuration saved in saves/temp/checkpoint-20/generation_config.json\n",
      "[INFO|modeling_utils.py:3580] 2025-04-30 19:17:51,732 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at saves/temp/checkpoint-20/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-30 19:17:51,736 >> tokenizer config file saved in saves/temp/checkpoint-20/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-30 19:17:51,747 >> Special tokens file saved in saves/temp/checkpoint-20/special_tokens_map.json\n",
      " 81%|██████████████████████████████████▋        | 25/31 [12:52<04:50, 48.42s/it][INFO|2025-04-30 19:23:44] llamafactory.train.callbacks:143 >> {'loss': 0.1358, 'learning_rate': 1.2062e-06, 'epoch': 0.78, 'throughput': 857.20}\n",
      "{'loss': 0.1358, 'grad_norm': 0.5180733799934387, 'learning_rate': 1.2062093865360458e-06, 'epoch': 0.78, 'num_input_tokens_seen': 662360}\n",
      " 97%|█████████████████████████████████████████▌ | 30/31 [13:36<00:15, 15.35s/it][INFO|2025-04-30 19:24:28] llamafactory.train.callbacks:143 >> {'loss': 0.1243, 'learning_rate': 1.0235e-07, 'epoch': 0.94, 'throughput': 972.03}\n",
      "{'loss': 0.1243, 'grad_norm': 0.5047882199287415, 'learning_rate': 1.0235029373752758e-07, 'epoch': 0.94, 'num_input_tokens_seen': 793656}\n",
      "100%|███████████████████████████████████████████| 31/31 [13:45<00:00, 13.54s/it][INFO|trainer.py:3984] 2025-04-30 19:24:37,634 >> Saving model checkpoint to saves/temp/checkpoint-31\n",
      "[INFO|configuration_utils.py:419] 2025-04-30 19:24:37,638 >> Configuration saved in saves/temp/checkpoint-31/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-04-30 19:24:37,640 >> Configuration saved in saves/temp/checkpoint-31/generation_config.json\n",
      "[INFO|modeling_utils.py:3580] 2025-04-30 19:28:42,275 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at saves/temp/checkpoint-31/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-30 19:28:42,483 >> tokenizer config file saved in saves/temp/checkpoint-31/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-30 19:28:42,485 >> Special tokens file saved in saves/temp/checkpoint-31/special_tokens_map.json\n",
      "[INFO|trainer.py:2681] 2025-04-30 19:33:45,598 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1373.6243, 'train_samples_per_second': 0.371, 'train_steps_per_second': 0.023, 'train_loss': 0.12768903903422818, 'epoch': 0.97, 'num_input_tokens_seen': 821696}\n",
      "100%|███████████████████████████████████████████| 31/31 [22:53<00:00, 44.31s/it]\n",
      "[INFO|trainer.py:3984] 2025-04-30 19:33:45,641 >> Saving model checkpoint to saves/temp\n",
      "[INFO|configuration_utils.py:419] 2025-04-30 19:33:45,646 >> Configuration saved in saves/temp/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-04-30 19:33:45,649 >> Configuration saved in saves/temp/generation_config.json\n",
      "[INFO|modeling_utils.py:3580] 2025-04-30 19:37:46,535 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at saves/temp/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-30 19:37:46,537 >> tokenizer config file saved in saves/temp/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-30 19:37:46,539 >> Special tokens file saved in saves/temp/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     0.9725\n",
      "  num_input_tokens_seen    =     821696\n",
      "  total_flos               =  6016539GF\n",
      "  train_loss               =     0.1277\n",
      "  train_runtime            = 0:22:53.62\n",
      "  train_samples_per_second =      0.371\n",
      "  train_steps_per_second   =      0.023\n",
      "Figure saved at: saves/temp/training_loss.png\n",
      "[WARNING|2025-04-30 19:37:47] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-04-30 19:37:47] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:450] 2025-04-30 19:37:47,617 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "[WARNING|2025-04-30 19:43:56] llamafactory.webui.common:148 >> Found complex path, some features may be not available.\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:56,698 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:56,698 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:56,698 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:56,698 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:56,698 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:56,698 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:56,698 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-30 19:43:57,197 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-04-30 19:43:57,199 >> loading configuration file saves/temp/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-30 19:43:57,201 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:57,203 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:57,203 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:57,203 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:57,203 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:57,203 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:57,203 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-04-30 19:43:57,203 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-30 19:43:57,692 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-04-30 19:43:57,763 >> loading configuration file saves/temp/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-04-30 19:43:57,766 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|2025-04-30 19:43:57] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|modeling_utils.py:1121] 2025-04-30 19:43:57,810 >> loading weights file saves/temp/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-04-30 19:43:57,811 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1142] 2025-04-30 19:43:57,812 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 715, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1675, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/gradio/utils.py\", line 735, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/gradio/utils.py\", line 729, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/gradio/utils.py\", line 712, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/gradio/utils.py\", line 873, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/home/featurize/work/LLaMA-Factory/src/llamafactory/webui/chatter.py\", line 144, in load_model\n",
      "    super().__init__(args)\n",
      "  File \"/home/featurize/work/LLaMA-Factory/src/llamafactory/chat/chat_model.py\", line 53, in __init__\n",
      "    self.engine: BaseEngine = HuggingfaceEngine(model_args, data_args, finetuning_args, generating_args)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/featurize/work/LLaMA-Factory/src/llamafactory/chat/hf_engine.py\", line 59, in __init__\n",
      "    self.model = load_model(\n",
      "                 ^^^^^^^^^^^\n",
      "  File \"/home/featurize/work/LLaMA-Factory/src/llamafactory/model/loader.py\", line 167, in load_model\n",
      "    model = load_class.from_pretrained(**init_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 571, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 279, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4399, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4822, in _load_pretrained_model\n",
      "    state_dict = load_state_dict(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 513, in load_state_dict\n",
      "    with safe_open(checkpoint_file, framework=\"pt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a268057-5794-4ef8-af68-7af483effeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/featurize/work/LLaMA-Factory-4-2/A_myModels/llamameshchange\n",
    "/home/featurize/work/LLaMA-Factory-4-2/saves/Llama-3.1-8B-Instruct/lora/sft/429/checkpoint-600\n",
    "create a 3D model of a table in obj file\n",
    "/home/featurize/work/LLaMA-Factory-4-2/saves/Qwen2.5-1.5B-Instruct/full/221_v3\n",
    "/home/featurize/work/LLaMA-Factory-4-2/A_myModels/Qwen2.5-1.5B-Instruct-bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ebb0e-8004-4fc2-b835-616d14ebb4ce",
   "metadata": {},
   "source": [
    "## LoRA 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6045cb36-a3fa-4029-9de8-bcbd322a77f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-16 09:16:42.802060: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-16 09:16:42.814480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739697402.828205   35886 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739697402.832422   35886 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-16 09:16:42.847569: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[INFO|2025-02-16 09:16:47] llamafactory.hparams.parser:386 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 09:16:47,719 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-16 09:16:47,721 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:47,996 >> loading file tokenizer.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:47,996 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:47,997 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:47,997 >> loading file special_tokens_map.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:47,997 >> loading file tokenizer_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:47,997 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-02-16 09:16:48,434 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 09:16:49,583 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-16 09:16:49,585 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:49,854 >> loading file tokenizer.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:49,854 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:49,854 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:49,854 >> loading file special_tokens_map.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:49,854 >> loading file tokenizer_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-16 09:16:49,854 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-02-16 09:16:50,303 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-02-16 09:16:50] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-02-16 09:16:50] llamafactory.data.template:157 >> Add <|eot_id|> to stop words.\n",
      "[INFO|2025-02-16 09:16:50] llamafactory.data.loader:157 >> Loading dataset /home/featurize/data/data_sharegpt/data_sharegpt_7k.json...\n",
      "Converting format of dataset (num_proc=16): 100%|█| 47156/47156 [00:00<00:00, 76\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 47156/47156 [00:18<00:00, 25\n",
      "training example:\n",
      "input_ids:\n",
      "[128006, 9125, 128007, 271, 2675, 527, 264, 11190, 220, 18, 35, 11546, 34579, 15592, 18328, 13, 128009, 128006, 882, 128007, 271, 4110, 264, 220, 18, 35, 2909, 1052, 1701, 279, 2768, 4096, 25, 264, 9501, 18618, 2971, 425, 449, 264, 6437, 12708, 389, 279, 6134, 13, 128009, 128006, 78191, 128007, 271, 85, 220, 5547, 220, 17, 220, 16, 198, 85, 220, 4331, 220, 17, 220, 16, 198, 85, 220, 2096, 220, 2137, 220, 16, 198, 85, 220, 2096, 220, 972, 220, 16, 198, 85, 220, 966, 220, 1758, 220, 16, 198, 85, 220, 966, 220, 1691, 220, 16, 198, 85, 220, 2491, 220, 1135, 220, 16, 198, 85, 220, 5547, 220, 1135, 220, 16, 198, 85, 220, 17, 220, 15, 220, 16, 198, 85, 220, 5547, 220, 15, 220, 16, 198, 85, 220, 5547, 220, 16, 220, 16, 198, 85, 220, 4331, 220, 16, 220, 16, 198, 85, 220, 3391, 220, 972, 220, 16, 198, 85, 220, 966, 220, 508, 220, 16, 198, 85, 220, 17, 220, 508, 220, 16, 198, 85, 220, 5547, 220, 17, 220, 16, 198, 85, 220, 5547, 220, 1135, 220, 16, 198, 85, 220, 2491, 220, 1135, 220, 16, 198, 85, 220, 2096, 220, 2137, 220, 16, 198, 85, 220, 4331, 220, 17, 220, 16, 198, 85, 220, 17, 220, 4103, 220, 16, 198, 85, 220, 17, 220, 1927, 220, 16, 198, 85, 220, 966, 220, 1927, 220, 16, 198, 85, 220, 2096, 220, 2137, 220, 16, 198, 85, 220, 2166, 220, 1135, 220, 16, 198, 85, 220, 5547, 220, 3971, 220, 16, 198, 85, 220, 5547, 220, 4103, 220, 16, 198, 85, 220, 17, 220, 15, 220, 16, 198, 85, 220, 17, 220, 508, 220, 16, 198, 85, 220, 966, 220, 508, 220, 16, 198, 85, 220, 3391, 220, 972, 220, 16, 198, 85, 220, 4331, 220, 16, 220, 16, 198, 85, 220, 5547, 220, 16, 220, 16, 198, 85, 220, 5547, 220, 15, 220, 16, 198, 85, 220, 17, 220, 4103, 220, 16, 198, 85, 220, 5547, 220, 4103, 220, 16, 198, 85, 220, 5547, 220, 3971, 220, 16, 198, 85, 220, 2166, 220, 1135, 220, 16, 198, 85, 220, 2096, 220, 2137, 220, 16, 198, 85, 220, 966, 220, 1927, 220, 16, 198, 85, 220, 17, 220, 1927, 220, 16, 198, 85, 220, 2096, 220, 972, 220, 16, 198, 85, 220, 966, 220, 1758, 220, 16, 198, 85, 220, 966, 220, 1691, 220, 16, 198, 85, 220, 17, 220, 1691, 220, 17, 198, 85, 220, 1682, 220, 1691, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1691, 220, 15, 198, 85, 220, 508, 220, 17, 220, 16, 198, 85, 220, 508, 220, 18, 220, 16, 198, 85, 220, 1419, 220, 18, 220, 17, 198, 85, 220, 1419, 220, 17, 220, 17, 198, 85, 220, 508, 220, 17, 220, 16, 198, 85, 220, 1419, 220, 17, 220, 17, 198, 85, 220, 1419, 220, 18, 220, 17, 198, 85, 220, 508, 220, 18, 220, 16, 198, 85, 220, 806, 220, 1114, 220, 16, 198, 85, 220, 806, 220, 777, 220, 16, 198, 85, 220, 1032, 220, 777, 220, 17, 198, 85, 220, 1032, 220, 1114, 220, 17, 198, 85, 220, 806, 220, 1114, 220, 16, 198, 85, 220, 1032, 220, 1114, 220, 17, 198, 85, 220, 1032, 220, 777, 220, 17, 198, 85, 220, 806, 220, 777, 220, 16, 198, 85, 220, 21, 220, 21, 220, 16, 198, 85, 220, 21, 220, 23, 220, 16, 198, 85, 220, 23, 220, 23, 220, 17, 198, 85, 220, 23, 220, 21, 220, 17, 198, 85, 220, 21, 220, 21, 220, 16, 198, 85, 220, 23, 220, 21, 220, 17, 198, 85, 220, 23, 220, 23, 220, 17, 198, 85, 220, 21, 220, 23, 220, 16, 198, 85, 220, 4331, 220, 3174, 220, 16, 198, 85, 220, 4331, 220, 2983, 220, 16, 198, 85, 220, 2131, 220, 2983, 220, 17, 198, 85, 220, 2131, 220, 3174, 220, 17, 198, 85, 220, 4331, 220, 3174, 220, 16, 198, 85, 220, 2131, 220, 3174, 220, 17, 198, 85, 220, 2131, 220, 2983, 220, 17, 198, 85, 220, 4331, 220, 2983, 220, 16, 198, 85, 220, 2491, 220, 1135, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2166, 220, 1135, 220, 15, 198, 85, 220, 2166, 220, 1135, 220, 15, 198, 85, 220, 5547, 220, 3971, 220, 15, 198, 85, 220, 5547, 220, 1135, 220, 15, 198, 85, 220, 2491, 220, 1135, 220, 15, 198, 85, 220, 17, 220, 1927, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 15, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 966, 220, 1758, 220, 15, 198, 85, 220, 966, 220, 1758, 220, 15, 198, 85, 220, 966, 220, 1691, 220, 15, 198, 85, 220, 1682, 220, 1691, 220, 15, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1691, 220, 15, 198, 85, 220, 966, 220, 508, 220, 15, 198, 85, 220, 17, 220, 508, 220, 15, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 2096, 220, 972, 220, 15, 198, 85, 220, 3391, 220, 972, 220, 15, 198, 85, 220, 966, 220, 508, 220, 15, 198, 85, 220, 966, 220, 1691, 220, 15, 198, 85, 220, 2096, 220, 972, 220, 15, 198, 85, 220, 4331, 220, 17, 220, 15, 198, 85, 220, 4331, 220, 16, 220, 15, 198, 85, 220, 3391, 220, 972, 220, 15, 198, 85, 220, 4331, 220, 17, 220, 15, 198, 85, 220, 5547, 220, 17, 220, 15, 198, 85, 220, 5547, 220, 16, 220, 15, 198, 85, 220, 4331, 220, 16, 220, 15, 198, 85, 220, 2491, 220, 1135, 220, 17, 198, 85, 220, 2166, 220, 1135, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2166, 220, 1135, 220, 17, 198, 85, 220, 2491, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 3971, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 17, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 966, 220, 1927, 220, 17, 198, 85, 220, 966, 220, 1927, 220, 17, 198, 85, 220, 966, 220, 1758, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 966, 220, 1758, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 1682, 220, 1691, 220, 17, 198, 85, 220, 966, 220, 1691, 220, 17, 198, 85, 220, 17, 220, 508, 220, 17, 198, 85, 220, 966, 220, 508, 220, 17, 198, 85, 220, 1682, 220, 1691, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 17, 198, 85, 220, 966, 220, 1691, 220, 17, 198, 85, 220, 966, 220, 508, 220, 17, 198, 85, 220, 3391, 220, 972, 220, 17, 198, 85, 220, 2096, 220, 972, 220, 17, 198, 85, 220, 2096, 220, 972, 220, 17, 198, 85, 220, 3391, 220, 972, 220, 17, 198, 85, 220, 4331, 220, 16, 220, 17, 198, 85, 220, 4331, 220, 17, 220, 17, 198, 85, 220, 4331, 220, 17, 220, 17, 198, 85, 220, 4331, 220, 16, 220, 17, 198, 85, 220, 5547, 220, 16, 220, 17, 198, 85, 220, 5547, 220, 17, 220, 17, 198, 85, 220, 15, 220, 1691, 220, 17, 198, 85, 220, 15, 220, 1691, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 15, 220, 1927, 220, 17, 198, 85, 220, 15, 220, 1927, 220, 15, 198, 85, 220, 15, 220, 1691, 220, 15, 198, 85, 220, 15, 220, 1691, 220, 17, 198, 85, 220, 1227, 220, 15, 220, 17, 198, 85, 220, 1227, 220, 15, 220, 15, 198, 85, 220, 1227, 220, 4103, 220, 15, 198, 85, 220, 1227, 220, 4103, 220, 17, 198, 85, 220, 1227, 220, 4103, 220, 17, 198, 85, 220, 1227, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 1927, 220, 15, 198, 85, 220, 15, 220, 1927, 220, 17, 198, 85, 220, 17, 220, 508, 220, 17, 198, 85, 220, 15, 220, 1691, 220, 17, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 1691, 220, 15, 198, 85, 220, 17, 220, 508, 220, 15, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 15, 220, 1927, 220, 17, 198, 85, 220, 15, 220, 1691, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 15, 220, 1691, 220, 15, 198, 85, 220, 15, 220, 1927, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 1927, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 17, 198, 85, 220, 17, 220, 4103, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 15, 198, 85, 220, 15, 220, 1927, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 17, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 17, 220, 17, 198, 85, 220, 1227, 220, 15, 220, 17, 198, 85, 220, 1227, 220, 4103, 220, 17, 198, 85, 220, 5547, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 1135, 220, 15, 198, 85, 220, 1227, 220, 4103, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 15, 198, 85, 220, 5547, 220, 17, 220, 15, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 4103, 220, 17, 198, 85, 220, 1227, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 17, 220, 4103, 220, 17, 198, 85, 220, 17, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 1227, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 16, 220, 17, 198, 85, 220, 5547, 220, 16, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 17, 220, 508, 220, 15, 198, 85, 220, 17, 220, 508, 220, 17, 198, 85, 220, 17, 220, 508, 220, 17, 198, 85, 220, 17, 220, 508, 220, 15, 198, 85, 220, 966, 220, 508, 220, 15, 198, 85, 220, 966, 220, 508, 220, 17, 198, 85, 220, 966, 220, 508, 220, 17, 198, 85, 220, 966, 220, 508, 220, 15, 198, 85, 220, 3391, 220, 972, 220, 15, 198, 85, 220, 3391, 220, 972, 220, 17, 198, 85, 220, 3391, 220, 972, 220, 17, 198, 85, 220, 3391, 220, 972, 220, 15, 198, 85, 220, 4331, 220, 16, 220, 15, 198, 85, 220, 4331, 220, 16, 220, 17, 198, 85, 220, 4331, 220, 16, 220, 17, 198, 85, 220, 4331, 220, 16, 220, 15, 198, 85, 220, 5547, 220, 16, 220, 15, 198, 85, 220, 5547, 220, 16, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 17, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1691, 220, 15, 198, 85, 220, 1682, 220, 1691, 220, 17, 198, 85, 220, 1682, 220, 1691, 220, 17, 198, 85, 220, 1682, 220, 1691, 220, 15, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 17, 220, 1691, 220, 17, 198, 85, 220, 5547, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 1135, 220, 15, 198, 85, 220, 5547, 220, 17, 220, 15, 198, 85, 220, 5547, 220, 17, 220, 17, 198, 85, 220, 5547, 220, 17, 220, 17, 198, 85, 220, 5547, 220, 17, 220, 15, 198, 85, 220, 4331, 220, 17, 220, 15, 198, 85, 220, 4331, 220, 17, 220, 17, 198, 85, 220, 4331, 220, 17, 220, 17, 198, 85, 220, 4331, 220, 17, 220, 15, 198, 85, 220, 2096, 220, 972, 220, 15, 198, 85, 220, 2096, 220, 972, 220, 17, 198, 85, 220, 2096, 220, 972, 220, 17, 198, 85, 220, 2096, 220, 972, 220, 15, 198, 85, 220, 966, 220, 1691, 220, 15, 198, 85, 220, 966, 220, 1691, 220, 17, 198, 85, 220, 966, 220, 1691, 220, 17, 198, 85, 220, 966, 220, 1691, 220, 15, 198, 85, 220, 966, 220, 1758, 220, 15, 198, 85, 220, 966, 220, 1758, 220, 17, 198, 85, 220, 966, 220, 1758, 220, 17, 198, 85, 220, 966, 220, 1758, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2491, 220, 1135, 220, 15, 198, 85, 220, 2491, 220, 1135, 220, 17, 198, 85, 220, 2491, 220, 1135, 220, 17, 198, 85, 220, 2491, 220, 1135, 220, 15, 198, 85, 220, 5547, 220, 1135, 220, 15, 198, 85, 220, 5547, 220, 1135, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 15, 198, 85, 220, 17, 220, 4103, 220, 15, 198, 85, 220, 17, 220, 4103, 220, 17, 198, 85, 220, 17, 220, 4103, 220, 17, 198, 85, 220, 17, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 4103, 220, 17, 198, 85, 220, 5547, 220, 4103, 220, 17, 198, 85, 220, 5547, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 3971, 220, 15, 198, 85, 220, 5547, 220, 3971, 220, 17, 198, 85, 220, 5547, 220, 3971, 220, 15, 198, 85, 220, 2166, 220, 1135, 220, 15, 198, 85, 220, 2166, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 3971, 220, 17, 198, 85, 220, 2166, 220, 1135, 220, 17, 198, 85, 220, 2166, 220, 1135, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 17, 198, 85, 220, 966, 220, 1927, 220, 17, 198, 85, 220, 966, 220, 1927, 220, 15, 198, 85, 220, 17, 220, 1927, 220, 15, 198, 85, 220, 17, 220, 1927, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 15, 198, 85, 220, 966, 220, 1691, 220, 15, 198, 85, 220, 966, 220, 508, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 966, 220, 508, 220, 17, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 15, 198, 85, 220, 17, 220, 508, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 1227, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 16, 220, 17, 198, 85, 220, 5547, 220, 3971, 220, 17, 198, 85, 220, 5547, 220, 4103, 220, 17, 198, 85, 220, 5547, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 3971, 220, 15, 198, 85, 220, 5547, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 3971, 220, 15, 198, 85, 220, 5547, 220, 16, 220, 17, 198, 85, 220, 5547, 220, 17, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 15, 198, 69, 220, 16, 220, 17, 220, 18, 198, 69, 220, 17, 220, 19, 220, 18, 198, 69, 220, 18, 220, 19, 220, 20, 198, 69, 220, 19, 220, 21, 220, 20, 198, 69, 220, 22, 220, 23, 220, 16, 198, 69, 220, 22, 220, 16, 220, 18, 198, 69, 220, 24, 220, 605, 220, 806, 198, 69, 220, 24, 220, 806, 220, 717, 198, 69, 220, 24, 220, 717, 220, 1032, 198, 69, 220, 24, 220, 1032, 220, 975, 198, 69, 220, 24, 220, 975, 220, 868, 198, 69, 220, 845, 220, 1114, 220, 972, 198, 69, 220, 845, 220, 972, 220, 777, 198, 69, 220, 845, 220, 777, 220, 508, 198, 69, 220, 1691, 220, 1313, 220, 1419, 198, 69, 220, 1691, 220, 1419, 220, 1187, 198, 69, 220, 1691, 220, 1187, 220, 914, 198, 69, 220, 1691, 220, 914, 220, 1627, 198, 69, 220, 1691, 220, 1627, 220, 1544, 198, 69, 220, 1591, 220, 1682, 220, 966, 198, 69, 220, 1591, 220, 966, 220, 2148, 198, 69, 220, 1591, 220, 2148, 220, 843, 198, 69, 220, 1591, 220, 843, 220, 1644, 198, 69, 220, 1591, 220, 1644, 220, 1958, 198, 69, 220, 1758, 220, 1927, 220, 1806, 198, 69, 220, 1758, 220, 1806, 220, 1987, 198, 69, 220, 1758, 220, 1987, 220, 2137, 198, 69, 220, 1758, 220, 2137, 220, 1272, 198, 69, 220, 1758, 220, 1272, 220, 3174, 198, 69, 220, 2983, 220, 508, 220, 777, 198, 69, 220, 2983, 220, 777, 220, 3391, 198, 69, 220, 2983, 220, 3391, 220, 2096, 198, 69, 220, 1774, 220, 2790, 220, 2618, 198, 69, 220, 1774, 220, 2618, 220, 2166, 198, 69, 220, 2491, 220, 1135, 220, 3971, 198, 69, 220, 2491, 220, 3971, 220, 4103, 198, 69, 220, 4331, 220, 4370, 220, 2131, 198, 69, 220, 4331, 220, 2131, 220, 3487, 198, 69, 220, 3226, 220, 2970, 220, 2946, 198, 69, 220, 3226, 220, 2946, 220, 1399, 198, 69, 220, 5547, 220, 5538, 220, 5495, 198, 69, 220, 5547, 220, 5495, 220, 1227, 198, 69, 220, 2397, 220, 2287, 220, 3080, 198, 69, 220, 2397, 220, 3080, 220, 2614, 198, 69, 220, 3076, 220, 2031, 220, 6028, 198, 69, 220, 3076, 220, 6028, 220, 5332, 198, 69, 220, 5958, 220, 5728, 220, 2075, 198, 69, 220, 5958, 220, 2075, 220, 4767, 198, 69, 220, 2813, 220, 2495, 220, 4643, 198, 69, 220, 2813, 220, 4643, 220, 1490, 198, 69, 220, 5932, 220, 6086, 220, 6069, 198, 69, 220, 5932, 220, 6069, 220, 5833, 198, 69, 220, 5313, 220, 4218, 220, 4044, 198, 69, 220, 5313, 220, 4044, 220, 2421, 198, 69, 220, 4578, 220, 1954, 220, 5925, 198, 69, 220, 4578, 220, 5925, 220, 6083, 198, 69, 220, 6365, 220, 6281, 220, 2721, 198, 69, 220, 6365, 220, 2721, 220, 4161, 198, 69, 220, 3534, 220, 3264, 220, 1484, 198, 69, 220, 3534, 220, 1484, 220, 1041, 198, 69, 220, 4645, 220, 4278, 220, 6889, 198, 69, 220, 4645, 220, 6889, 220, 6849, 198, 69, 220, 6550, 220, 7461, 220, 7699, 198, 69, 220, 6550, 220, 7699, 220, 6640, 198, 69, 220, 7743, 220, 5120, 220, 5037, 198, 69, 220, 7743, 220, 5037, 220, 7261, 198, 69, 220, 8190, 220, 8011, 220, 7322, 198, 69, 220, 8190, 220, 7322, 220, 8027, 198, 69, 220, 8546, 220, 8899, 220, 9079, 198, 69, 220, 8546, 220, 9079, 220, 4364, 198, 69, 220, 7994, 220, 8259, 220, 4513, 198, 69, 220, 7994, 220, 4513, 220, 8874, 198, 69, 220, 6549, 220, 9390, 220, 6804, 198, 69, 220, 6549, 220, 6804, 220, 4386, 198, 69, 220, 9748, 220, 5894, 220, 9263, 198, 69, 220, 9748, 220, 9263, 220, 9413, 198, 69, 220, 9423, 220, 9565, 220, 8878, 198, 69, 220, 9423, 220, 8878, 220, 9795, 198, 69, 220, 10148, 220, 10350, 220, 10125, 198, 69, 220, 10148, 220, 10125, 220, 6860, 198, 69, 220, 9335, 220, 10239, 220, 10290, 198, 69, 220, 9335, 220, 10290, 220, 8929, 198, 69, 220, 9591, 220, 10465, 220, 10288, 198, 69, 220, 9591, 220, 10288, 220, 10410, 198, 69, 220, 10161, 220, 3965, 220, 9690, 198, 69, 220, 10161, 220, 9690, 220, 9756, 198, 69, 220, 9800, 220, 10559, 220, 9992, 198, 69, 220, 9800, 220, 9992, 220, 10132, 198, 69, 220, 10895, 220, 11286, 220, 11068, 198, 69, 220, 10895, 220, 11068, 220, 6330, 198, 69, 220, 10718, 220, 10674, 220, 9892, 198, 69, 220, 10718, 220, 9892, 220, 10513, 198, 69, 220, 10680, 220, 11247, 220, 11515, 198, 69, 220, 10680, 220, 11515, 220, 8953, 198, 69, 220, 11739, 220, 8258, 220, 11123, 198, 69, 220, 11739, 220, 11123, 220, 10861, 198, 69, 220, 11908, 220, 11771, 220, 10005, 198, 69, 220, 11908, 220, 10005, 220, 10967, 198, 69, 220, 11242, 220, 11256, 220, 11128, 198, 69, 220, 11242, 220, 11128, 220, 5245, 198, 69, 220, 10562, 220, 10828, 220, 10750, 198, 69, 220, 10562, 220, 10750, 220, 10336, 198, 69, 220, 9741, 220, 9714, 220, 9674, 198, 69, 220, 9741, 220, 9674, 220, 9367, 198, 69, 220, 9378, 220, 7028, 220, 7529, 198, 69, 220, 9378, 220, 7529, 220, 5926, 198, 69, 220, 7285, 220, 6393, 220, 6280, 198, 69, 220, 7285, 220, 6280, 220, 5162, 198, 69, 220, 4468, 220, 3753, 220, 2550, 198, 69, 220, 4468, 220, 2550, 220, 1049, 198, 69, 220, 679, 220, 2366, 220, 9639, 198, 69, 220, 679, 220, 9639, 220, 7854, 198, 69, 220, 10866, 220, 11056, 220, 12060, 198, 69, 220, 10866, 220, 12060, 220, 12171, 198, 69, 220, 12652, 220, 8848, 220, 11483, 198, 69, 220, 12652, 220, 11483, 220, 11227, 198, 69, 220, 11702, 220, 11584, 220, 12112, 198, 69, 220, 11702, 220, 12112, 220, 12463, 198, 69, 220, 13460, 220, 13302, 220, 13762, 198, 69, 220, 13460, 220, 13762, 220, 8610, 198, 69, 220, 9367, 220, 12425, 220, 9716, 198, 69, 220, 9367, 220, 9716, 220, 12533, 198, 69, 220, 10697, 220, 11057, 220, 14057, 198, 69, 220, 10697, 220, 14057, 220, 6330, 198, 69, 220, 14206, 220, 14261, 220, 14378, 198, 69, 220, 14206, 220, 14378, 220, 5245, 198, 69, 220, 9870, 220, 12245, 220, 12338, 198, 69, 220, 9870, 220, 12338, 220, 12994, 198, 69, 220, 11727, 220, 12422, 220, 14087, 198, 69, 220, 11727, 220, 14087, 220, 14590, 198, 69, 220, 11908, 220, 13895, 220, 11123, 198, 69, 220, 11908, 220, 11123, 220, 14815, 198, 69, 220, 8273, 220, 13341, 220, 12754, 198, 69, 220, 8273, 220, 12754, 220, 14052, 198, 69, 220, 13719, 220, 13078, 220, 14205, 198, 69, 220, 13719, 220, 14205, 220, 14125, 198, 69, 220, 14185, 220, 14735, 220, 5154, 198, 69, 220, 14185, 220, 5154, 220, 13860, 198, 69, 220, 12326, 220, 14022, 220, 12375, 198, 69, 220, 12326, 220, 12375, 220, 3192, 198, 69, 220, 4146, 220, 15574, 220, 15966, 198, 69, 220, 4146, 220, 15966, 220, 15537, 198, 69, 220, 11387, 220, 15602, 220, 14274, 198, 69, 220, 11387, 220, 14274, 220, 15666, 198, 69, 220, 12815, 220, 14374, 220, 15999, 198, 69, 220, 12815, 220, 15999, 220, 16567, 198, 69, 220, 16332, 220, 16955, 220, 10914, 198, 69, 220, 16332, 220, 10914, 220, 15828, 198, 69, 220, 15741, 220, 15451, 220, 16590, 198, 69, 220, 15741, 220, 16590, 220, 14417, 198, 69, 220, 16660, 220, 16367, 220, 16949, 198, 69, 220, 16660, 220, 16949, 220, 17267, 198, 69, 220, 11209, 220, 15282, 220, 16544, 198, 69, 220, 11209, 220, 16544, 220, 16085, 198, 69, 220, 17058, 220, 15935, 220, 17361, 198, 69, 220, 17058, 220, 17361, 220, 17897, 198, 69, 220, 15287, 220, 17212, 220, 13754, 198, 69, 220, 15287, 220, 13754, 220, 17335, 198, 69, 220, 16443, 220, 17313, 220, 17168, 198, 69, 220, 16443, 220, 17168, 220, 16780, 198, 69, 220, 17408, 220, 18163, 220, 17690, 198, 69, 220, 17408, 220, 17690, 220, 15531, 198, 69, 220, 3101, 220, 12405, 220, 13121, 198, 69, 220, 3101, 220, 13121, 220, 13236, 198, 69, 220, 12166, 220, 13364, 220, 12879, 198, 69, 220, 12166, 220, 12879, 220, 14777, 198, 69, 220, 14498, 220, 15500, 220, 12226, 198, 69, 220, 14498, 220, 12226, 220, 15134, 198, 69, 220, 13384, 220, 15231, 220, 16104, 198, 69, 220, 13384, 220, 16104, 220, 15189, 198, 69, 220, 15340, 220, 16718, 220, 17592, 198, 69, 220, 15340, 220, 17592, 220, 16874, 198, 69, 220, 9588, 220, 14423, 220, 15805, 198, 69, 220, 9588, 220, 15805, 220, 15726, 198, 69, 220, 16723, 220, 15257, 220, 17470, 198, 69, 220, 16723, 220, 17470, 220, 13817, 198, 69, 220, 16884, 220, 18196, 220, 10568, 198, 69, 220, 16884, 220, 10568, 220, 16707, 198, 69, 220, 17079, 220, 8765, 220, 17153, 198, 69, 220, 17079, 220, 17153, 220, 16596, 198, 69, 220, 17014, 220, 17609, 220, 18633, 198, 69, 220, 17014, 220, 18633, 220, 17887, 198, 69, 220, 13679, 220, 16546, 220, 17590, 198, 69, 220, 13679, 220, 17590, 220, 16522, 198, 69, 220, 17451, 220, 12901, 220, 18061, 198, 69, 220, 17451, 220, 18061, 220, 17678, 198, 69, 220, 10866, 220, 19746, 220, 18634, 198, 69, 220, 10866, 220, 18634, 220, 10336, 198, 69, 220, 1041, 220, 8652, 220, 18113, 198, 69, 220, 16482, 220, 17228, 220, 6889, 198, 69, 220, 10148, 220, 18384, 220, 17306, 198, 69, 220, 9591, 220, 10125, 220, 18349, 198, 69, 220, 9748, 220, 3753, 220, 18520, 198, 69, 220, 4161, 220, 6280, 220, 17112, 198, 69, 220, 8929, 220, 7529, 220, 19192, 198, 69, 220, 7699, 220, 9714, 220, 6843, 198, 69, 220, 18277, 220, 18509, 220, 18199, 198, 69, 220, 15951, 220, 11483, 220, 12676, 198, 69, 220, 18044, 220, 14087, 220, 18775, 198, 69, 220, 19057, 220, 11483, 220, 4386, 198, 69, 220, 19929, 220, 14087, 220, 5925, 198, 69, 220, 14648, 220, 18509, 220, 10132, 198, 69, 220, 18650, 220, 12112, 220, 9079, 198, 69, 220, 18277, 220, 13762, 220, 17662, 198, 69, 220, 1049, 220, 12338, 220, 18017, 198, 69, 220, 18265, 220, 12935, 220, 7854, 198, 69, 220, 18322, 220, 10898, 220, 19166, 198, 69, 220, 19867, 220, 10898, 220, 9079, 198, 128009, 128009]\n",
      "inputs:\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful 3D mesh modeling AI assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Create a 3D obj file using the following description: a metal gate/cage with a blue logo on the door.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "v 61 2 1\n",
      "v 53 2 1\n",
      "v 44 39 1\n",
      "v 44 18 1\n",
      "v 30 35 1\n",
      "v 30 21 1\n",
      "v 49 50 1\n",
      "v 61 50 1\n",
      "v 2 0 1\n",
      "v 61 0 1\n",
      "v 61 1 1\n",
      "v 53 1 1\n",
      "v 43 18 1\n",
      "v 30 20 1\n",
      "v 2 20 1\n",
      "v 61 2 1\n",
      "v 61 50 1\n",
      "v 49 50 1\n",
      "v 44 39 1\n",
      "v 53 2 1\n",
      "v 2 52 1\n",
      "v 2 36 1\n",
      "v 30 36 1\n",
      "v 44 39 1\n",
      "v 48 50 1\n",
      "v 61 51 1\n",
      "v 61 52 1\n",
      "v 2 0 1\n",
      "v 2 20 1\n",
      "v 30 20 1\n",
      "v 43 18 1\n",
      "v 53 1 1\n",
      "v 61 1 1\n",
      "v 61 0 1\n",
      "v 2 52 1\n",
      "v 61 52 1\n",
      "v 61 51 1\n",
      "v 48 50 1\n",
      "v 44 39 1\n",
      "v 30 36 1\n",
      "v 2 36 1\n",
      "v 44 18 1\n",
      "v 30 35 1\n",
      "v 30 21 1\n",
      "v 2 21 2\n",
      "v 29 21 2\n",
      "v 29 35 2\n",
      "v 2 35 2\n",
      "v 2 21 0\n",
      "v 2 35 0\n",
      "v 29 35 0\n",
      "v 29 21 0\n",
      "v 20 2 1\n",
      "v 20 3 1\n",
      "v 23 3 2\n",
      "v 23 2 2\n",
      "v 20 2 1\n",
      "v 23 2 2\n",
      "v 23 3 2\n",
      "v 20 3 1\n",
      "v 11 17 1\n",
      "v 11 19 1\n",
      "v 13 19 2\n",
      "v 13 17 2\n",
      "v 11 17 1\n",
      "v 13 17 2\n",
      "v 13 19 2\n",
      "v 11 19 1\n",
      "v 6 6 1\n",
      "v 6 8 1\n",
      "v 8 8 2\n",
      "v 8 6 2\n",
      "v 6 6 1\n",
      "v 8 6 2\n",
      "v 8 8 2\n",
      "v 6 8 1\n",
      "v 53 41 1\n",
      "v 53 42 1\n",
      "v 55 42 2\n",
      "v 55 41 2\n",
      "v 53 41 1\n",
      "v 55 41 2\n",
      "v 55 42 2\n",
      "v 53 42 1\n",
      "v 49 50 0\n",
      "v 44 39 0\n",
      "v 44 39 0\n",
      "v 48 50 0\n",
      "v 48 50 0\n",
      "v 61 51 0\n",
      "v 61 50 0\n",
      "v 49 50 0\n",
      "v 2 36 0\n",
      "v 30 36 0\n",
      "v 29 35 0\n",
      "v 2 35 0\n",
      "v 30 36 0\n",
      "v 44 39 0\n",
      "v 44 39 0\n",
      "v 30 35 0\n",
      "v 30 35 0\n",
      "v 30 21 0\n",
      "v 29 21 0\n",
      "v 29 35 0\n",
      "v 29 21 0\n",
      "v 30 20 0\n",
      "v 2 20 0\n",
      "v 2 21 0\n",
      "v 44 18 0\n",
      "v 43 18 0\n",
      "v 30 20 0\n",
      "v 30 21 0\n",
      "v 44 18 0\n",
      "v 53 2 0\n",
      "v 53 1 0\n",
      "v 43 18 0\n",
      "v 53 2 0\n",
      "v 61 2 0\n",
      "v 61 1 0\n",
      "v 53 1 0\n",
      "v 49 50 2\n",
      "v 48 50 2\n",
      "v 44 39 2\n",
      "v 44 39 2\n",
      "v 48 50 2\n",
      "v 49 50 2\n",
      "v 61 50 2\n",
      "v 61 51 2\n",
      "v 2 36 2\n",
      "v 2 35 2\n",
      "v 29 35 2\n",
      "v 30 36 2\n",
      "v 30 36 2\n",
      "v 30 35 2\n",
      "v 44 39 2\n",
      "v 44 39 2\n",
      "v 30 35 2\n",
      "v 29 35 2\n",
      "v 29 21 2\n",
      "v 30 21 2\n",
      "v 2 20 2\n",
      "v 30 20 2\n",
      "v 29 21 2\n",
      "v 2 21 2\n",
      "v 30 21 2\n",
      "v 30 20 2\n",
      "v 43 18 2\n",
      "v 44 18 2\n",
      "v 44 18 2\n",
      "v 43 18 2\n",
      "v 53 1 2\n",
      "v 53 2 2\n",
      "v 53 2 2\n",
      "v 53 1 2\n",
      "v 61 1 2\n",
      "v 61 2 2\n",
      "v 0 21 2\n",
      "v 0 21 0\n",
      "v 0 0 0\n",
      "v 0 0 2\n",
      "v 0 36 2\n",
      "v 0 36 0\n",
      "v 0 21 0\n",
      "v 0 21 2\n",
      "v 64 0 2\n",
      "v 64 0 0\n",
      "v 64 52 0\n",
      "v 64 52 2\n",
      "v 64 52 2\n",
      "v 64 52 0\n",
      "v 0 52 0\n",
      "v 0 52 2\n",
      "v 0 52 2\n",
      "v 0 52 0\n",
      "v 0 36 0\n",
      "v 0 36 2\n",
      "v 2 20 2\n",
      "v 0 21 2\n",
      "v 0 0 2\n",
      "v 2 0 2\n",
      "v 2 0 2\n",
      "v 0 0 2\n",
      "v 0 0 0\n",
      "v 2 0 0\n",
      "v 0 0 0\n",
      "v 0 21 0\n",
      "v 2 20 0\n",
      "v 2 0 0\n",
      "v 2 35 2\n",
      "v 0 36 2\n",
      "v 0 21 2\n",
      "v 2 21 2\n",
      "v 2 21 0\n",
      "v 0 21 0\n",
      "v 0 36 0\n",
      "v 2 35 0\n",
      "v 0 52 2\n",
      "v 0 36 2\n",
      "v 2 36 2\n",
      "v 2 52 2\n",
      "v 2 36 0\n",
      "v 0 36 0\n",
      "v 0 52 0\n",
      "v 2 52 0\n",
      "v 61 0 0\n",
      "v 64 0 0\n",
      "v 64 0 2\n",
      "v 61 0 2\n",
      "v 61 2 2\n",
      "v 64 0 2\n",
      "v 64 52 2\n",
      "v 61 50 2\n",
      "v 61 50 0\n",
      "v 64 52 0\n",
      "v 64 0 0\n",
      "v 61 2 0\n",
      "v 2 0 2\n",
      "v 2 0 2\n",
      "v 61 0 2\n",
      "v 61 0 2\n",
      "v 2 0 0\n",
      "v 0 0 0\n",
      "v 0 0 0\n",
      "v 0 0 0\n",
      "v 0 0 0\n",
      "v 0 0 2\n",
      "v 0 0 2\n",
      "v 0 0 2\n",
      "v 2 0 2\n",
      "v 61 52 2\n",
      "v 64 52 2\n",
      "v 0 52 2\n",
      "v 2 52 2\n",
      "v 2 52 0\n",
      "v 0 52 0\n",
      "v 64 52 0\n",
      "v 61 52 0\n",
      "v 0 52 2\n",
      "v 0 52 0\n",
      "v 61 1 2\n",
      "v 61 1 0\n",
      "v 61 0 0\n",
      "v 61 0 2\n",
      "v 61 0 2\n",
      "v 61 0 0\n",
      "v 2 0 0\n",
      "v 2 0 2\n",
      "v 2 0 2\n",
      "v 2 0 0\n",
      "v 2 20 0\n",
      "v 2 20 2\n",
      "v 2 20 2\n",
      "v 2 20 0\n",
      "v 30 20 0\n",
      "v 30 20 2\n",
      "v 30 20 2\n",
      "v 30 20 0\n",
      "v 43 18 0\n",
      "v 43 18 2\n",
      "v 43 18 2\n",
      "v 43 18 0\n",
      "v 53 1 0\n",
      "v 53 1 2\n",
      "v 53 1 2\n",
      "v 53 1 0\n",
      "v 61 1 0\n",
      "v 61 1 2\n",
      "v 2 21 2\n",
      "v 2 21 0\n",
      "v 2 35 0\n",
      "v 2 35 2\n",
      "v 2 35 2\n",
      "v 2 35 0\n",
      "v 29 35 0\n",
      "v 29 35 2\n",
      "v 29 35 2\n",
      "v 29 35 0\n",
      "v 29 21 0\n",
      "v 29 21 2\n",
      "v 29 21 2\n",
      "v 29 21 0\n",
      "v 2 21 0\n",
      "v 2 21 2\n",
      "v 61 50 2\n",
      "v 61 50 0\n",
      "v 61 2 0\n",
      "v 61 2 2\n",
      "v 61 2 2\n",
      "v 61 2 0\n",
      "v 53 2 0\n",
      "v 53 2 2\n",
      "v 53 2 2\n",
      "v 53 2 0\n",
      "v 44 18 0\n",
      "v 44 18 2\n",
      "v 44 18 2\n",
      "v 44 18 0\n",
      "v 30 21 0\n",
      "v 30 21 2\n",
      "v 30 21 2\n",
      "v 30 21 0\n",
      "v 30 35 0\n",
      "v 30 35 2\n",
      "v 30 35 2\n",
      "v 30 35 0\n",
      "v 44 39 0\n",
      "v 44 39 2\n",
      "v 44 39 2\n",
      "v 44 39 0\n",
      "v 49 50 0\n",
      "v 49 50 2\n",
      "v 49 50 2\n",
      "v 49 50 0\n",
      "v 61 50 0\n",
      "v 61 50 2\n",
      "v 2 36 2\n",
      "v 2 36 0\n",
      "v 2 52 0\n",
      "v 2 52 2\n",
      "v 2 52 2\n",
      "v 2 52 0\n",
      "v 61 52 0\n",
      "v 61 52 2\n",
      "v 61 52 2\n",
      "v 61 52 0\n",
      "v 61 51 0\n",
      "v 61 51 2\n",
      "v 61 51 0\n",
      "v 48 50 0\n",
      "v 48 50 2\n",
      "v 61 51 2\n",
      "v 48 50 2\n",
      "v 48 50 0\n",
      "v 44 39 0\n",
      "v 44 39 2\n",
      "v 44 39 2\n",
      "v 44 39 0\n",
      "v 30 36 0\n",
      "v 30 36 2\n",
      "v 30 36 2\n",
      "v 30 36 0\n",
      "v 2 36 0\n",
      "v 2 36 2\n",
      "v 61 0 0\n",
      "v 2 0 0\n",
      "v 2 0 0\n",
      "v 61 0 0\n",
      "v 61 0 2\n",
      "v 2 0 2\n",
      "v 29 35 0\n",
      "v 30 36 0\n",
      "v 30 21 0\n",
      "v 30 20 0\n",
      "v 30 36 2\n",
      "v 29 35 2\n",
      "v 30 20 2\n",
      "v 2 35 2\n",
      "v 2 36 0\n",
      "v 2 20 2\n",
      "v 2 21 0\n",
      "v 61 0 2\n",
      "v 64 0 2\n",
      "v 61 1 2\n",
      "v 61 51 2\n",
      "v 61 52 2\n",
      "v 61 52 0\n",
      "v 61 51 0\n",
      "v 61 50 2\n",
      "v 61 51 0\n",
      "v 61 1 2\n",
      "v 61 2 0\n",
      "v 64 0 2\n",
      "v 0 52 2\n",
      "v 0 52 0\n",
      "v 0 52 0\n",
      "v 61 0 0\n",
      "v 61 0 0\n",
      "v 64 0 0\n",
      "v 64 0 0\n",
      "f 1 2 3\n",
      "f 2 4 3\n",
      "f 3 4 5\n",
      "f 4 6 5\n",
      "f 7 8 1\n",
      "f 7 1 3\n",
      "f 9 10 11\n",
      "f 9 11 12\n",
      "f 9 12 13\n",
      "f 9 13 14\n",
      "f 9 14 15\n",
      "f 16 17 18\n",
      "f 16 18 19\n",
      "f 16 19 20\n",
      "f 21 22 23\n",
      "f 21 23 24\n",
      "f 21 24 25\n",
      "f 21 25 26\n",
      "f 21 26 27\n",
      "f 28 29 30\n",
      "f 28 30 31\n",
      "f 28 31 32\n",
      "f 28 32 33\n",
      "f 28 33 34\n",
      "f 35 36 37\n",
      "f 35 37 38\n",
      "f 35 38 39\n",
      "f 35 39 40\n",
      "f 35 40 41\n",
      "f 42 20 19\n",
      "f 42 19 43\n",
      "f 42 43 44\n",
      "f 45 46 47\n",
      "f 45 47 48\n",
      "f 49 50 51\n",
      "f 49 51 52\n",
      "f 53 54 55\n",
      "f 53 55 56\n",
      "f 57 58 59\n",
      "f 57 59 60\n",
      "f 61 62 63\n",
      "f 61 63 64\n",
      "f 65 66 67\n",
      "f 65 67 68\n",
      "f 69 70 71\n",
      "f 69 71 72\n",
      "f 73 74 75\n",
      "f 73 75 76\n",
      "f 77 78 79\n",
      "f 77 79 80\n",
      "f 81 82 83\n",
      "f 81 83 84\n",
      "f 85 86 87\n",
      "f 85 87 88\n",
      "f 89 90 91\n",
      "f 89 91 92\n",
      "f 93 94 95\n",
      "f 93 95 96\n",
      "f 97 98 99\n",
      "f 97 99 100\n",
      "f 101 102 103\n",
      "f 101 103 104\n",
      "f 105 106 107\n",
      "f 105 107 108\n",
      "f 109 110 111\n",
      "f 109 111 112\n",
      "f 113 114 115\n",
      "f 113 115 116\n",
      "f 117 118 119\n",
      "f 117 119 120\n",
      "f 121 122 123\n",
      "f 121 123 124\n",
      "f 125 126 127\n",
      "f 125 127 128\n",
      "f 129 130 131\n",
      "f 129 131 132\n",
      "f 133 134 135\n",
      "f 133 135 136\n",
      "f 137 138 139\n",
      "f 137 139 140\n",
      "f 141 142 143\n",
      "f 141 143 144\n",
      "f 145 146 147\n",
      "f 145 147 148\n",
      "f 149 150 151\n",
      "f 149 151 152\n",
      "f 153 154 155\n",
      "f 153 155 156\n",
      "f 157 158 159\n",
      "f 157 159 160\n",
      "f 161 162 163\n",
      "f 161 163 164\n",
      "f 165 166 167\n",
      "f 165 167 168\n",
      "f 169 170 171\n",
      "f 169 171 172\n",
      "f 173 174 175\n",
      "f 173 175 176\n",
      "f 177 178 179\n",
      "f 177 179 180\n",
      "f 181 182 183\n",
      "f 181 183 184\n",
      "f 185 186 187\n",
      "f 185 187 188\n",
      "f 189 190 191\n",
      "f 189 191 192\n",
      "f 193 194 195\n",
      "f 193 195 196\n",
      "f 197 198 199\n",
      "f 197 199 200\n",
      "f 201 202 203\n",
      "f 201 203 204\n",
      "f 205 206 207\n",
      "f 205 207 208\n",
      "f 209 210 211\n",
      "f 209 211 212\n",
      "f 213 214 215\n",
      "f 213 215 216\n",
      "f 217 218 219\n",
      "f 217 219 220\n",
      "f 188 221 222\n",
      "f 188 222 223\n",
      "f 224 225 226\n",
      "f 224 226 160\n",
      "f 227 228 229\n",
      "f 227 229 180\n",
      "f 230 231 232\n",
      "f 230 232 233\n",
      "f 234 235 236\n",
      "f 234 236 237\n",
      "f 173 238 171\n",
      "f 173 171 239\n",
      "f 240 241 242\n",
      "f 240 242 243\n",
      "f 244 245 246\n",
      "f 244 246 247\n",
      "f 248 249 250\n",
      "f 248 250 251\n",
      "f 252 253 254\n",
      "f 252 254 255\n",
      "f 256 257 258\n",
      "f 256 258 259\n",
      "f 260 261 262\n",
      "f 260 262 263\n",
      "f 264 265 266\n",
      "f 264 266 267\n",
      "f 268 269 270\n",
      "f 268 270 271\n",
      "f 272 273 274\n",
      "f 272 274 275\n",
      "f 276 277 278\n",
      "f 276 278 279\n",
      "f 280 281 282\n",
      "f 280 282 283\n",
      "f 284 285 286\n",
      "f 284 286 287\n",
      "f 288 289 290\n",
      "f 288 290 291\n",
      "f 292 293 294\n",
      "f 292 294 295\n",
      "f 296 297 298\n",
      "f 296 298 299\n",
      "f 300 301 302\n",
      "f 300 302 303\n",
      "f 304 305 306\n",
      "f 304 306 307\n",
      "f 308 309 310\n",
      "f 308 310 311\n",
      "f 312 313 314\n",
      "f 312 314 315\n",
      "f 316 317 318\n",
      "f 316 318 319\n",
      "f 320 321 322\n",
      "f 320 322 323\n",
      "f 324 325 326\n",
      "f 324 326 327\n",
      "f 328 329 330\n",
      "f 328 330 331\n",
      "f 332 333 334\n",
      "f 332 334 335\n",
      "f 336 337 338\n",
      "f 336 338 339\n",
      "f 340 341 342\n",
      "f 340 342 343\n",
      "f 344 345 346\n",
      "f 344 346 347\n",
      "f 205 348 349\n",
      "f 205 349 184\n",
      "f 100 350 351\n",
      "f 352 353 103\n",
      "f 137 354 355\n",
      "f 145 139 356\n",
      "f 129 198 357\n",
      "f 96 195 358\n",
      "f 144 191 359\n",
      "f 107 186 360\n",
      "f 361 362 363\n",
      "f 364 211 365\n",
      "f 366 236 367\n",
      "f 368 211 128\n",
      "f 369 236 91\n",
      "f 370 362 156\n",
      "f 371 215 119\n",
      "f 361 219 372\n",
      "f 200 232 373\n",
      "f 374 375 204\n",
      "f 376 377 378\n",
      "f 379 377 119\n",
      "<|eot_id|><|eot_id|>\n",
      "label_ids:\n",
      "[128009, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 85, 220, 5547, 220, 17, 220, 16, 198, 85, 220, 4331, 220, 17, 220, 16, 198, 85, 220, 2096, 220, 2137, 220, 16, 198, 85, 220, 2096, 220, 972, 220, 16, 198, 85, 220, 966, 220, 1758, 220, 16, 198, 85, 220, 966, 220, 1691, 220, 16, 198, 85, 220, 2491, 220, 1135, 220, 16, 198, 85, 220, 5547, 220, 1135, 220, 16, 198, 85, 220, 17, 220, 15, 220, 16, 198, 85, 220, 5547, 220, 15, 220, 16, 198, 85, 220, 5547, 220, 16, 220, 16, 198, 85, 220, 4331, 220, 16, 220, 16, 198, 85, 220, 3391, 220, 972, 220, 16, 198, 85, 220, 966, 220, 508, 220, 16, 198, 85, 220, 17, 220, 508, 220, 16, 198, 85, 220, 5547, 220, 17, 220, 16, 198, 85, 220, 5547, 220, 1135, 220, 16, 198, 85, 220, 2491, 220, 1135, 220, 16, 198, 85, 220, 2096, 220, 2137, 220, 16, 198, 85, 220, 4331, 220, 17, 220, 16, 198, 85, 220, 17, 220, 4103, 220, 16, 198, 85, 220, 17, 220, 1927, 220, 16, 198, 85, 220, 966, 220, 1927, 220, 16, 198, 85, 220, 2096, 220, 2137, 220, 16, 198, 85, 220, 2166, 220, 1135, 220, 16, 198, 85, 220, 5547, 220, 3971, 220, 16, 198, 85, 220, 5547, 220, 4103, 220, 16, 198, 85, 220, 17, 220, 15, 220, 16, 198, 85, 220, 17, 220, 508, 220, 16, 198, 85, 220, 966, 220, 508, 220, 16, 198, 85, 220, 3391, 220, 972, 220, 16, 198, 85, 220, 4331, 220, 16, 220, 16, 198, 85, 220, 5547, 220, 16, 220, 16, 198, 85, 220, 5547, 220, 15, 220, 16, 198, 85, 220, 17, 220, 4103, 220, 16, 198, 85, 220, 5547, 220, 4103, 220, 16, 198, 85, 220, 5547, 220, 3971, 220, 16, 198, 85, 220, 2166, 220, 1135, 220, 16, 198, 85, 220, 2096, 220, 2137, 220, 16, 198, 85, 220, 966, 220, 1927, 220, 16, 198, 85, 220, 17, 220, 1927, 220, 16, 198, 85, 220, 2096, 220, 972, 220, 16, 198, 85, 220, 966, 220, 1758, 220, 16, 198, 85, 220, 966, 220, 1691, 220, 16, 198, 85, 220, 17, 220, 1691, 220, 17, 198, 85, 220, 1682, 220, 1691, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1691, 220, 15, 198, 85, 220, 508, 220, 17, 220, 16, 198, 85, 220, 508, 220, 18, 220, 16, 198, 85, 220, 1419, 220, 18, 220, 17, 198, 85, 220, 1419, 220, 17, 220, 17, 198, 85, 220, 508, 220, 17, 220, 16, 198, 85, 220, 1419, 220, 17, 220, 17, 198, 85, 220, 1419, 220, 18, 220, 17, 198, 85, 220, 508, 220, 18, 220, 16, 198, 85, 220, 806, 220, 1114, 220, 16, 198, 85, 220, 806, 220, 777, 220, 16, 198, 85, 220, 1032, 220, 777, 220, 17, 198, 85, 220, 1032, 220, 1114, 220, 17, 198, 85, 220, 806, 220, 1114, 220, 16, 198, 85, 220, 1032, 220, 1114, 220, 17, 198, 85, 220, 1032, 220, 777, 220, 17, 198, 85, 220, 806, 220, 777, 220, 16, 198, 85, 220, 21, 220, 21, 220, 16, 198, 85, 220, 21, 220, 23, 220, 16, 198, 85, 220, 23, 220, 23, 220, 17, 198, 85, 220, 23, 220, 21, 220, 17, 198, 85, 220, 21, 220, 21, 220, 16, 198, 85, 220, 23, 220, 21, 220, 17, 198, 85, 220, 23, 220, 23, 220, 17, 198, 85, 220, 21, 220, 23, 220, 16, 198, 85, 220, 4331, 220, 3174, 220, 16, 198, 85, 220, 4331, 220, 2983, 220, 16, 198, 85, 220, 2131, 220, 2983, 220, 17, 198, 85, 220, 2131, 220, 3174, 220, 17, 198, 85, 220, 4331, 220, 3174, 220, 16, 198, 85, 220, 2131, 220, 3174, 220, 17, 198, 85, 220, 2131, 220, 2983, 220, 17, 198, 85, 220, 4331, 220, 2983, 220, 16, 198, 85, 220, 2491, 220, 1135, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2166, 220, 1135, 220, 15, 198, 85, 220, 2166, 220, 1135, 220, 15, 198, 85, 220, 5547, 220, 3971, 220, 15, 198, 85, 220, 5547, 220, 1135, 220, 15, 198, 85, 220, 2491, 220, 1135, 220, 15, 198, 85, 220, 17, 220, 1927, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 15, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 966, 220, 1758, 220, 15, 198, 85, 220, 966, 220, 1758, 220, 15, 198, 85, 220, 966, 220, 1691, 220, 15, 198, 85, 220, 1682, 220, 1691, 220, 15, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1691, 220, 15, 198, 85, 220, 966, 220, 508, 220, 15, 198, 85, 220, 17, 220, 508, 220, 15, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 2096, 220, 972, 220, 15, 198, 85, 220, 3391, 220, 972, 220, 15, 198, 85, 220, 966, 220, 508, 220, 15, 198, 85, 220, 966, 220, 1691, 220, 15, 198, 85, 220, 2096, 220, 972, 220, 15, 198, 85, 220, 4331, 220, 17, 220, 15, 198, 85, 220, 4331, 220, 16, 220, 15, 198, 85, 220, 3391, 220, 972, 220, 15, 198, 85, 220, 4331, 220, 17, 220, 15, 198, 85, 220, 5547, 220, 17, 220, 15, 198, 85, 220, 5547, 220, 16, 220, 15, 198, 85, 220, 4331, 220, 16, 220, 15, 198, 85, 220, 2491, 220, 1135, 220, 17, 198, 85, 220, 2166, 220, 1135, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2166, 220, 1135, 220, 17, 198, 85, 220, 2491, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 3971, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 17, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 966, 220, 1927, 220, 17, 198, 85, 220, 966, 220, 1927, 220, 17, 198, 85, 220, 966, 220, 1758, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 966, 220, 1758, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 1682, 220, 1691, 220, 17, 198, 85, 220, 966, 220, 1691, 220, 17, 198, 85, 220, 17, 220, 508, 220, 17, 198, 85, 220, 966, 220, 508, 220, 17, 198, 85, 220, 1682, 220, 1691, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 17, 198, 85, 220, 966, 220, 1691, 220, 17, 198, 85, 220, 966, 220, 508, 220, 17, 198, 85, 220, 3391, 220, 972, 220, 17, 198, 85, 220, 2096, 220, 972, 220, 17, 198, 85, 220, 2096, 220, 972, 220, 17, 198, 85, 220, 3391, 220, 972, 220, 17, 198, 85, 220, 4331, 220, 16, 220, 17, 198, 85, 220, 4331, 220, 17, 220, 17, 198, 85, 220, 4331, 220, 17, 220, 17, 198, 85, 220, 4331, 220, 16, 220, 17, 198, 85, 220, 5547, 220, 16, 220, 17, 198, 85, 220, 5547, 220, 17, 220, 17, 198, 85, 220, 15, 220, 1691, 220, 17, 198, 85, 220, 15, 220, 1691, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 15, 220, 1927, 220, 17, 198, 85, 220, 15, 220, 1927, 220, 15, 198, 85, 220, 15, 220, 1691, 220, 15, 198, 85, 220, 15, 220, 1691, 220, 17, 198, 85, 220, 1227, 220, 15, 220, 17, 198, 85, 220, 1227, 220, 15, 220, 15, 198, 85, 220, 1227, 220, 4103, 220, 15, 198, 85, 220, 1227, 220, 4103, 220, 17, 198, 85, 220, 1227, 220, 4103, 220, 17, 198, 85, 220, 1227, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 1927, 220, 15, 198, 85, 220, 15, 220, 1927, 220, 17, 198, 85, 220, 17, 220, 508, 220, 17, 198, 85, 220, 15, 220, 1691, 220, 17, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 1691, 220, 15, 198, 85, 220, 17, 220, 508, 220, 15, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 15, 220, 1927, 220, 17, 198, 85, 220, 15, 220, 1691, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 15, 220, 1691, 220, 15, 198, 85, 220, 15, 220, 1927, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 1927, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 17, 198, 85, 220, 17, 220, 4103, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 15, 198, 85, 220, 15, 220, 1927, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 17, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 17, 220, 17, 198, 85, 220, 1227, 220, 15, 220, 17, 198, 85, 220, 1227, 220, 4103, 220, 17, 198, 85, 220, 5547, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 1135, 220, 15, 198, 85, 220, 1227, 220, 4103, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 15, 198, 85, 220, 5547, 220, 17, 220, 15, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 15, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 15, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 4103, 220, 17, 198, 85, 220, 1227, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 17, 220, 4103, 220, 17, 198, 85, 220, 17, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 1227, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 16, 220, 17, 198, 85, 220, 5547, 220, 16, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 17, 220, 508, 220, 15, 198, 85, 220, 17, 220, 508, 220, 17, 198, 85, 220, 17, 220, 508, 220, 17, 198, 85, 220, 17, 220, 508, 220, 15, 198, 85, 220, 966, 220, 508, 220, 15, 198, 85, 220, 966, 220, 508, 220, 17, 198, 85, 220, 966, 220, 508, 220, 17, 198, 85, 220, 966, 220, 508, 220, 15, 198, 85, 220, 3391, 220, 972, 220, 15, 198, 85, 220, 3391, 220, 972, 220, 17, 198, 85, 220, 3391, 220, 972, 220, 17, 198, 85, 220, 3391, 220, 972, 220, 15, 198, 85, 220, 4331, 220, 16, 220, 15, 198, 85, 220, 4331, 220, 16, 220, 17, 198, 85, 220, 4331, 220, 16, 220, 17, 198, 85, 220, 4331, 220, 16, 220, 15, 198, 85, 220, 5547, 220, 16, 220, 15, 198, 85, 220, 5547, 220, 16, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 15, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 17, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 1682, 220, 1691, 220, 15, 198, 85, 220, 1682, 220, 1691, 220, 17, 198, 85, 220, 1682, 220, 1691, 220, 17, 198, 85, 220, 1682, 220, 1691, 220, 15, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 17, 220, 1691, 220, 17, 198, 85, 220, 5547, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 1135, 220, 15, 198, 85, 220, 5547, 220, 17, 220, 15, 198, 85, 220, 5547, 220, 17, 220, 17, 198, 85, 220, 5547, 220, 17, 220, 17, 198, 85, 220, 5547, 220, 17, 220, 15, 198, 85, 220, 4331, 220, 17, 220, 15, 198, 85, 220, 4331, 220, 17, 220, 17, 198, 85, 220, 4331, 220, 17, 220, 17, 198, 85, 220, 4331, 220, 17, 220, 15, 198, 85, 220, 2096, 220, 972, 220, 15, 198, 85, 220, 2096, 220, 972, 220, 17, 198, 85, 220, 2096, 220, 972, 220, 17, 198, 85, 220, 2096, 220, 972, 220, 15, 198, 85, 220, 966, 220, 1691, 220, 15, 198, 85, 220, 966, 220, 1691, 220, 17, 198, 85, 220, 966, 220, 1691, 220, 17, 198, 85, 220, 966, 220, 1691, 220, 15, 198, 85, 220, 966, 220, 1758, 220, 15, 198, 85, 220, 966, 220, 1758, 220, 17, 198, 85, 220, 966, 220, 1758, 220, 17, 198, 85, 220, 966, 220, 1758, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2491, 220, 1135, 220, 15, 198, 85, 220, 2491, 220, 1135, 220, 17, 198, 85, 220, 2491, 220, 1135, 220, 17, 198, 85, 220, 2491, 220, 1135, 220, 15, 198, 85, 220, 5547, 220, 1135, 220, 15, 198, 85, 220, 5547, 220, 1135, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 15, 198, 85, 220, 17, 220, 4103, 220, 15, 198, 85, 220, 17, 220, 4103, 220, 17, 198, 85, 220, 17, 220, 4103, 220, 17, 198, 85, 220, 17, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 4103, 220, 17, 198, 85, 220, 5547, 220, 4103, 220, 17, 198, 85, 220, 5547, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 3971, 220, 15, 198, 85, 220, 5547, 220, 3971, 220, 17, 198, 85, 220, 5547, 220, 3971, 220, 15, 198, 85, 220, 2166, 220, 1135, 220, 15, 198, 85, 220, 2166, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 3971, 220, 17, 198, 85, 220, 2166, 220, 1135, 220, 17, 198, 85, 220, 2166, 220, 1135, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 17, 198, 85, 220, 2096, 220, 2137, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 17, 198, 85, 220, 966, 220, 1927, 220, 17, 198, 85, 220, 966, 220, 1927, 220, 15, 198, 85, 220, 17, 220, 1927, 220, 15, 198, 85, 220, 17, 220, 1927, 220, 17, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 17, 220, 15, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 17, 220, 15, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 15, 198, 85, 220, 966, 220, 1691, 220, 15, 198, 85, 220, 966, 220, 508, 220, 15, 198, 85, 220, 966, 220, 1927, 220, 17, 198, 85, 220, 1682, 220, 1758, 220, 17, 198, 85, 220, 966, 220, 508, 220, 17, 198, 85, 220, 17, 220, 1758, 220, 17, 198, 85, 220, 17, 220, 1927, 220, 15, 198, 85, 220, 17, 220, 508, 220, 17, 198, 85, 220, 17, 220, 1691, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 17, 198, 85, 220, 1227, 220, 15, 220, 17, 198, 85, 220, 5547, 220, 16, 220, 17, 198, 85, 220, 5547, 220, 3971, 220, 17, 198, 85, 220, 5547, 220, 4103, 220, 17, 198, 85, 220, 5547, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 3971, 220, 15, 198, 85, 220, 5547, 220, 1135, 220, 17, 198, 85, 220, 5547, 220, 3971, 220, 15, 198, 85, 220, 5547, 220, 16, 220, 17, 198, 85, 220, 5547, 220, 17, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 17, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 15, 220, 4103, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 5547, 220, 15, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 15, 198, 85, 220, 1227, 220, 15, 220, 15, 198, 69, 220, 16, 220, 17, 220, 18, 198, 69, 220, 17, 220, 19, 220, 18, 198, 69, 220, 18, 220, 19, 220, 20, 198, 69, 220, 19, 220, 21, 220, 20, 198, 69, 220, 22, 220, 23, 220, 16, 198, 69, 220, 22, 220, 16, 220, 18, 198, 69, 220, 24, 220, 605, 220, 806, 198, 69, 220, 24, 220, 806, 220, 717, 198, 69, 220, 24, 220, 717, 220, 1032, 198, 69, 220, 24, 220, 1032, 220, 975, 198, 69, 220, 24, 220, 975, 220, 868, 198, 69, 220, 845, 220, 1114, 220, 972, 198, 69, 220, 845, 220, 972, 220, 777, 198, 69, 220, 845, 220, 777, 220, 508, 198, 69, 220, 1691, 220, 1313, 220, 1419, 198, 69, 220, 1691, 220, 1419, 220, 1187, 198, 69, 220, 1691, 220, 1187, 220, 914, 198, 69, 220, 1691, 220, 914, 220, 1627, 198, 69, 220, 1691, 220, 1627, 220, 1544, 198, 69, 220, 1591, 220, 1682, 220, 966, 198, 69, 220, 1591, 220, 966, 220, 2148, 198, 69, 220, 1591, 220, 2148, 220, 843, 198, 69, 220, 1591, 220, 843, 220, 1644, 198, 69, 220, 1591, 220, 1644, 220, 1958, 198, 69, 220, 1758, 220, 1927, 220, 1806, 198, 69, 220, 1758, 220, 1806, 220, 1987, 198, 69, 220, 1758, 220, 1987, 220, 2137, 198, 69, 220, 1758, 220, 2137, 220, 1272, 198, 69, 220, 1758, 220, 1272, 220, 3174, 198, 69, 220, 2983, 220, 508, 220, 777, 198, 69, 220, 2983, 220, 777, 220, 3391, 198, 69, 220, 2983, 220, 3391, 220, 2096, 198, 69, 220, 1774, 220, 2790, 220, 2618, 198, 69, 220, 1774, 220, 2618, 220, 2166, 198, 69, 220, 2491, 220, 1135, 220, 3971, 198, 69, 220, 2491, 220, 3971, 220, 4103, 198, 69, 220, 4331, 220, 4370, 220, 2131, 198, 69, 220, 4331, 220, 2131, 220, 3487, 198, 69, 220, 3226, 220, 2970, 220, 2946, 198, 69, 220, 3226, 220, 2946, 220, 1399, 198, 69, 220, 5547, 220, 5538, 220, 5495, 198, 69, 220, 5547, 220, 5495, 220, 1227, 198, 69, 220, 2397, 220, 2287, 220, 3080, 198, 69, 220, 2397, 220, 3080, 220, 2614, 198, 69, 220, 3076, 220, 2031, 220, 6028, 198, 69, 220, 3076, 220, 6028, 220, 5332, 198, 69, 220, 5958, 220, 5728, 220, 2075, 198, 69, 220, 5958, 220, 2075, 220, 4767, 198, 69, 220, 2813, 220, 2495, 220, 4643, 198, 69, 220, 2813, 220, 4643, 220, 1490, 198, 69, 220, 5932, 220, 6086, 220, 6069, 198, 69, 220, 5932, 220, 6069, 220, 5833, 198, 69, 220, 5313, 220, 4218, 220, 4044, 198, 69, 220, 5313, 220, 4044, 220, 2421, 198, 69, 220, 4578, 220, 1954, 220, 5925, 198, 69, 220, 4578, 220, 5925, 220, 6083, 198, 69, 220, 6365, 220, 6281, 220, 2721, 198, 69, 220, 6365, 220, 2721, 220, 4161, 198, 69, 220, 3534, 220, 3264, 220, 1484, 198, 69, 220, 3534, 220, 1484, 220, 1041, 198, 69, 220, 4645, 220, 4278, 220, 6889, 198, 69, 220, 4645, 220, 6889, 220, 6849, 198, 69, 220, 6550, 220, 7461, 220, 7699, 198, 69, 220, 6550, 220, 7699, 220, 6640, 198, 69, 220, 7743, 220, 5120, 220, 5037, 198, 69, 220, 7743, 220, 5037, 220, 7261, 198, 69, 220, 8190, 220, 8011, 220, 7322, 198, 69, 220, 8190, 220, 7322, 220, 8027, 198, 69, 220, 8546, 220, 8899, 220, 9079, 198, 69, 220, 8546, 220, 9079, 220, 4364, 198, 69, 220, 7994, 220, 8259, 220, 4513, 198, 69, 220, 7994, 220, 4513, 220, 8874, 198, 69, 220, 6549, 220, 9390, 220, 6804, 198, 69, 220, 6549, 220, 6804, 220, 4386, 198, 69, 220, 9748, 220, 5894, 220, 9263, 198, 69, 220, 9748, 220, 9263, 220, 9413, 198, 69, 220, 9423, 220, 9565, 220, 8878, 198, 69, 220, 9423, 220, 8878, 220, 9795, 198, 69, 220, 10148, 220, 10350, 220, 10125, 198, 69, 220, 10148, 220, 10125, 220, 6860, 198, 69, 220, 9335, 220, 10239, 220, 10290, 198, 69, 220, 9335, 220, 10290, 220, 8929, 198, 69, 220, 9591, 220, 10465, 220, 10288, 198, 69, 220, 9591, 220, 10288, 220, 10410, 198, 69, 220, 10161, 220, 3965, 220, 9690, 198, 69, 220, 10161, 220, 9690, 220, 9756, 198, 69, 220, 9800, 220, 10559, 220, 9992, 198, 69, 220, 9800, 220, 9992, 220, 10132, 198, 69, 220, 10895, 220, 11286, 220, 11068, 198, 69, 220, 10895, 220, 11068, 220, 6330, 198, 69, 220, 10718, 220, 10674, 220, 9892, 198, 69, 220, 10718, 220, 9892, 220, 10513, 198, 69, 220, 10680, 220, 11247, 220, 11515, 198, 69, 220, 10680, 220, 11515, 220, 8953, 198, 69, 220, 11739, 220, 8258, 220, 11123, 198, 69, 220, 11739, 220, 11123, 220, 10861, 198, 69, 220, 11908, 220, 11771, 220, 10005, 198, 69, 220, 11908, 220, 10005, 220, 10967, 198, 69, 220, 11242, 220, 11256, 220, 11128, 198, 69, 220, 11242, 220, 11128, 220, 5245, 198, 69, 220, 10562, 220, 10828, 220, 10750, 198, 69, 220, 10562, 220, 10750, 220, 10336, 198, 69, 220, 9741, 220, 9714, 220, 9674, 198, 69, 220, 9741, 220, 9674, 220, 9367, 198, 69, 220, 9378, 220, 7028, 220, 7529, 198, 69, 220, 9378, 220, 7529, 220, 5926, 198, 69, 220, 7285, 220, 6393, 220, 6280, 198, 69, 220, 7285, 220, 6280, 220, 5162, 198, 69, 220, 4468, 220, 3753, 220, 2550, 198, 69, 220, 4468, 220, 2550, 220, 1049, 198, 69, 220, 679, 220, 2366, 220, 9639, 198, 69, 220, 679, 220, 9639, 220, 7854, 198, 69, 220, 10866, 220, 11056, 220, 12060, 198, 69, 220, 10866, 220, 12060, 220, 12171, 198, 69, 220, 12652, 220, 8848, 220, 11483, 198, 69, 220, 12652, 220, 11483, 220, 11227, 198, 69, 220, 11702, 220, 11584, 220, 12112, 198, 69, 220, 11702, 220, 12112, 220, 12463, 198, 69, 220, 13460, 220, 13302, 220, 13762, 198, 69, 220, 13460, 220, 13762, 220, 8610, 198, 69, 220, 9367, 220, 12425, 220, 9716, 198, 69, 220, 9367, 220, 9716, 220, 12533, 198, 69, 220, 10697, 220, 11057, 220, 14057, 198, 69, 220, 10697, 220, 14057, 220, 6330, 198, 69, 220, 14206, 220, 14261, 220, 14378, 198, 69, 220, 14206, 220, 14378, 220, 5245, 198, 69, 220, 9870, 220, 12245, 220, 12338, 198, 69, 220, 9870, 220, 12338, 220, 12994, 198, 69, 220, 11727, 220, 12422, 220, 14087, 198, 69, 220, 11727, 220, 14087, 220, 14590, 198, 69, 220, 11908, 220, 13895, 220, 11123, 198, 69, 220, 11908, 220, 11123, 220, 14815, 198, 69, 220, 8273, 220, 13341, 220, 12754, 198, 69, 220, 8273, 220, 12754, 220, 14052, 198, 69, 220, 13719, 220, 13078, 220, 14205, 198, 69, 220, 13719, 220, 14205, 220, 14125, 198, 69, 220, 14185, 220, 14735, 220, 5154, 198, 69, 220, 14185, 220, 5154, 220, 13860, 198, 69, 220, 12326, 220, 14022, 220, 12375, 198, 69, 220, 12326, 220, 12375, 220, 3192, 198, 69, 220, 4146, 220, 15574, 220, 15966, 198, 69, 220, 4146, 220, 15966, 220, 15537, 198, 69, 220, 11387, 220, 15602, 220, 14274, 198, 69, 220, 11387, 220, 14274, 220, 15666, 198, 69, 220, 12815, 220, 14374, 220, 15999, 198, 69, 220, 12815, 220, 15999, 220, 16567, 198, 69, 220, 16332, 220, 16955, 220, 10914, 198, 69, 220, 16332, 220, 10914, 220, 15828, 198, 69, 220, 15741, 220, 15451, 220, 16590, 198, 69, 220, 15741, 220, 16590, 220, 14417, 198, 69, 220, 16660, 220, 16367, 220, 16949, 198, 69, 220, 16660, 220, 16949, 220, 17267, 198, 69, 220, 11209, 220, 15282, 220, 16544, 198, 69, 220, 11209, 220, 16544, 220, 16085, 198, 69, 220, 17058, 220, 15935, 220, 17361, 198, 69, 220, 17058, 220, 17361, 220, 17897, 198, 69, 220, 15287, 220, 17212, 220, 13754, 198, 69, 220, 15287, 220, 13754, 220, 17335, 198, 69, 220, 16443, 220, 17313, 220, 17168, 198, 69, 220, 16443, 220, 17168, 220, 16780, 198, 69, 220, 17408, 220, 18163, 220, 17690, 198, 69, 220, 17408, 220, 17690, 220, 15531, 198, 69, 220, 3101, 220, 12405, 220, 13121, 198, 69, 220, 3101, 220, 13121, 220, 13236, 198, 69, 220, 12166, 220, 13364, 220, 12879, 198, 69, 220, 12166, 220, 12879, 220, 14777, 198, 69, 220, 14498, 220, 15500, 220, 12226, 198, 69, 220, 14498, 220, 12226, 220, 15134, 198, 69, 220, 13384, 220, 15231, 220, 16104, 198, 69, 220, 13384, 220, 16104, 220, 15189, 198, 69, 220, 15340, 220, 16718, 220, 17592, 198, 69, 220, 15340, 220, 17592, 220, 16874, 198, 69, 220, 9588, 220, 14423, 220, 15805, 198, 69, 220, 9588, 220, 15805, 220, 15726, 198, 69, 220, 16723, 220, 15257, 220, 17470, 198, 69, 220, 16723, 220, 17470, 220, 13817, 198, 69, 220, 16884, 220, 18196, 220, 10568, 198, 69, 220, 16884, 220, 10568, 220, 16707, 198, 69, 220, 17079, 220, 8765, 220, 17153, 198, 69, 220, 17079, 220, 17153, 220, 16596, 198, 69, 220, 17014, 220, 17609, 220, 18633, 198, 69, 220, 17014, 220, 18633, 220, 17887, 198, 69, 220, 13679, 220, 16546, 220, 17590, 198, 69, 220, 13679, 220, 17590, 220, 16522, 198, 69, 220, 17451, 220, 12901, 220, 18061, 198, 69, 220, 17451, 220, 18061, 220, 17678, 198, 69, 220, 10866, 220, 19746, 220, 18634, 198, 69, 220, 10866, 220, 18634, 220, 10336, 198, 69, 220, 1041, 220, 8652, 220, 18113, 198, 69, 220, 16482, 220, 17228, 220, 6889, 198, 69, 220, 10148, 220, 18384, 220, 17306, 198, 69, 220, 9591, 220, 10125, 220, 18349, 198, 69, 220, 9748, 220, 3753, 220, 18520, 198, 69, 220, 4161, 220, 6280, 220, 17112, 198, 69, 220, 8929, 220, 7529, 220, 19192, 198, 69, 220, 7699, 220, 9714, 220, 6843, 198, 69, 220, 18277, 220, 18509, 220, 18199, 198, 69, 220, 15951, 220, 11483, 220, 12676, 198, 69, 220, 18044, 220, 14087, 220, 18775, 198, 69, 220, 19057, 220, 11483, 220, 4386, 198, 69, 220, 19929, 220, 14087, 220, 5925, 198, 69, 220, 14648, 220, 18509, 220, 10132, 198, 69, 220, 18650, 220, 12112, 220, 9079, 198, 69, 220, 18277, 220, 13762, 220, 17662, 198, 69, 220, 1049, 220, 12338, 220, 18017, 198, 69, 220, 18265, 220, 12935, 220, 7854, 198, 69, 220, 18322, 220, 10898, 220, 19166, 198, 69, 220, 19867, 220, 10898, 220, 9079, 198, 128009, 128009]\n",
      "labels:\n",
      "<|eot_id|>v 61 2 1\n",
      "v 53 2 1\n",
      "v 44 39 1\n",
      "v 44 18 1\n",
      "v 30 35 1\n",
      "v 30 21 1\n",
      "v 49 50 1\n",
      "v 61 50 1\n",
      "v 2 0 1\n",
      "v 61 0 1\n",
      "v 61 1 1\n",
      "v 53 1 1\n",
      "v 43 18 1\n",
      "v 30 20 1\n",
      "v 2 20 1\n",
      "v 61 2 1\n",
      "v 61 50 1\n",
      "v 49 50 1\n",
      "v 44 39 1\n",
      "v 53 2 1\n",
      "v 2 52 1\n",
      "v 2 36 1\n",
      "v 30 36 1\n",
      "v 44 39 1\n",
      "v 48 50 1\n",
      "v 61 51 1\n",
      "v 61 52 1\n",
      "v 2 0 1\n",
      "v 2 20 1\n",
      "v 30 20 1\n",
      "v 43 18 1\n",
      "v 53 1 1\n",
      "v 61 1 1\n",
      "v 61 0 1\n",
      "v 2 52 1\n",
      "v 61 52 1\n",
      "v 61 51 1\n",
      "v 48 50 1\n",
      "v 44 39 1\n",
      "v 30 36 1\n",
      "v 2 36 1\n",
      "v 44 18 1\n",
      "v 30 35 1\n",
      "v 30 21 1\n",
      "v 2 21 2\n",
      "v 29 21 2\n",
      "v 29 35 2\n",
      "v 2 35 2\n",
      "v 2 21 0\n",
      "v 2 35 0\n",
      "v 29 35 0\n",
      "v 29 21 0\n",
      "v 20 2 1\n",
      "v 20 3 1\n",
      "v 23 3 2\n",
      "v 23 2 2\n",
      "v 20 2 1\n",
      "v 23 2 2\n",
      "v 23 3 2\n",
      "v 20 3 1\n",
      "v 11 17 1\n",
      "v 11 19 1\n",
      "v 13 19 2\n",
      "v 13 17 2\n",
      "v 11 17 1\n",
      "v 13 17 2\n",
      "v 13 19 2\n",
      "v 11 19 1\n",
      "v 6 6 1\n",
      "v 6 8 1\n",
      "v 8 8 2\n",
      "v 8 6 2\n",
      "v 6 6 1\n",
      "v 8 6 2\n",
      "v 8 8 2\n",
      "v 6 8 1\n",
      "v 53 41 1\n",
      "v 53 42 1\n",
      "v 55 42 2\n",
      "v 55 41 2\n",
      "v 53 41 1\n",
      "v 55 41 2\n",
      "v 55 42 2\n",
      "v 53 42 1\n",
      "v 49 50 0\n",
      "v 44 39 0\n",
      "v 44 39 0\n",
      "v 48 50 0\n",
      "v 48 50 0\n",
      "v 61 51 0\n",
      "v 61 50 0\n",
      "v 49 50 0\n",
      "v 2 36 0\n",
      "v 30 36 0\n",
      "v 29 35 0\n",
      "v 2 35 0\n",
      "v 30 36 0\n",
      "v 44 39 0\n",
      "v 44 39 0\n",
      "v 30 35 0\n",
      "v 30 35 0\n",
      "v 30 21 0\n",
      "v 29 21 0\n",
      "v 29 35 0\n",
      "v 29 21 0\n",
      "v 30 20 0\n",
      "v 2 20 0\n",
      "v 2 21 0\n",
      "v 44 18 0\n",
      "v 43 18 0\n",
      "v 30 20 0\n",
      "v 30 21 0\n",
      "v 44 18 0\n",
      "v 53 2 0\n",
      "v 53 1 0\n",
      "v 43 18 0\n",
      "v 53 2 0\n",
      "v 61 2 0\n",
      "v 61 1 0\n",
      "v 53 1 0\n",
      "v 49 50 2\n",
      "v 48 50 2\n",
      "v 44 39 2\n",
      "v 44 39 2\n",
      "v 48 50 2\n",
      "v 49 50 2\n",
      "v 61 50 2\n",
      "v 61 51 2\n",
      "v 2 36 2\n",
      "v 2 35 2\n",
      "v 29 35 2\n",
      "v 30 36 2\n",
      "v 30 36 2\n",
      "v 30 35 2\n",
      "v 44 39 2\n",
      "v 44 39 2\n",
      "v 30 35 2\n",
      "v 29 35 2\n",
      "v 29 21 2\n",
      "v 30 21 2\n",
      "v 2 20 2\n",
      "v 30 20 2\n",
      "v 29 21 2\n",
      "v 2 21 2\n",
      "v 30 21 2\n",
      "v 30 20 2\n",
      "v 43 18 2\n",
      "v 44 18 2\n",
      "v 44 18 2\n",
      "v 43 18 2\n",
      "v 53 1 2\n",
      "v 53 2 2\n",
      "v 53 2 2\n",
      "v 53 1 2\n",
      "v 61 1 2\n",
      "v 61 2 2\n",
      "v 0 21 2\n",
      "v 0 21 0\n",
      "v 0 0 0\n",
      "v 0 0 2\n",
      "v 0 36 2\n",
      "v 0 36 0\n",
      "v 0 21 0\n",
      "v 0 21 2\n",
      "v 64 0 2\n",
      "v 64 0 0\n",
      "v 64 52 0\n",
      "v 64 52 2\n",
      "v 64 52 2\n",
      "v 64 52 0\n",
      "v 0 52 0\n",
      "v 0 52 2\n",
      "v 0 52 2\n",
      "v 0 52 0\n",
      "v 0 36 0\n",
      "v 0 36 2\n",
      "v 2 20 2\n",
      "v 0 21 2\n",
      "v 0 0 2\n",
      "v 2 0 2\n",
      "v 2 0 2\n",
      "v 0 0 2\n",
      "v 0 0 0\n",
      "v 2 0 0\n",
      "v 0 0 0\n",
      "v 0 21 0\n",
      "v 2 20 0\n",
      "v 2 0 0\n",
      "v 2 35 2\n",
      "v 0 36 2\n",
      "v 0 21 2\n",
      "v 2 21 2\n",
      "v 2 21 0\n",
      "v 0 21 0\n",
      "v 0 36 0\n",
      "v 2 35 0\n",
      "v 0 52 2\n",
      "v 0 36 2\n",
      "v 2 36 2\n",
      "v 2 52 2\n",
      "v 2 36 0\n",
      "v 0 36 0\n",
      "v 0 52 0\n",
      "v 2 52 0\n",
      "v 61 0 0\n",
      "v 64 0 0\n",
      "v 64 0 2\n",
      "v 61 0 2\n",
      "v 61 2 2\n",
      "v 64 0 2\n",
      "v 64 52 2\n",
      "v 61 50 2\n",
      "v 61 50 0\n",
      "v 64 52 0\n",
      "v 64 0 0\n",
      "v 61 2 0\n",
      "v 2 0 2\n",
      "v 2 0 2\n",
      "v 61 0 2\n",
      "v 61 0 2\n",
      "v 2 0 0\n",
      "v 0 0 0\n",
      "v 0 0 0\n",
      "v 0 0 0\n",
      "v 0 0 0\n",
      "v 0 0 2\n",
      "v 0 0 2\n",
      "v 0 0 2\n",
      "v 2 0 2\n",
      "v 61 52 2\n",
      "v 64 52 2\n",
      "v 0 52 2\n",
      "v 2 52 2\n",
      "v 2 52 0\n",
      "v 0 52 0\n",
      "v 64 52 0\n",
      "v 61 52 0\n",
      "v 0 52 2\n",
      "v 0 52 0\n",
      "v 61 1 2\n",
      "v 61 1 0\n",
      "v 61 0 0\n",
      "v 61 0 2\n",
      "v 61 0 2\n",
      "v 61 0 0\n",
      "v 2 0 0\n",
      "v 2 0 2\n",
      "v 2 0 2\n",
      "v 2 0 0\n",
      "v 2 20 0\n",
      "v 2 20 2\n",
      "v 2 20 2\n",
      "v 2 20 0\n",
      "v 30 20 0\n",
      "v 30 20 2\n",
      "v 30 20 2\n",
      "v 30 20 0\n",
      "v 43 18 0\n",
      "v 43 18 2\n",
      "v 43 18 2\n",
      "v 43 18 0\n",
      "v 53 1 0\n",
      "v 53 1 2\n",
      "v 53 1 2\n",
      "v 53 1 0\n",
      "v 61 1 0\n",
      "v 61 1 2\n",
      "v 2 21 2\n",
      "v 2 21 0\n",
      "v 2 35 0\n",
      "v 2 35 2\n",
      "v 2 35 2\n",
      "v 2 35 0\n",
      "v 29 35 0\n",
      "v 29 35 2\n",
      "v 29 35 2\n",
      "v 29 35 0\n",
      "v 29 21 0\n",
      "v 29 21 2\n",
      "v 29 21 2\n",
      "v 29 21 0\n",
      "v 2 21 0\n",
      "v 2 21 2\n",
      "v 61 50 2\n",
      "v 61 50 0\n",
      "v 61 2 0\n",
      "v 61 2 2\n",
      "v 61 2 2\n",
      "v 61 2 0\n",
      "v 53 2 0\n",
      "v 53 2 2\n",
      "v 53 2 2\n",
      "v 53 2 0\n",
      "v 44 18 0\n",
      "v 44 18 2\n",
      "v 44 18 2\n",
      "v 44 18 0\n",
      "v 30 21 0\n",
      "v 30 21 2\n",
      "v 30 21 2\n",
      "v 30 21 0\n",
      "v 30 35 0\n",
      "v 30 35 2\n",
      "v 30 35 2\n",
      "v 30 35 0\n",
      "v 44 39 0\n",
      "v 44 39 2\n",
      "v 44 39 2\n",
      "v 44 39 0\n",
      "v 49 50 0\n",
      "v 49 50 2\n",
      "v 49 50 2\n",
      "v 49 50 0\n",
      "v 61 50 0\n",
      "v 61 50 2\n",
      "v 2 36 2\n",
      "v 2 36 0\n",
      "v 2 52 0\n",
      "v 2 52 2\n",
      "v 2 52 2\n",
      "v 2 52 0\n",
      "v 61 52 0\n",
      "v 61 52 2\n",
      "v 61 52 2\n",
      "v 61 52 0\n",
      "v 61 51 0\n",
      "v 61 51 2\n",
      "v 61 51 0\n",
      "v 48 50 0\n",
      "v 48 50 2\n",
      "v 61 51 2\n",
      "v 48 50 2\n",
      "v 48 50 0\n",
      "v 44 39 0\n",
      "v 44 39 2\n",
      "v 44 39 2\n",
      "v 44 39 0\n",
      "v 30 36 0\n",
      "v 30 36 2\n",
      "v 30 36 2\n",
      "v 30 36 0\n",
      "v 2 36 0\n",
      "v 2 36 2\n",
      "v 61 0 0\n",
      "v 2 0 0\n",
      "v 2 0 0\n",
      "v 61 0 0\n",
      "v 61 0 2\n",
      "v 2 0 2\n",
      "v 29 35 0\n",
      "v 30 36 0\n",
      "v 30 21 0\n",
      "v 30 20 0\n",
      "v 30 36 2\n",
      "v 29 35 2\n",
      "v 30 20 2\n",
      "v 2 35 2\n",
      "v 2 36 0\n",
      "v 2 20 2\n",
      "v 2 21 0\n",
      "v 61 0 2\n",
      "v 64 0 2\n",
      "v 61 1 2\n",
      "v 61 51 2\n",
      "v 61 52 2\n",
      "v 61 52 0\n",
      "v 61 51 0\n",
      "v 61 50 2\n",
      "v 61 51 0\n",
      "v 61 1 2\n",
      "v 61 2 0\n",
      "v 64 0 2\n",
      "v 0 52 2\n",
      "v 0 52 0\n",
      "v 0 52 0\n",
      "v 61 0 0\n",
      "v 61 0 0\n",
      "v 64 0 0\n",
      "v 64 0 0\n",
      "f 1 2 3\n",
      "f 2 4 3\n",
      "f 3 4 5\n",
      "f 4 6 5\n",
      "f 7 8 1\n",
      "f 7 1 3\n",
      "f 9 10 11\n",
      "f 9 11 12\n",
      "f 9 12 13\n",
      "f 9 13 14\n",
      "f 9 14 15\n",
      "f 16 17 18\n",
      "f 16 18 19\n",
      "f 16 19 20\n",
      "f 21 22 23\n",
      "f 21 23 24\n",
      "f 21 24 25\n",
      "f 21 25 26\n",
      "f 21 26 27\n",
      "f 28 29 30\n",
      "f 28 30 31\n",
      "f 28 31 32\n",
      "f 28 32 33\n",
      "f 28 33 34\n",
      "f 35 36 37\n",
      "f 35 37 38\n",
      "f 35 38 39\n",
      "f 35 39 40\n",
      "f 35 40 41\n",
      "f 42 20 19\n",
      "f 42 19 43\n",
      "f 42 43 44\n",
      "f 45 46 47\n",
      "f 45 47 48\n",
      "f 49 50 51\n",
      "f 49 51 52\n",
      "f 53 54 55\n",
      "f 53 55 56\n",
      "f 57 58 59\n",
      "f 57 59 60\n",
      "f 61 62 63\n",
      "f 61 63 64\n",
      "f 65 66 67\n",
      "f 65 67 68\n",
      "f 69 70 71\n",
      "f 69 71 72\n",
      "f 73 74 75\n",
      "f 73 75 76\n",
      "f 77 78 79\n",
      "f 77 79 80\n",
      "f 81 82 83\n",
      "f 81 83 84\n",
      "f 85 86 87\n",
      "f 85 87 88\n",
      "f 89 90 91\n",
      "f 89 91 92\n",
      "f 93 94 95\n",
      "f 93 95 96\n",
      "f 97 98 99\n",
      "f 97 99 100\n",
      "f 101 102 103\n",
      "f 101 103 104\n",
      "f 105 106 107\n",
      "f 105 107 108\n",
      "f 109 110 111\n",
      "f 109 111 112\n",
      "f 113 114 115\n",
      "f 113 115 116\n",
      "f 117 118 119\n",
      "f 117 119 120\n",
      "f 121 122 123\n",
      "f 121 123 124\n",
      "f 125 126 127\n",
      "f 125 127 128\n",
      "f 129 130 131\n",
      "f 129 131 132\n",
      "f 133 134 135\n",
      "f 133 135 136\n",
      "f 137 138 139\n",
      "f 137 139 140\n",
      "f 141 142 143\n",
      "f 141 143 144\n",
      "f 145 146 147\n",
      "f 145 147 148\n",
      "f 149 150 151\n",
      "f 149 151 152\n",
      "f 153 154 155\n",
      "f 153 155 156\n",
      "f 157 158 159\n",
      "f 157 159 160\n",
      "f 161 162 163\n",
      "f 161 163 164\n",
      "f 165 166 167\n",
      "f 165 167 168\n",
      "f 169 170 171\n",
      "f 169 171 172\n",
      "f 173 174 175\n",
      "f 173 175 176\n",
      "f 177 178 179\n",
      "f 177 179 180\n",
      "f 181 182 183\n",
      "f 181 183 184\n",
      "f 185 186 187\n",
      "f 185 187 188\n",
      "f 189 190 191\n",
      "f 189 191 192\n",
      "f 193 194 195\n",
      "f 193 195 196\n",
      "f 197 198 199\n",
      "f 197 199 200\n",
      "f 201 202 203\n",
      "f 201 203 204\n",
      "f 205 206 207\n",
      "f 205 207 208\n",
      "f 209 210 211\n",
      "f 209 211 212\n",
      "f 213 214 215\n",
      "f 213 215 216\n",
      "f 217 218 219\n",
      "f 217 219 220\n",
      "f 188 221 222\n",
      "f 188 222 223\n",
      "f 224 225 226\n",
      "f 224 226 160\n",
      "f 227 228 229\n",
      "f 227 229 180\n",
      "f 230 231 232\n",
      "f 230 232 233\n",
      "f 234 235 236\n",
      "f 234 236 237\n",
      "f 173 238 171\n",
      "f 173 171 239\n",
      "f 240 241 242\n",
      "f 240 242 243\n",
      "f 244 245 246\n",
      "f 244 246 247\n",
      "f 248 249 250\n",
      "f 248 250 251\n",
      "f 252 253 254\n",
      "f 252 254 255\n",
      "f 256 257 258\n",
      "f 256 258 259\n",
      "f 260 261 262\n",
      "f 260 262 263\n",
      "f 264 265 266\n",
      "f 264 266 267\n",
      "f 268 269 270\n",
      "f 268 270 271\n",
      "f 272 273 274\n",
      "f 272 274 275\n",
      "f 276 277 278\n",
      "f 276 278 279\n",
      "f 280 281 282\n",
      "f 280 282 283\n",
      "f 284 285 286\n",
      "f 284 286 287\n",
      "f 288 289 290\n",
      "f 288 290 291\n",
      "f 292 293 294\n",
      "f 292 294 295\n",
      "f 296 297 298\n",
      "f 296 298 299\n",
      "f 300 301 302\n",
      "f 300 302 303\n",
      "f 304 305 306\n",
      "f 304 306 307\n",
      "f 308 309 310\n",
      "f 308 310 311\n",
      "f 312 313 314\n",
      "f 312 314 315\n",
      "f 316 317 318\n",
      "f 316 318 319\n",
      "f 320 321 322\n",
      "f 320 322 323\n",
      "f 324 325 326\n",
      "f 324 326 327\n",
      "f 328 329 330\n",
      "f 328 330 331\n",
      "f 332 333 334\n",
      "f 332 334 335\n",
      "f 336 337 338\n",
      "f 336 338 339\n",
      "f 340 341 342\n",
      "f 340 342 343\n",
      "f 344 345 346\n",
      "f 344 346 347\n",
      "f 205 348 349\n",
      "f 205 349 184\n",
      "f 100 350 351\n",
      "f 352 353 103\n",
      "f 137 354 355\n",
      "f 145 139 356\n",
      "f 129 198 357\n",
      "f 96 195 358\n",
      "f 144 191 359\n",
      "f 107 186 360\n",
      "f 361 362 363\n",
      "f 364 211 365\n",
      "f 366 236 367\n",
      "f 368 211 128\n",
      "f 369 236 91\n",
      "f 370 362 156\n",
      "f 371 215 119\n",
      "f 361 219 372\n",
      "f 200 232 373\n",
      "f 374 375 204\n",
      "f 376 377 378\n",
      "f 379 377 119\n",
      "<|eot_id|><|eot_id|>\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 09:17:11,868 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-16 09:17:11,870 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 09:17:47,883 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unsloth--Llama-3.1-8B-Instruct/snapshots/f04b2ff47ccc1612d7592106b89b53eb49f7804d/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-16 09:17:47,885 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.546 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 09:17:50,262 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unslothai--kaggle/snapshots/b632e7c464e861a6f1762dd396048ab1ed7a10ec/config.json\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 09:17:50,544 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unslothai--repeat/snapshots/7c48478c02f84ed89f149b0815cc0216ee831fb0/config.json\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 09:17:51,633 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unslothai--vram-24/snapshots/61324ceeacd75b2b31f7a789a9c9d82058e6118c/config.json\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 09:17:52,760 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unslothai--1/snapshots/7ec782b7604cd9ea0781c23a4270f031650f5617/config.json\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 09:17:53,028 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unsloth--Llama-3.1-8B-Instruct/snapshots/f04b2ff47ccc1612d7592106b89b53eb49f7804d/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-16 09:17:53,030 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 09:17:53,332 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unsloth--Llama-3.1-8B-Instruct/snapshots/f04b2ff47ccc1612d7592106b89b53eb49f7804d/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-16 09:17:53,334 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3904] 2025-02-16 09:17:53,369 >> loading weights file model.safetensors from cache at /home/featurize/.cache/huggingface/hub/models--unsloth--Llama-3.1-8B-Instruct/snapshots/f04b2ff47ccc1612d7592106b89b53eb49f7804d/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1582] 2025-02-16 09:17:53,371 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1140] 2025-02-16 09:17:53,374 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"pad_token_id\": 128004\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.07it/s]\n",
      "[INFO|modeling_utils.py:4888] 2025-02-16 09:17:57,721 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4896] 2025-02-16 09:17:57,721 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/Llama-3.1-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-02-16 09:17:58,017 >> loading configuration file generation_config.json from cache at /home/featurize/.cache/huggingface/hub/models--unsloth--Llama-3.1-8B-Instruct/snapshots/f04b2ff47ccc1612d7592106b89b53eb49f7804d/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-02-16 09:17:58,018 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 131072,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-02-16 09:18:00] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-02-16 09:18:00] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-02-16 09:18:00] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-02-16 09:18:00] llamafactory.model.model_utils.misc:157 >> Found linear modules: q_proj,up_proj,gate_proj,v_proj,down_proj,k_proj,o_proj\n",
      "[WARNING|logging.py:328] 2025-02-16 09:18:06,746 >> Unsloth 2025.2.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "[INFO|2025-02-16 09:18:09] llamafactory.model.loader:157 >> trainable params: 335,544,320 || all params: 8,365,805,568 || trainable%: 4.0109\n",
      "[INFO|trainer.py:741] 2025-02-16 09:18:09,285 >> Using auto half precision backend\n",
      "[WARNING|<string>:215] 2025-02-16 09:18:09,771 >> ==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 47,156 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 64\n",
      "\\        /    Total batch size = 64 | Total steps = 1,472\n",
      " \"-____-\"     Number of trainable parameters = 335,544,320\n",
      "{'loss': 0.4029, 'grad_norm': 0.7407169938087463, 'learning_rate': 1.0096153846153846e-06, 'epoch': 0.01}\n",
      "{'loss': 0.4168, 'grad_norm': 0.7866204380989075, 'learning_rate': 2.0192307692307692e-06, 'epoch': 0.01}\n",
      "{'loss': 0.3987, 'grad_norm': 0.5493893027305603, 'learning_rate': 3.0288461538461535e-06, 'epoch': 0.02}\n",
      "{'loss': 0.3639, 'grad_norm': 0.434982568025589, 'learning_rate': 4.0384615384615385e-06, 'epoch': 0.03}\n",
      "{'loss': 0.3594, 'grad_norm': 0.304977685213089, 'learning_rate': 5.048076923076923e-06, 'epoch': 0.03}\n",
      "{'loss': 0.3444, 'grad_norm': 0.21245653927326202, 'learning_rate': 6.057692307692307e-06, 'epoch': 0.04}\n",
      "{'loss': 0.3512, 'grad_norm': 0.19359558820724487, 'learning_rate': 7.067307692307693e-06, 'epoch': 0.05}\n",
      "{'loss': 0.3188, 'grad_norm': 0.18368074297904968, 'learning_rate': 8.076923076923077e-06, 'epoch': 0.05}\n",
      "{'loss': 0.335, 'grad_norm': 0.15228047966957092, 'learning_rate': 9.086538461538462e-06, 'epoch': 0.06}\n",
      "{'loss': 0.3335, 'grad_norm': 0.15069861710071564, 'learning_rate': 1.0096153846153845e-05, 'epoch': 0.07}\n",
      "{'loss': 0.3322, 'grad_norm': 0.1524277627468109, 'learning_rate': 1.110576923076923e-05, 'epoch': 0.07}\n",
      "{'loss': 0.3368, 'grad_norm': 0.16549266874790192, 'learning_rate': 1.2115384615384614e-05, 'epoch': 0.08}\n",
      "{'loss': 0.32, 'grad_norm': 0.15318584442138672, 'learning_rate': 1.3124999999999999e-05, 'epoch': 0.09}\n",
      "{'loss': 0.31, 'grad_norm': 0.1606837511062622, 'learning_rate': 1.4134615384615386e-05, 'epoch': 0.1}\n",
      "{'loss': 0.3182, 'grad_norm': 0.15943460166454315, 'learning_rate': 1.5144230769230769e-05, 'epoch': 0.1}\n",
      "{'loss': 0.3116, 'grad_norm': 0.16616712510585785, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.11}\n",
      "{'loss': 0.2914, 'grad_norm': 0.19420593976974487, 'learning_rate': 1.7163461538461537e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2925, 'grad_norm': 0.16205070912837982, 'learning_rate': 1.8173076923076924e-05, 'epoch': 0.12}\n",
      "{'loss': 0.3079, 'grad_norm': 0.15372593700885773, 'learning_rate': 1.9182692307692307e-05, 'epoch': 0.13}\n",
      "{'loss': 0.3252, 'grad_norm': 0.1858271360397339, 'learning_rate': 2.019230769230769e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2895, 'grad_norm': 0.17124468088150024, 'learning_rate': 2.0999972312313867e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2937, 'grad_norm': 0.19667501747608185, 'learning_rate': 2.099900325863129e-05, 'epoch': 0.15}\n",
      "{'loss': 0.2998, 'grad_norm': 0.19949983060359955, 'learning_rate': 2.0996649966659172e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2844, 'grad_norm': 0.16631434857845306, 'learning_rate': 2.0992912746666668e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2812, 'grad_norm': 0.18348224461078644, 'learning_rate': 2.098779209138655e-05, 'epoch': 0.17}\n",
      "{'loss': 0.3041, 'grad_norm': 2.5272016525268555, 'learning_rate': 2.0981288675950258e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2905, 'grad_norm': 0.22203966975212097, 'learning_rate': 2.097340335779886e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3059, 'grad_norm': 0.3081614375114441, 'learning_rate': 2.096413717657003e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3014, 'grad_norm': 0.2038029283285141, 'learning_rate': 2.0953491353960975e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3012, 'grad_norm': 0.19660241901874542, 'learning_rate': 2.0941467293567327e-05, 'epoch': 0.2}\n",
      "{'loss': 0.2922, 'grad_norm': 0.157965749502182, 'learning_rate': 2.0928066580698135e-05, 'epoch': 0.21}\n",
      "{'loss': 0.2895, 'grad_norm': 0.1731056421995163, 'learning_rate': 2.0913290982166826e-05, 'epoch': 0.22}\n",
      "{'loss': 0.278, 'grad_norm': 0.16806715726852417, 'learning_rate': 2.089714244605824e-05, 'epoch': 0.22}\n",
      "{'loss': 0.2677, 'grad_norm': 0.15032051503658295, 'learning_rate': 2.0879623101471835e-05, 'epoch': 0.23}\n",
      "{'loss': 0.2759, 'grad_norm': 0.1836157590150833, 'learning_rate': 2.086073525824093e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2805, 'grad_norm': 0.1728236824274063, 'learning_rate': 2.0840481406628184e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2981, 'grad_norm': 0.17554514110088348, 'learning_rate': 2.0818864216997274e-05, 'epoch': 0.25}\n",
      "{'loss': 0.2725, 'grad_norm': 0.19204431772232056, 'learning_rate': 2.079588653946081e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2758, 'grad_norm': 0.1778772473335266, 'learning_rate': 2.0771551403504567e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2819, 'grad_norm': 0.17108629643917084, 'learning_rate': 2.0745862017588073e-05, 'epoch': 0.27}\n",
      "{'loss': 0.2596, 'grad_norm': 0.20241211354732513, 'learning_rate': 2.0718821768721573e-05, 'epoch': 0.28}\n",
      "{'loss': 0.2697, 'grad_norm': 0.1510620415210724, 'learning_rate': 2.0690434222019476e-05, 'epoch': 0.29}\n",
      "{'loss': 0.2551, 'grad_norm': 0.16905586421489716, 'learning_rate': 2.066070312023032e-05, 'epoch': 0.29}\n",
      "{'loss': 0.2659, 'grad_norm': 0.17185071110725403, 'learning_rate': 2.0629632383243314e-05, 'epoch': 0.3}\n",
      "{'loss': 0.2628, 'grad_norm': 0.16369910538196564, 'learning_rate': 2.0597226107571495e-05, 'epoch': 0.31}\n",
      "{'loss': 0.2769, 'grad_norm': 0.16006694734096527, 'learning_rate': 2.0563488565811654e-05, 'epoch': 0.31}\n",
      "{'loss': 0.2527, 'grad_norm': 0.13811808824539185, 'learning_rate': 2.0528424206081002e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2552, 'grad_norm': 0.1958446353673935, 'learning_rate': 2.0492037651430713e-05, 'epoch': 0.33}\n",
      "{'loss': 0.2598, 'grad_norm': 0.1471910923719406, 'learning_rate': 2.04543336992364e-05, 'epoch': 0.33}\n",
      "{'loss': 0.255, 'grad_norm': 0.18768629431724548, 'learning_rate': 2.04153173205656e-05, 'epoch': 0.34}\n",
      "{'loss': 0.2641, 'grad_norm': 0.19097769260406494, 'learning_rate': 2.0374993659522372e-05, 'epoch': 0.35}\n",
      "{'loss': 0.2555, 'grad_norm': 0.15696945786476135, 'learning_rate': 2.0333368032569074e-05, 'epoch': 0.35}\n",
      "{'loss': 0.2726, 'grad_norm': 0.1730719655752182, 'learning_rate': 2.0290445927825407e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2394, 'grad_norm': 0.14737680554389954, 'learning_rate': 2.024623300434484e-05, 'epoch': 0.37}\n",
      "{'loss': 0.257, 'grad_norm': 0.1614234447479248, 'learning_rate': 2.020073509136851e-05, 'epoch': 0.37}\n",
      "{'loss': 0.2549, 'grad_norm': 0.1508018672466278, 'learning_rate': 2.015395818755663e-05, 'epoch': 0.38}\n",
      "{'loss': 0.2631, 'grad_norm': 0.18132606148719788, 'learning_rate': 2.0105908460197634e-05, 'epoch': 0.39}\n",
      "{'loss': 0.2671, 'grad_norm': 0.16606397926807404, 'learning_rate': 2.0056592244395035e-05, 'epoch': 0.39}\n",
      "{'loss': 0.2779, 'grad_norm': 0.20905598998069763, 'learning_rate': 2.0006016042232173e-05, 'epoch': 0.4}\n",
      "{'loss': 0.2533, 'grad_norm': 0.17914411425590515, 'learning_rate': 1.995418652191496e-05, 'epoch': 0.41}\n",
      "{'loss': 0.2794, 'grad_norm': 0.2214992195367813, 'learning_rate': 1.9901110516892704e-05, 'epoch': 0.41}\n",
      "{'loss': 0.2463, 'grad_norm': 0.1737179309129715, 'learning_rate': 1.9846795024957156e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2576, 'grad_norm': 0.2087332010269165, 'learning_rate': 1.9791247207319892e-05, 'epoch': 0.43}\n",
      "{'loss': 0.273, 'grad_norm': 0.1813395917415619, 'learning_rate': 1.9734474387668133e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2478, 'grad_norm': 0.1779261976480484, 'learning_rate': 1.967648405119918e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2457, 'grad_norm': 0.18766796588897705, 'learning_rate': 1.9617283843633505e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2673, 'grad_norm': 0.2933553159236908, 'learning_rate': 1.955688157020671e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2612, 'grad_norm': 0.22801515460014343, 'learning_rate': 1.9495285194640462e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2541, 'grad_norm': 0.16673007607460022, 'learning_rate': 1.9432502838092487e-05, 'epoch': 0.47}\n",
      "{'loss': 0.2806, 'grad_norm': 0.19958674907684326, 'learning_rate': 1.936854277808588e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2665, 'grad_norm': 0.1807345449924469, 'learning_rate': 1.9303413447417722e-05, 'epoch': 0.48}\n",
      "{'loss': 0.274, 'grad_norm': 0.17934641242027283, 'learning_rate': 1.9237123433047274e-05, 'epoch': 0.49}\n",
      "{'loss': 0.259, 'grad_norm': 0.16289973258972168, 'learning_rate': 1.9169681474963834e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2512, 'grad_norm': 0.15818312764167786, 'learning_rate': 1.9101096465034415e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2478, 'grad_norm': 0.1771574765443802, 'learning_rate': 1.9031377445831383e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2774, 'grad_norm': 0.18343959748744965, 'learning_rate': 1.896053360944027e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2429, 'grad_norm': 0.19409893453121185, 'learning_rate': 1.888857429624782e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2627, 'grad_norm': 0.18371444940567017, 'learning_rate': 1.8815508993710534e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2417, 'grad_norm': 0.21080440282821655, 'learning_rate': 1.8741347335103762e-05, 'epoch': 0.54}\n",
      "{'loss': 0.274, 'grad_norm': 0.20110106468200684, 'learning_rate': 1.8666099098251646e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2385, 'grad_norm': 0.16442885994911194, 'learning_rate': 1.8589774204237938e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2576, 'grad_norm': 0.1678166538476944, 'learning_rate': 1.8512382716097967e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2531, 'grad_norm': 0.17481274902820587, 'learning_rate': 1.843393483749189e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2553, 'grad_norm': 0.19965192675590515, 'learning_rate': 1.835444091135937e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2561, 'grad_norm': 0.41139623522758484, 'learning_rate': 1.8273911418555942e-05, 'epoch': 0.58}\n",
      "{'loss': 0.2613, 'grad_norm': 0.2132491171360016, 'learning_rate': 1.8192356976471137e-05, 'epoch': 0.58}\n",
      "{'loss': 0.2493, 'grad_norm': 0.19358761608600616, 'learning_rate': 1.810978833762867e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2399, 'grad_norm': 0.1857142299413681, 'learning_rate': 1.8026216388268736e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2572, 'grad_norm': 0.21152018010616302, 'learning_rate': 1.7941652146912762e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2345, 'grad_norm': 0.17683860659599304, 'learning_rate': 1.7856106762910622e-05, 'epoch': 0.61}\n",
      "{'loss': 0.259, 'grad_norm': 0.17416253685951233, 'learning_rate': 1.7769591514970708e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2563, 'grad_norm': 0.17990092933177948, 'learning_rate': 1.7682117809672853e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2607, 'grad_norm': 0.17784243822097778, 'learning_rate': 1.7593697179964434e-05, 'epoch': 0.63}\n",
      "{'loss': 0.2573, 'grad_norm': 0.21946962177753448, 'learning_rate': 1.7504341283639838e-05, 'epoch': 0.64}\n",
      "{'loss': 0.237, 'grad_norm': 0.1783546656370163, 'learning_rate': 1.7414061901803432e-05, 'epoch': 0.64}\n",
      "{'loss': 0.237, 'grad_norm': 0.20574486255645752, 'learning_rate': 1.7322870937316276e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2615, 'grad_norm': 0.17575253546237946, 'learning_rate': 1.7230780413226805e-05, 'epoch': 0.66}\n",
      "{'loss': 0.2464, 'grad_norm': 0.1849721521139145, 'learning_rate': 1.7137802471185654e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2525, 'grad_norm': 0.19198918342590332, 'learning_rate': 1.7043949369844833e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2427, 'grad_norm': 0.17548398673534393, 'learning_rate': 1.6949233483241513e-05, 'epoch': 0.68}\n",
      " 34%|███████████▌                      | 500/1472 [11:16:28<21:51:09, 80.94s/it][INFO|trainer.py:3910] 2025-02-16 20:34:39,493 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/sft/216v3/checkpoint-500\n",
      "[INFO|configuration_utils.py:696] 2025-02-16 20:34:40,149 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unsloth--Llama-3.1-8B-Instruct/snapshots/f04b2ff47ccc1612d7592106b89b53eb49f7804d/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-16 20:34:40,151 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "{'loss': 0.2251, 'grad_norm': 0.1567573845386505, 'learning_rate': 1.6853667299166535e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2486, 'grad_norm': 0.19055013358592987, 'learning_rate': 1.6757263417518e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2434, 'grad_norm': 0.19431200623512268, 'learning_rate': 1.6660034548640024e-05, 'epoch': 0.7}\n",
      "{'loss': 0.2317, 'grad_norm': 0.20772086083889008, 'learning_rate': 1.6561993511646937e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2504, 'grad_norm': 0.20419245958328247, 'learning_rate': 1.6463153232733158e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2526, 'grad_norm': 0.17647184431552887, 'learning_rate': 1.6363526743468947e-05, 'epoch': 0.72}\n",
      "{'loss': 0.2356, 'grad_norm': 0.1944543421268463, 'learning_rate': 1.626312717908226e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2463, 'grad_norm': 0.1807279735803604, 'learning_rate': 1.616196777672693e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2455, 'grad_norm': 0.1648789644241333, 'learning_rate': 1.6060061873737442e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2575, 'grad_norm': 0.18112201988697052, 'learning_rate': 1.5957422905870428e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2388, 'grad_norm': 0.17106780409812927, 'learning_rate': 1.5854064405533296e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2378, 'grad_norm': 0.18617361783981323, 'learning_rate': 1.575e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2268, 'grad_norm': 0.15375061333179474, 'learning_rate': 1.564524340961438e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2391, 'grad_norm': 0.17645569145679474, 'learning_rate': 1.5539808445981212e-05, 'epoch': 0.77}\n",
      "{'loss': 0.267, 'grad_norm': 0.21240215003490448, 'learning_rate': 1.5433709010145207e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2565, 'grad_norm': 0.2665491998195648, 'learning_rate': 1.5326959090758237e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2491, 'grad_norm': 0.23608069121837616, 'learning_rate': 1.5219572762234996e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2506, 'grad_norm': 0.318478524684906, 'learning_rate': 1.5111564182897388e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2342, 'grad_norm': 0.22022636234760284, 'learning_rate': 1.50029475931078e-05, 'epoch': 0.81}\n",
      "{'loss': 0.2545, 'grad_norm': 0.19628120958805084, 'learning_rate': 1.489373731339159e-05, 'epoch': 0.81}\n",
      "{'loss': 0.2598, 'grad_norm': 0.1846761703491211, 'learning_rate': 1.478394774254902e-05, 'epoch': 0.82}\n",
      "{'loss': 0.2268, 'grad_norm': 0.211562842130661, 'learning_rate': 1.4673593355756825e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2545, 'grad_norm': 0.2106846123933792, 'learning_rate': 1.4562688702659755e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2372, 'grad_norm': 0.17824393510818481, 'learning_rate': 1.4451248405452274e-05, 'epoch': 0.84}\n",
      "{'loss': 0.266, 'grad_norm': 0.17880713939666748, 'learning_rate': 1.4339287156950692e-05, 'epoch': 0.85}\n",
      "{'loss': 0.2659, 'grad_norm': 0.17688052356243134, 'learning_rate': 1.4226819718656013e-05, 'epoch': 0.86}\n",
      "{'loss': 0.2371, 'grad_norm': 0.1659204065799713, 'learning_rate': 1.4113860918807705e-05, 'epoch': 0.86}\n",
      "{'loss': 0.2393, 'grad_norm': 0.15440896153450012, 'learning_rate': 1.4000425650428654e-05, 'epoch': 0.87}\n",
      "{'loss': 0.2196, 'grad_norm': 0.15120309591293335, 'learning_rate': 1.3886528869361618e-05, 'epoch': 0.88}\n",
      "{'loss': 0.2556, 'grad_norm': 0.20481853187084198, 'learning_rate': 1.3772185592297384e-05, 'epoch': 0.88}\n",
      "{'loss': 0.2233, 'grad_norm': 0.16866421699523926, 'learning_rate': 1.3657410894794867e-05, 'epoch': 0.89}\n",
      "{'loss': 0.2262, 'grad_norm': 0.18268944323062897, 'learning_rate': 1.3542219909293509e-05, 'epoch': 0.9}\n",
      "{'loss': 0.2051, 'grad_norm': 0.16038726270198822, 'learning_rate': 1.3426627823118122e-05, 'epoch': 0.9}\n",
      "{'loss': 0.243, 'grad_norm': 0.19115851819515228, 'learning_rate': 1.3310649876476538e-05, 'epoch': 0.91}\n",
      "{'loss': 0.2527, 'grad_norm': 0.19986853003501892, 'learning_rate': 1.3194301360450254e-05, 'epoch': 0.92}\n",
      "{'loss': 0.2493, 'grad_norm': 0.17533357441425323, 'learning_rate': 1.3077597614978392e-05, 'epoch': 0.92}\n",
      "{'loss': 0.2439, 'grad_norm': 0.19255352020263672, 'learning_rate': 1.2960554026835202e-05, 'epoch': 0.93}\n",
      "{'loss': 0.2391, 'grad_norm': 0.155012845993042, 'learning_rate': 1.2843186027601403e-05, 'epoch': 0.94}\n",
      "{'loss': 0.2564, 'grad_norm': 0.16319182515144348, 'learning_rate': 1.272550909162961e-05, 'epoch': 0.94}\n",
      "{'loss': 0.2393, 'grad_norm': 0.17071689665317535, 'learning_rate': 1.260753873400412e-05, 'epoch': 0.95}\n",
      "{'loss': 0.2341, 'grad_norm': 0.16779126226902008, 'learning_rate': 1.2489290508495318e-05, 'epoch': 0.96}\n",
      "{'loss': 0.2561, 'grad_norm': 0.16733303666114807, 'learning_rate': 1.2370780005509028e-05, 'epoch': 0.96}\n",
      "{'loss': 0.2386, 'grad_norm': 0.17899735271930695, 'learning_rate': 1.2252022850030957e-05, 'epoch': 0.97}\n",
      "{'loss': 0.2434, 'grad_norm': 0.1896630972623825, 'learning_rate': 1.2133034699566653e-05, 'epoch': 0.98}\n",
      "{'loss': 0.2341, 'grad_norm': 0.16550832986831665, 'learning_rate': 1.2013831242077139e-05, 'epoch': 0.98}\n",
      "{'loss': 0.2382, 'grad_norm': 0.18761061131954193, 'learning_rate': 1.1894428193910535e-05, 'epoch': 0.99}\n",
      "{'loss': 0.2598, 'grad_norm': 0.170895054936409, 'learning_rate': 1.1774841297729936e-05, 'epoch': 1.0}\n",
      "{'loss': 0.2306, 'grad_norm': 0.15488623082637787, 'learning_rate': 1.1655086320437832e-05, 'epoch': 1.0}\n",
      "{'loss': 0.2521, 'grad_norm': 0.17572319507598877, 'learning_rate': 1.153517905109732e-05, 'epoch': 1.01}\n",
      "{'loss': 0.218, 'grad_norm': 0.16679710149765015, 'learning_rate': 1.1415135298850413e-05, 'epoch': 1.02}\n",
      "{'loss': 0.2348, 'grad_norm': 0.17513272166252136, 'learning_rate': 1.1294970890833655e-05, 'epoch': 1.02}\n",
      "{'loss': 0.2613, 'grad_norm': 0.19954082369804382, 'learning_rate': 1.1174701670091437e-05, 'epoch': 1.03}\n",
      "{'loss': 0.2522, 'grad_norm': 0.20174269378185272, 'learning_rate': 1.1054343493487146e-05, 'epoch': 1.04}\n",
      "{'loss': 0.24, 'grad_norm': 0.19999577105045319, 'learning_rate': 1.0933912229612538e-05, 'epoch': 1.04}\n",
      "{'loss': 0.2307, 'grad_norm': 0.17492763698101044, 'learning_rate': 1.0813423756695537e-05, 'epoch': 1.05}\n",
      "{'loss': 0.22, 'grad_norm': 191.52023315429688, 'learning_rate': 1.0692893960506771e-05, 'epoch': 1.06}\n",
      "{'loss': 0.2415, 'grad_norm': 0.1958514004945755, 'learning_rate': 1.0572338732265137e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2365, 'grad_norm': 0.16046470403671265, 'learning_rate': 1.0451773966542606e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2233, 'grad_norm': 0.18078263103961945, 'learning_rate': 1.0331215559168619e-05, 'epoch': 1.08}\n",
      "{'loss': 0.2337, 'grad_norm': 0.20705056190490723, 'learning_rate': 1.02106794051343e-05, 'epoch': 1.09}\n",
      "{'loss': 0.2386, 'grad_norm': 0.20310528576374054, 'learning_rate': 1.0090181396496782e-05, 'epoch': 1.09}\n",
      "{'loss': 0.2314, 'grad_norm': 0.18981190025806427, 'learning_rate': 9.969737420283942e-06, 'epoch': 1.1}\n",
      "{'loss': 0.2209, 'grad_norm': 0.166929692029953, 'learning_rate': 9.849363356399758e-06, 'epoch': 1.11}\n",
      "{'loss': 0.2435, 'grad_norm': 0.1782020926475525, 'learning_rate': 9.729075075530629e-06, 'epoch': 1.11}\n",
      "{'loss': 0.2263, 'grad_norm': 0.19145889580249786, 'learning_rate': 9.60888843705293e-06, 'epoch': 1.12}\n",
      "{'loss': 0.2393, 'grad_norm': 0.1689966470003128, 'learning_rate': 9.488819286942013e-06, 'epoch': 1.13}\n",
      "{'loss': 0.2642, 'grad_norm': 0.1737370491027832, 'learning_rate': 9.368883455683003e-06, 'epoch': 1.13}\n",
      "{'loss': 0.2328, 'grad_norm': 0.18085666000843048, 'learning_rate': 9.249096756183651e-06, 'epoch': 1.14}\n",
      "{'loss': 0.2254, 'grad_norm': 0.16020914912223816, 'learning_rate': 9.12947498168946e-06, 'epoch': 1.15}\n",
      "{'loss': 0.2346, 'grad_norm': 0.16511502861976624, 'learning_rate': 9.010033903701439e-06, 'epoch': 1.15}\n",
      "{'loss': 0.2352, 'grad_norm': 0.16817039251327515, 'learning_rate': 8.89078926989672e-06, 'epoch': 1.16}\n",
      "{'loss': 0.235, 'grad_norm': 0.18952776491641998, 'learning_rate': 8.771756802052292e-06, 'epoch': 1.17}\n",
      "{'loss': 0.2132, 'grad_norm': 0.15754948556423187, 'learning_rate': 8.652952193972204e-06, 'epoch': 1.17}\n",
      "{'loss': 0.2436, 'grad_norm': 0.1740715056657791, 'learning_rate': 8.53439110941837e-06, 'epoch': 1.18}\n",
      "{'loss': 0.2171, 'grad_norm': 0.1615869700908661, 'learning_rate': 8.41608918004542e-06, 'epoch': 1.19}\n",
      "{'loss': 0.2394, 'grad_norm': 0.1728506088256836, 'learning_rate': 8.298062003339741e-06, 'epoch': 1.19}\n",
      "{'loss': 0.2342, 'grad_norm': 0.16225500404834747, 'learning_rate': 8.18032514056302e-06, 'epoch': 1.2}\n",
      "{'loss': 0.2238, 'grad_norm': 0.16471192240715027, 'learning_rate': 8.06289411470059e-06, 'epoch': 1.21}\n",
      "{'loss': 0.2322, 'grad_norm': 0.16378045082092285, 'learning_rate': 7.945784408414785e-06, 'epoch': 1.21}\n",
      "{'loss': 0.2316, 'grad_norm': 0.19649246335029602, 'learning_rate': 7.829011462003658e-06, 'epoch': 1.22}\n",
      "{'loss': 0.2344, 'grad_norm': 0.15426281094551086, 'learning_rate': 7.712590671365243e-06, 'epoch': 1.23}\n",
      "{'loss': 0.2329, 'grad_norm': 0.17679448425769806, 'learning_rate': 7.596537385967681e-06, 'epoch': 1.23}\n",
      "{'loss': 0.2258, 'grad_norm': 0.17294751107692719, 'learning_rate': 7.480866906825492e-06, 'epoch': 1.24}\n",
      "{'loss': 0.2265, 'grad_norm': 0.16667360067367554, 'learning_rate': 7.365594484482209e-06, 'epoch': 1.25}\n",
      "{'loss': 0.2492, 'grad_norm': 0.18805156648159027, 'learning_rate': 7.250735316999661e-06, 'epoch': 1.26}\n",
      "{'loss': 0.2177, 'grad_norm': 0.15190304815769196, 'learning_rate': 7.1363045479542074e-06, 'epoch': 1.26}\n",
      "{'loss': 0.2276, 'grad_norm': 0.17052367329597473, 'learning_rate': 7.0223172644401315e-06, 'epoch': 1.27}\n",
      "{'loss': 0.2289, 'grad_norm': 0.17559009790420532, 'learning_rate': 6.908788495080478e-06, 'epoch': 1.28}\n",
      "{'loss': 0.2497, 'grad_norm': 0.176185742020607, 'learning_rate': 6.795733208045623e-06, 'epoch': 1.28}\n",
      "{'loss': 0.2486, 'grad_norm': 0.2375318855047226, 'learning_rate': 6.683166309079785e-06, 'epoch': 1.29}\n",
      "{'loss': 0.2283, 'grad_norm': 0.15035268664360046, 'learning_rate': 6.5711026395357945e-06, 'epoch': 1.3}\n",
      "{'loss': 0.2443, 'grad_norm': 0.1725156307220459, 'learning_rate': 6.4595569744183216e-06, 'epoch': 1.3}\n",
      "{'loss': 0.225, 'grad_norm': 0.17392875254154205, 'learning_rate': 6.348544020435891e-06, 'epoch': 1.31}\n",
      "{'loss': 0.2311, 'grad_norm': 0.1710028350353241, 'learning_rate': 6.238078414061868e-06, 'epoch': 1.32}\n",
      "{'loss': 0.2244, 'grad_norm': 0.1632748246192932, 'learning_rate': 6.128174719604711e-06, 'epoch': 1.32}\n",
      "{'loss': 0.232, 'grad_norm': 0.1887950748205185, 'learning_rate': 6.018847427287762e-06, 'epoch': 1.33}\n",
      "{'loss': 0.2203, 'grad_norm': 0.17166516184806824, 'learning_rate': 5.9101109513387666e-06, 'epoch': 1.34}\n",
      "{'loss': 0.246, 'grad_norm': 0.17911726236343384, 'learning_rate': 5.801979628089455e-06, 'epoch': 1.34}\n",
      "{'loss': 0.242, 'grad_norm': 0.16901324689388275, 'learning_rate': 5.694467714085345e-06, 'epoch': 1.35}\n",
      "{'loss': 0.2276, 'grad_norm': 0.1937755048274994, 'learning_rate': 5.58758938420612e-06, 'epoch': 1.36}\n",
      " 68%|██████████████████████▍          | 1000/1472 [22:34:07<10:48:16, 82.41s/it][INFO|trainer.py:3910] 2025-02-17 07:52:18,287 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/sft/216v3/checkpoint-1000\n",
      "[INFO|configuration_utils.py:696] 2025-02-17 07:52:19,059 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unsloth--Llama-3.1-8B-Instruct/snapshots/f04b2ff47ccc1612d7592106b89b53eb49f7804d/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-17 07:52:19,061 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "{'loss': 0.2174, 'grad_norm': 0.17964407801628113, 'learning_rate': 5.481358729796729e-06, 'epoch': 1.36}\n",
      "{'loss': 0.2294, 'grad_norm': 0.15715603530406952, 'learning_rate': 5.375789756809522e-06, 'epoch': 1.37}\n",
      "{'loss': 0.2288, 'grad_norm': 0.1810445636510849, 'learning_rate': 5.270896383957636e-06, 'epoch': 1.38}\n",
      "{'loss': 0.2485, 'grad_norm': 0.15973375737667084, 'learning_rate': 5.166692440879892e-06, 'epoch': 1.38}\n",
      "{'loss': 0.2344, 'grad_norm': 0.16826032102108002, 'learning_rate': 5.06319166631742e-06, 'epoch': 1.39}\n",
      "{'loss': 0.2389, 'grad_norm': 0.16732081770896912, 'learning_rate': 4.9604077063022914e-06, 'epoch': 1.4}\n",
      "{'loss': 0.2157, 'grad_norm': 0.15907074511051178, 'learning_rate': 4.858354112358348e-06, 'epoch': 1.4}\n",
      "{'loss': 0.2209, 'grad_norm': 0.1763055920600891, 'learning_rate': 4.757044339714519e-06, 'epoch': 1.41}\n",
      "{'loss': 0.2447, 'grad_norm': 0.18994459509849548, 'learning_rate': 4.656491745530801e-06, 'epoch': 1.42}\n",
      "{'loss': 0.2175, 'grad_norm': 0.17024283111095428, 'learning_rate': 4.556709587137206e-06, 'epoch': 1.42}\n",
      "{'loss': 0.2325, 'grad_norm': 0.16485810279846191, 'learning_rate': 4.457711020285844e-06, 'epoch': 1.43}\n",
      "{'loss': 0.2261, 'grad_norm': 0.17484794557094574, 'learning_rate': 4.359509097416406e-06, 'epoch': 1.44}\n",
      "{'loss': 0.2201, 'grad_norm': 0.16133928298950195, 'learning_rate': 4.26211676593527e-06, 'epoch': 1.45}\n",
      "{'loss': 0.2289, 'grad_norm': 0.18768921494483948, 'learning_rate': 4.165546866508453e-06, 'epoch': 1.45}\n",
      "{'loss': 0.2396, 'grad_norm': 0.18348872661590576, 'learning_rate': 4.06981213136863e-06, 'epoch': 1.46}\n",
      "{'loss': 0.2479, 'grad_norm': 0.19575653970241547, 'learning_rate': 3.9749251826364715e-06, 'epoch': 1.47}\n",
      "{'loss': 0.2326, 'grad_norm': 0.20011383295059204, 'learning_rate': 3.8808985306564725e-06, 'epoch': 1.47}\n",
      "{'loss': 0.2412, 'grad_norm': 0.18011406064033508, 'learning_rate': 3.787744572347531e-06, 'epoch': 1.48}\n",
      "{'loss': 0.2398, 'grad_norm': 0.16400736570358276, 'learning_rate': 3.6954755895684797e-06, 'epoch': 1.49}\n",
      "{'loss': 0.2333, 'grad_norm': 0.17115028202533722, 'learning_rate': 3.604103747498789e-06, 'epoch': 1.49}\n",
      "{'loss': 0.237, 'grad_norm': 0.18117189407348633, 'learning_rate': 3.513641093034651e-06, 'epoch': 1.5}\n",
      "{'loss': 0.243, 'grad_norm': 0.17702051997184753, 'learning_rate': 3.4240995532006608e-06, 'epoch': 1.51}\n",
      "{'loss': 0.2206, 'grad_norm': 0.22119440138339996, 'learning_rate': 3.3354909335773e-06, 'epoch': 1.51}\n",
      "{'loss': 0.2219, 'grad_norm': 0.17949126660823822, 'learning_rate': 3.2478269167444347e-06, 'epoch': 1.52}\n",
      "{'loss': 0.2478, 'grad_norm': 0.16772598028182983, 'learning_rate': 3.1611190607410206e-06, 'epoch': 1.53}\n",
      "{'loss': 0.2424, 'grad_norm': 0.16793711483478546, 'learning_rate': 3.0753787975412514e-06, 'epoch': 1.53}\n",
      "{'loss': 0.2318, 'grad_norm': 0.18327870965003967, 'learning_rate': 2.9906174315473036e-06, 'epoch': 1.54}\n",
      "{'loss': 0.2412, 'grad_norm': 0.16125230491161346, 'learning_rate': 2.906846138098909e-06, 'epoch': 1.55}\n",
      "{'loss': 0.2061, 'grad_norm': 0.17657122015953064, 'learning_rate': 2.824075961999955e-06, 'epoch': 1.55}\n",
      "{'loss': 0.2137, 'grad_norm': 0.19228966534137726, 'learning_rate': 2.7423178160622677e-06, 'epoch': 1.56}\n",
      "{'loss': 0.2053, 'grad_norm': 0.17633670568466187, 'learning_rate': 2.6615824796668357e-06, 'epoch': 1.57}\n",
      "{'loss': 0.2241, 'grad_norm': 0.1665387600660324, 'learning_rate': 2.5818805973425906e-06, 'epoch': 1.57}\n",
      "{'loss': 0.2125, 'grad_norm': 0.16157743334770203, 'learning_rate': 2.503222677362989e-06, 'epoch': 1.58}\n",
      "{'loss': 0.211, 'grad_norm': 0.14833617210388184, 'learning_rate': 2.4256190903605524e-06, 'epoch': 1.59}\n",
      "{'loss': 0.2406, 'grad_norm': 0.16652245819568634, 'learning_rate': 2.3490800679595468e-06, 'epoch': 1.59}\n",
      "{'loss': 0.2329, 'grad_norm': 0.18267151713371277, 'learning_rate': 2.2736157014270056e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2361, 'grad_norm': 0.15801256895065308, 'learning_rate': 2.1992359403422424e-06, 'epoch': 1.61}\n",
      "{'loss': 0.2212, 'grad_norm': 0.17260980606079102, 'learning_rate': 2.1259505912850555e-06, 'epoch': 1.61}\n",
      "{'loss': 0.2117, 'grad_norm': 0.1862441599369049, 'learning_rate': 2.0537693165427853e-06, 'epoch': 1.62}\n",
      "{'loss': 0.2253, 'grad_norm': 0.17323079705238342, 'learning_rate': 1.9827016328363774e-06, 'epoch': 1.63}\n",
      "{'loss': 0.2458, 'grad_norm': 0.16966691613197327, 'learning_rate': 1.912756910065673e-06, 'epoch': 1.64}\n",
      "{'loss': 0.2162, 'grad_norm': 0.1572270542383194, 'learning_rate': 1.8439443700740235e-06, 'epoch': 1.64}\n",
      "{'loss': 0.2466, 'grad_norm': 0.13408367335796356, 'learning_rate': 1.7762730854324469e-06, 'epoch': 1.65}\n",
      "{'loss': 0.2176, 'grad_norm': 0.16343408823013306, 'learning_rate': 1.7097519782434502e-06, 'epoch': 1.66}\n",
      "{'loss': 0.225, 'grad_norm': 0.1715601682662964, 'learning_rate': 1.6443898189647021e-06, 'epoch': 1.66}\n",
      "{'loss': 0.2325, 'grad_norm': 0.16194438934326172, 'learning_rate': 1.5801952252526878e-06, 'epoch': 1.67}\n",
      "{'loss': 0.2172, 'grad_norm': 4.676455497741699, 'learning_rate': 1.5171766608265285e-06, 'epoch': 1.68}\n",
      "{'loss': 0.2279, 'grad_norm': 0.15983904898166656, 'learning_rate': 1.455342434352073e-06, 'epoch': 1.68}\n",
      "{'loss': 0.2341, 'grad_norm': 0.17658476531505585, 'learning_rate': 1.3947006983464485e-06, 'epoch': 1.69}\n",
      "{'loss': 0.2272, 'grad_norm': 0.1698773056268692, 'learning_rate': 1.3352594481031949e-06, 'epoch': 1.7}\n",
      "{'loss': 0.2394, 'grad_norm': 0.17939509451389313, 'learning_rate': 1.2770265206381253e-06, 'epoch': 1.7}\n",
      "{'loss': 0.235, 'grad_norm': 0.17560340464115143, 'learning_rate': 1.220009593656059e-06, 'epoch': 1.71}\n",
      "{'loss': 0.2316, 'grad_norm': 0.16980955004692078, 'learning_rate': 1.1642161845385546e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2111, 'grad_norm': 0.14878389239311218, 'learning_rate': 1.1096536493527842e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2208, 'grad_norm': 0.17375557124614716, 'learning_rate': 1.0563291818816732e-06, 'epoch': 1.73}\n",
      "{'loss': 0.2152, 'grad_norm': 0.1590663343667984, 'learning_rate': 1.004249812675437e-06, 'epoch': 1.74}\n",
      "{'loss': 0.2157, 'grad_norm': 0.14792118966579437, 'learning_rate': 9.534224081246459e-07, 'epoch': 1.74}\n",
      "{'loss': 0.2405, 'grad_norm': 0.18248350918293, 'learning_rate': 9.038536695549195e-07, 'epoch': 1.75}\n",
      "{'loss': 0.2227, 'grad_norm': 0.1701756864786148, 'learning_rate': 8.555501323434e-07, 'epoch': 1.76}\n",
      "{'loss': 0.2233, 'grad_norm': 0.1549939066171646, 'learning_rate': 8.085181650570929e-07, 'epoch': 1.76}\n",
      "{'loss': 0.2328, 'grad_norm': 0.19297994673252106, 'learning_rate': 7.627639686132001e-07, 'epoch': 1.77}\n",
      "{'loss': 0.2125, 'grad_norm': 0.15940237045288086, 'learning_rate': 7.182935754615717e-07, 'epoch': 1.78}\n",
      "{'loss': 0.2343, 'grad_norm': 0.17604726552963257, 'learning_rate': 6.751128487893558e-07, 'epoch': 1.78}\n",
      "{'loss': 0.2257, 'grad_norm': 0.1673153042793274, 'learning_rate': 6.332274817479627e-07, 'epoch': 1.79}\n",
      "{'loss': 0.2459, 'grad_norm': 0.16857579350471497, 'learning_rate': 5.926429967024597e-07, 'epoch': 1.8}\n",
      "{'loss': 0.2309, 'grad_norm': 0.16821402311325073, 'learning_rate': 5.533647445034736e-07, 'epoch': 1.8}\n",
      "{'loss': 0.2128, 'grad_norm': 0.15382924675941467, 'learning_rate': 5.153979037817092e-07, 'epoch': 1.81}\n",
      "{'loss': 0.2138, 'grad_norm': 0.16640326380729675, 'learning_rate': 4.787474802651712e-07, 'epoch': 1.82}\n",
      "{'loss': 0.22, 'grad_norm': 0.16236183047294617, 'learning_rate': 4.4341830611919446e-07, 'epoch': 1.83}\n",
      "{'loss': 0.2167, 'grad_norm': 0.15749527513980865, 'learning_rate': 4.094150393093371e-07, 'epoch': 1.83}\n",
      "{'loss': 0.2347, 'grad_norm': 0.16109885275363922, 'learning_rate': 3.76742162987257e-07, 'epoch': 1.84}\n",
      "{'loss': 0.2411, 'grad_norm': 0.18753647804260254, 'learning_rate': 3.45403984899638e-07, 'epoch': 1.85}\n",
      "{'loss': 0.2378, 'grad_norm': 0.1501423865556717, 'learning_rate': 3.15404636820224e-07, 'epoch': 1.85}\n",
      "{'loss': 0.2372, 'grad_norm': 0.18725284934043884, 'learning_rate': 2.8674807400507865e-07, 'epoch': 1.86}\n",
      "{'loss': 0.2357, 'grad_norm': 0.14884036779403687, 'learning_rate': 2.5943807467109536e-07, 'epoch': 1.87}\n",
      "{'loss': 0.2217, 'grad_norm': 0.1641802340745926, 'learning_rate': 2.3347823949786838e-07, 'epoch': 1.87}\n",
      "{'loss': 0.2422, 'grad_norm': 0.1782694309949875, 'learning_rate': 2.088719911529513e-07, 'epoch': 1.88}\n",
      "{'loss': 0.2198, 'grad_norm': 0.17155280709266663, 'learning_rate': 1.8562257384060813e-07, 'epoch': 1.89}\n",
      "{'loss': 0.2424, 'grad_norm': 0.1672501266002655, 'learning_rate': 1.637330528740742e-07, 'epoch': 1.89}\n",
      "{'loss': 0.2299, 'grad_norm': 0.15863899886608124, 'learning_rate': 1.432063142714156e-07, 'epoch': 1.9}\n",
      "{'loss': 0.2049, 'grad_norm': 0.17037692666053772, 'learning_rate': 1.2404506437501644e-07, 'epoch': 1.91}\n",
      "{'loss': 0.2499, 'grad_norm': 0.1653563380241394, 'learning_rate': 1.0625182949476946e-07, 'epoch': 1.91}\n",
      "{'loss': 0.2443, 'grad_norm': 0.1763637810945511, 'learning_rate': 8.982895557499098e-08, 'epoch': 1.92}\n",
      "{'loss': 0.2262, 'grad_norm': 0.18735070526599884, 'learning_rate': 7.477860788511891e-08, 'epoch': 1.93}\n",
      "{'loss': 0.2345, 'grad_norm': 0.1551639884710312, 'learning_rate': 6.110277073423965e-08, 'epoch': 1.93}\n",
      "{'loss': 0.2303, 'grad_norm': 0.16639018058776855, 'learning_rate': 4.880324720945994e-08, 'epoch': 1.94}\n",
      "{'loss': 0.2471, 'grad_norm': 0.1846466064453125, 'learning_rate': 3.788165893818468e-08, 'epoch': 1.95}\n",
      "{'loss': 0.2183, 'grad_norm': 0.1508273184299469, 'learning_rate': 2.8339445874312973e-08, 'epoch': 1.95}\n",
      "{'loss': 0.2398, 'grad_norm': 0.19098958373069763, 'learning_rate': 2.01778661083844e-08, 'epoch': 1.96}\n",
      "{'loss': 0.2215, 'grad_norm': 20.831586837768555, 'learning_rate': 1.3397995701712516e-08, 'epoch': 1.97}\n",
      "{'loss': 0.2244, 'grad_norm': 0.1549319326877594, 'learning_rate': 8.000728544503344e-09, 'epoch': 1.97}\n",
      "{'loss': 0.2427, 'grad_norm': 0.16994807124137878, 'learning_rate': 3.986776238010136e-09, 'epoch': 1.98}\n",
      "{'loss': 0.2325, 'grad_norm': 0.15986481308937073, 'learning_rate': 1.356668000705774e-09, 'epoch': 1.99}\n",
      "{'loss': 0.2217, 'grad_norm': 0.1733306348323822, 'learning_rate': 1.1075059851123336e-10, 'epoch': 1.99}\n",
      "100%|████████████████████████████████████| 1472/1472 [33:17:32<00:00, 84.94s/it][INFO|trainer.py:3910] 2025-02-17 18:35:43,820 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/sft/216v3/checkpoint-1472\n",
      "[INFO|configuration_utils.py:696] 2025-02-17 18:35:44,528 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unsloth--Llama-3.1-8B-Instruct/snapshots/f04b2ff47ccc1612d7592106b89b53eb49f7804d/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-17 18:35:44,529 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|<string>:494] 2025-02-17 18:39:57,545 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 120106.4933, 'train_samples_per_second': 0.785, 'train_steps_per_second': 0.012, 'train_loss': 0.2494365125692085, 'epoch': 2.0}\n",
      "100%|████████████████████████████████████| 1472/1472 [33:21:46<00:00, 81.59s/it]\n",
      "[INFO|trainer.py:3910] 2025-02-17 18:39:57,792 >> Saving model checkpoint to saves/Llama-3.1-8B-Instruct/lora/sft/216v3\n",
      "[INFO|configuration_utils.py:696] 2025-02-17 18:39:58,486 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--unsloth--Llama-3.1-8B-Instruct/snapshots/f04b2ff47ccc1612d7592106b89b53eb49f7804d/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-17 18:39:58,488 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "***** train metrics *****\n",
      "  epoch                    =            1.9975\n",
      "  total_flos               =     12221933455GF\n",
      "  train_loss               =            0.2494\n",
      "  train_runtime            = 1 day, 9:21:46.49\n",
      "  train_samples_per_second =             0.785\n",
      "  train_steps_per_second   =             0.012\n",
      "Figure saved at: saves/Llama-3.1-8B-Instruct/lora/sft/216v3/training_loss.png\n",
      "[WARNING|2025-02-17 18:40:20] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-02-17 18:40:20] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:449] 2025-02-17 18:40:20,762 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "# 指令监督微调\n",
    "!PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True llamafactory-cli train ./config/llama-3.1-8B-216-v1/llama3_1_8b_lora_sft.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b486afb-0c44-4f5a-8f22-c29f804a8c0a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-20 15:55:36.954901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740066936.976129    4050 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740066936.982471    4050 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-20 15:55:37.004316: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[INFO|configuration_utils.py:696] 2025-02-20 15:55:42,878 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-20 15:55:42,880 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:43,152 >> loading file tokenizer.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:43,153 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:43,153 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:43,153 >> loading file special_tokens_map.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:43,153 >> loading file tokenizer_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:43,153 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-02-20 15:55:43,773 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:696] 2025-02-20 15:55:45,018 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-20 15:55:45,020 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:45,295 >> loading file tokenizer.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:45,295 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:45,295 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:45,295 >> loading file special_tokens_map.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:45,295 >> loading file tokenizer_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2034] 2025-02-20 15:55:45,295 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-02-20 15:55:45,875 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-02-20 15:55:45] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-02-20 15:55:45] llamafactory.data.template:157 >> Add <|eot_id|> to stop words.\n",
      "[INFO|configuration_utils.py:696] 2025-02-20 15:55:46,180 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-02-20 15:55:46,182 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-02-20 15:55:46] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3904] 2025-02-20 15:55:46,224 >> loading weights file model.safetensors from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1582] 2025-02-20 15:55:46,225 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1140] 2025-02-20 15:55:46,227 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:06<00:00,  1.62s/it]\n",
      "[INFO|modeling_utils.py:4888] 2025-02-20 15:55:52,956 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4896] 2025-02-20 15:55:52,956 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.1-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-02-20 15:55:53,254 >> loading configuration file generation_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-02-20 15:55:53,255 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-02-20 15:55:53] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-02-20 15:55:57] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
      "[INFO|2025-02-20 15:55:57] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.1-8B-Instruct/lora/sft/216v3\n",
      "[INFO|2025-02-20 15:55:57] llamafactory.model.loader:157 >> all params: 8,030,261,248\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://a8418c836a4b00a418.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    }
   ],
   "source": [
    "# 使用浏览器对话框推理 LoRA 模型\n",
    "!GRADIO_SHARE=1 llamafactory-cli webchat ./config/llama-3.1-8B-216-v1/llama3_1_8b_lora_sft_infer.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d275884-e860-4b4b-934c-4cdd31cb2ac6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/environment/miniconda3/bin/llamafactory-cli\", line 5, in <module>\n",
      "    from llamafactory.cli import main\n",
      "  File \"/home/featurize/work/LLaMA-Factory-4-2/src/llamafactory/__init__.py\", line 44, in <module>\n",
      "    from .extras.env import VERSION\n",
      "  File \"/home/featurize/work/LLaMA-Factory-4-2/src/llamafactory/extras/env.py\", line 20, in <module>\n",
      "    import accelerate\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/accelerate/__init__.py\", line 16, in <module>\n",
      "    from .accelerator import Accelerator\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/accelerate/accelerator.py\", line 32, in <module>\n",
      "    import torch\n",
      "  File \"/environment/miniconda3/lib/python3.11/site-packages/torch/__init__.py\", line 405, in <module>\n",
      "    from torch._C import *  # noqa: F403\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 216, in _lock_unlock_module\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 合并 LoRA 适配器\n",
    "!llamafactory-cli export ./config/temp_lora_sft_merge_lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de6cef-7fe7-4237-b63a-71196c84d928",
   "metadata": {},
   "source": [
    "以下是指令参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1d61485-df5b-4526-b4cf-3facb1a6a596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!llamafactory-cli train ./llama3_lora_sft.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc7576a-b5a5-4646-95af-145df30ca445",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-15 12:01:46.834640: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736942506.855379    2924 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736942506.861600    2924 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-15 12:01:46.883563: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "config.json: 100%|█████████████████████████████| 877/877 [00:00<00:00, 2.70MB/s]\n",
      "[INFO|configuration_utils.py:679] 2025-01-15 12:01:52,957 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-15 12:01:52,958 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100%|████████████████| 54.5k/54.5k [00:00<00:00, 897kB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 9.39MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 296/296 [00:00<00:00, 1.02MB/s]\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:01:56,278 >> loading file tokenizer.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:01:56,279 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:01:56,279 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:01:56,279 >> loading file special_tokens_map.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:01:56,279 >> loading file tokenizer_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-01-15 12:01:56,862 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:679] 2025-01-15 12:01:57,992 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-15 12:01:57,995 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:01:58,289 >> loading file tokenizer.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:01:58,289 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:01:58,289 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:01:58,289 >> loading file special_tokens_map.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-15 12:01:58,289 >> loading file tokenizer_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-01-15 12:01:58,820 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-01-15 12:01:58] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-01-15 12:01:58] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.\n",
      "[INFO|configuration_utils.py:679] 2025-01-15 12:01:59,121 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-15 12:01:59,123 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-01-15 12:01:59] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
      "model.safetensors: 100%|████████████████████| 2.47G/2.47G [00:05<00:00, 422MB/s]\n",
      "[INFO|modeling_utils.py:3937] 2025-01-15 12:02:05,741 >> loading weights file model.safetensors from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/model.safetensors\n",
      "[INFO|modeling_utils.py:1670] 2025-01-15 12:02:05,782 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2025-01-15 12:02:05,784 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4800] 2025-01-15 12:02:07,196 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2025-01-15 12:02:07,196 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "generation_config.json: 100%|██████████████████| 189/189 [00:00<00:00, 1.05MB/s]\n",
      "[INFO|configuration_utils.py:1051] 2025-01-15 12:02:07,792 >> loading configuration file generation_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2025-01-15 12:02:07,792 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-01-15 12:02:07] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-01-15 12:02:09] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
      "[INFO|2025-01-15 12:02:09] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.2-1B-Instruct/lora/sft\n",
      "[INFO|2025-01-15 12:02:09] llamafactory.model.loader:157 >> all params: 1,235,814,400\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://56fc3df9cfde7401fd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:7860 <> https://56fc3df9cfde7401fd.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!GRADIO_SHARE=1 llamafactory-cli webchat ./config/llama3_lora_sft_infer.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae2fcb0e-28a6-40e2-9275-ba1de9633c7f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-14 19:17:01.327628: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-14 19:17:01.337872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736882221.350164    3230 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736882221.353984    3230 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-14 19:17:01.366551: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[INFO|configuration_utils.py:679] 2025-01-14 19:17:06,022 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-14 19:17:06,024 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 19:17:06,296 >> loading file tokenizer.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 19:17:06,296 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 19:17:06,296 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 19:17:06,296 >> loading file special_tokens_map.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 19:17:06,296 >> loading file tokenizer_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-01-14 19:17:06,624 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:679] 2025-01-14 19:17:07,701 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-14 19:17:07,703 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 19:17:07,975 >> loading file tokenizer.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 19:17:07,975 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 19:17:07,975 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 19:17:07,976 >> loading file special_tokens_map.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 19:17:07,976 >> loading file tokenizer_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-01-14 19:17:08,293 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-01-14 19:17:08] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-01-14 19:17:08] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.\n",
      "[INFO|configuration_utils.py:679] 2025-01-14 19:17:08,587 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-14 19:17:08,587 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-01-14 19:17:08] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3937] 2025-01-14 19:17:08,606 >> loading weights file model.safetensors from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/model.safetensors\n",
      "[INFO|modeling_utils.py:1670] 2025-01-14 19:17:08,610 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2025-01-14 19:17:08,611 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4800] 2025-01-14 19:17:08,885 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2025-01-14 19:17:08,886 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1051] 2025-01-14 19:17:09,146 >> loading configuration file generation_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2025-01-14 19:17:09,146 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-01-14 19:17:09] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-01-14 19:17:12] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
      "[INFO|2025-01-14 19:17:12] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.2-1B-Instruct/lora/sft\n",
      "[INFO|2025-01-14 19:17:12] llamafactory.model.loader:157 >> all params: 1,235,814,400\n",
      "[INFO|2025-01-14 19:17:12] llamafactory.train.tuner:157 >> Convert model dtype to: torch.bfloat16.\n",
      "[INFO|configuration_utils.py:414] 2025-01-14 19:17:12,136 >> Configuration saved in myModels/llama3.2-1B_lora_sft/config.json\n",
      "[INFO|configuration_utils.py:865] 2025-01-14 19:17:12,137 >> Configuration saved in myModels/llama3.2-1B_lora_sft/generation_config.json\n",
      "[INFO|modeling_utils.py:3043] 2025-01-14 19:17:26,621 >> The model is bigger than the maximum size per checkpoint (2GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at myModels/llama3.2-1B_lora_sft/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2646] 2025-01-14 19:17:26,629 >> tokenizer config file saved in myModels/llama3.2-1B_lora_sft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2025-01-14 19:17:26,630 >> Special tokens file saved in myModels/llama3.2-1B_lora_sft/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli export ./config/llama3_lora_sft_merge_lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80370de7-e9f0-4da8-bf0e-7f117801ada0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-14 20:24:01.310677: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-14 20:24:01.321032: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736886241.333397    8409 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736886241.337216    8409 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-14 20:24:01.349726: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[INFO|configuration_utils.py:679] 2025-01-14 20:24:06,458 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-14 20:24:06,460 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 20:24:06,778 >> loading file tokenizer.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 20:24:06,778 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 20:24:06,778 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 20:24:06,779 >> loading file special_tokens_map.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 20:24:06,779 >> loading file tokenizer_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-01-14 20:24:07,106 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:679] 2025-01-14 20:24:08,401 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-14 20:24:08,402 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 20:24:08,713 >> loading file tokenizer.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 20:24:08,713 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 20:24:08,713 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 20:24:08,713 >> loading file special_tokens_map.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2211] 2025-01-14 20:24:08,713 >> loading file tokenizer_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2475] 2025-01-14 20:24:09,030 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-01-14 20:24:09] llamafactory.data.template:157 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-01-14 20:24:09] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.\n",
      "[INFO|configuration_utils.py:679] 2025-01-14 20:24:09,391 >> loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json\n",
      "[INFO|configuration_utils.py:746] 2025-01-14 20:24:09,392 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-01-14 20:24:09] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3937] 2025-01-14 20:24:09,411 >> loading weights file model.safetensors from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/model.safetensors\n",
      "[INFO|modeling_utils.py:1670] 2025-01-14 20:24:09,414 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1096] 2025-01-14 20:24:09,415 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4800] 2025-01-14 20:24:10,069 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2025-01-14 20:24:10,069 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1051] 2025-01-14 20:24:10,384 >> loading configuration file generation_config.json from cache at /home/featurize/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2025-01-14 20:24:10,384 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-01-14 20:24:10] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-01-14 20:24:10] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
      "[INFO|2025-01-14 20:24:10] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.2-1B-Instruct/lora/sft\n",
      "[INFO|2025-01-14 20:24:10] llamafactory.model.loader:157 >> all params: 1,235,814,400\n",
      "Visit http://localhost:8000/docs for API document.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m8409\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:47072 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:39618 - \"\u001b[1mGET /docs HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:39618 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli api ./config/llama3_lora_sft_infer.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6d56f-87e3-4425-9b7f-77e4579d3917",
   "metadata": {},
   "source": [
    "## full 微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a91f9f7-0fd0-4250-a143-243e1986cae3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-30 01:00:05.604813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745974805.622399    5813 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745974805.627873    5813 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745974805.640822    5813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745974805.640854    5813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745974805.640858    5813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745974805.640861    5813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-30 01:00:05.644802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[INFO|2025-04-30 01:00:10] llamafactory.hparams.parser:386 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|configuration_utils.py:694] 2025-04-30 01:00:10,605 >> loading configuration file /home/featurize/work/LLaMA-Factory-4-2/saves/Qwen2.5-1.5B-Instruct/full/221_v3/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-04-30 01:00:10,606 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/home/featurize/work/LLaMA-Factory-4-2/saves/Qwen2.5-1.5B-Instruct/full/221_v3\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,610 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,610 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,610 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,610 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,610 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,610 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,610 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-04-30 01:00:10,986 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:694] 2025-04-30 01:00:10,988 >> loading configuration file /home/featurize/work/LLaMA-Factory-4-2/saves/Qwen2.5-1.5B-Instruct/full/221_v3/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-04-30 01:00:10,989 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/home/featurize/work/LLaMA-Factory-4-2/saves/Qwen2.5-1.5B-Instruct/full/221_v3\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,990 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,990 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,990 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,990 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,990 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,990 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-04-30 01:00:10,991 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-04-30 01:00:11,395 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-04-30 01:00:11] llamafactory.data.template:157 >> Add <|eot_id|> to stop words.\n",
      "[INFO|2025-04-30 01:00:11] llamafactory.data.loader:157 >> Loading dataset /home/featurize/data/data_sharegpt/data_sharegp_special_6k.json...\n",
      "Converting format of dataset (num_proc=30): 100%|█| 16881/16881 [00:00<00:00, 44\n",
      "Running tokenizer on dataset (num_proc=30): 100%|█| 16881/16881 [00:07<00:00, 22\n",
      "training example:\n",
      "input_ids:\n",
      "[27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91, 408, 8757, 842, 91, 1339, 38849, 264, 220, 18, 35, 5944, 315, 264, 9317, 18126, 2899, 424, 448, 264, 6303, 12426, 389, 279, 6006, 13, 151665, 27, 91, 2468, 8757, 842, 91, 29, 77091, 27, 91, 408, 8757, 842, 91, 1339, 85, 220, 18, 15, 220, 16, 16, 220, 22, 198, 85, 220, 18, 17, 220, 16, 16, 220, 22, 198, 85, 220, 18, 15, 220, 17, 22, 220, 22, 198, 85, 220, 18, 17, 220, 17, 22, 220, 22, 198, 85, 220, 18, 15, 220, 18, 23, 220, 22, 198, 85, 220, 18, 17, 220, 18, 23, 220, 22, 198, 85, 220, 18, 15, 220, 20, 16, 220, 22, 198, 85, 220, 18, 17, 220, 20, 16, 220, 22, 198, 85, 220, 18, 15, 220, 16, 16, 220, 23, 198, 85, 220, 18, 16, 220, 16, 16, 220, 23, 198, 85, 220, 18, 17, 220, 16, 16, 220, 23, 198, 85, 220, 18, 15, 220, 17, 22, 220, 23, 198, 85, 220, 18, 16, 220, 17, 22, 220, 23, 198, 85, 220, 18, 17, 220, 17, 22, 220, 23, 198, 85, 220, 18, 15, 220, 18, 23, 220, 23, 198, 85, 220, 18, 17, 220, 18, 23, 220, 23, 198, 85, 220, 18, 15, 220, 18, 24, 220, 23, 198, 85, 220, 18, 16, 220, 18, 24, 220, 23, 198, 85, 220, 18, 17, 220, 18, 24, 220, 23, 198, 85, 220, 18, 15, 220, 20, 16, 220, 23, 198, 85, 220, 18, 16, 220, 20, 16, 220, 23, 198, 85, 220, 18, 17, 220, 20, 16, 220, 23, 198, 85, 220, 18, 16, 220, 16, 21, 220, 16, 17, 198, 85, 220, 18, 16, 220, 16, 22, 220, 16, 17, 198, 85, 220, 18, 17, 220, 16, 21, 220, 16, 19, 198, 85, 220, 18, 17, 220, 16, 22, 220, 16, 19, 198, 85, 220, 18, 16, 220, 17, 19, 220, 16, 20, 198, 85, 220, 18, 16, 220, 17, 20, 220, 16, 20, 198, 85, 220, 18, 17, 220, 17, 19, 220, 16, 22, 198, 85, 220, 18, 17, 220, 17, 20, 220, 16, 22, 198, 85, 220, 18, 16, 220, 16, 18, 220, 17, 17, 198, 85, 220, 18, 16, 220, 16, 19, 220, 17, 17, 198, 85, 220, 18, 16, 220, 16, 18, 220, 17, 19, 198, 85, 220, 18, 16, 220, 16, 19, 220, 17, 19, 198, 85, 220, 18, 15, 220, 17, 22, 220, 17, 24, 198, 85, 220, 18, 17, 220, 17, 22, 220, 17, 24, 198, 85, 220, 18, 15, 220, 18, 23, 220, 17, 24, 198, 85, 220, 18, 17, 220, 18, 23, 220, 17, 24, 198, 85, 220, 18, 15, 220, 17, 22, 220, 18, 15, 198, 85, 220, 18, 16, 220, 17, 22, 220, 18, 15, 198, 85, 220, 18, 17, 220, 17, 22, 220, 18, 15, 198, 85, 220, 18, 15, 220, 18, 23, 220, 18, 15, 198, 85, 220, 18, 16, 220, 18, 23, 220, 18, 15, 198, 85, 220, 18, 17, 220, 18, 23, 220, 18, 15, 198, 85, 220, 18, 15, 220, 18, 24, 220, 18, 15, 198, 85, 220, 18, 16, 220, 18, 24, 220, 18, 15, 198, 85, 220, 18, 17, 220, 18, 24, 220, 18, 15, 198, 85, 220, 18, 15, 220, 17, 20, 220, 19, 15, 198, 85, 220, 18, 16, 220, 17, 20, 220, 19, 15, 198, 85, 220, 18, 17, 220, 17, 20, 220, 19, 15, 198, 85, 220, 18, 15, 220, 19, 16, 220, 19, 15, 198, 85, 220, 18, 16, 220, 19, 16, 220, 19, 15, 198, 85, 220, 18, 17, 220, 19, 16, 220, 19, 15, 198, 85, 220, 18, 15, 220, 19, 16, 220, 19, 16, 198, 85, 220, 18, 16, 220, 19, 16, 220, 19, 16, 198, 85, 220, 18, 17, 220, 19, 16, 220, 19, 16, 198, 85, 220, 18, 15, 220, 19, 24, 220, 19, 19, 198, 85, 220, 18, 16, 220, 19, 24, 220, 19, 19, 198, 85, 220, 18, 17, 220, 19, 24, 220, 19, 19, 198, 85, 220, 18, 15, 220, 20, 15, 220, 19, 19, 198, 85, 220, 18, 16, 220, 20, 15, 220, 19, 19, 198, 85, 220, 18, 17, 220, 20, 15, 220, 19, 19, 198, 85, 220, 18, 15, 220, 16, 17, 220, 19, 22, 198, 85, 220, 18, 16, 220, 16, 17, 220, 19, 22, 198, 85, 220, 18, 17, 220, 16, 17, 220, 19, 22, 198, 85, 220, 18, 16, 220, 19, 18, 220, 19, 22, 198, 85, 220, 18, 16, 220, 19, 19, 220, 19, 22, 198, 85, 220, 18, 15, 220, 16, 18, 220, 19, 23, 198, 85, 220, 18, 16, 220, 16, 18, 220, 19, 23, 198, 85, 220, 18, 17, 220, 16, 18, 220, 19, 23, 198, 85, 220, 18, 17, 220, 19, 18, 220, 19, 24, 198, 85, 220, 18, 17, 220, 19, 19, 220, 19, 24, 198, 85, 220, 18, 15, 220, 16, 16, 220, 20, 19, 198, 85, 220, 18, 16, 220, 16, 16, 220, 20, 19, 198, 85, 220, 18, 17, 220, 16, 16, 220, 20, 19, 198, 85, 220, 18, 15, 220, 16, 17, 220, 20, 19, 198, 85, 220, 18, 16, 220, 16, 17, 220, 20, 19, 198, 85, 220, 18, 17, 220, 16, 17, 220, 20, 19, 198, 85, 220, 18, 15, 220, 16, 18, 220, 20, 19, 198, 85, 220, 18, 16, 220, 16, 18, 220, 20, 19, 198, 85, 220, 18, 17, 220, 16, 18, 220, 20, 19, 198, 85, 220, 18, 15, 220, 19, 24, 220, 20, 19, 198, 85, 220, 18, 16, 220, 19, 24, 220, 20, 19, 198, 85, 220, 18, 17, 220, 19, 24, 220, 20, 19, 198, 85, 220, 18, 15, 220, 20, 15, 220, 20, 19, 198, 85, 220, 18, 16, 220, 20, 15, 220, 20, 19, 198, 85, 220, 18, 17, 220, 20, 15, 220, 20, 19, 198, 85, 220, 18, 15, 220, 20, 16, 220, 20, 19, 198, 85, 220, 18, 16, 220, 20, 16, 220, 20, 19, 198, 85, 220, 18, 17, 220, 20, 16, 220, 20, 19, 198, 85, 220, 18, 15, 220, 16, 16, 220, 20, 20, 198, 85, 220, 18, 17, 220, 16, 16, 220, 20, 20, 198, 85, 220, 18, 15, 220, 20, 16, 220, 20, 20, 198, 85, 220, 18, 17, 220, 20, 16, 220, 20, 20, 198, 69, 220, 16, 220, 17, 220, 19, 198, 69, 220, 16, 220, 16, 16, 220, 17, 198, 69, 220, 16, 220, 19, 220, 18, 198, 69, 220, 16, 220, 18, 220, 16, 17, 198, 69, 220, 16, 220, 24, 220, 16, 16, 198, 69, 220, 16, 220, 16, 17, 220, 24, 198, 69, 220, 17, 220, 16, 19, 220, 19, 198, 69, 220, 17, 220, 16, 16, 220, 16, 19, 198, 69, 220, 18, 220, 19, 220, 21, 198, 69, 220, 18, 220, 21, 220, 20, 198, 69, 220, 18, 220, 20, 220, 16, 17, 198, 69, 220, 19, 220, 16, 21, 220, 21, 198, 69, 220, 19, 220, 16, 19, 220, 16, 21, 198, 69, 220, 20, 220, 21, 220, 23, 198, 69, 220, 20, 220, 23, 220, 22, 198, 69, 220, 20, 220, 22, 220, 16, 22, 198, 69, 220, 20, 220, 16, 20, 220, 16, 17, 198, 69, 220, 20, 220, 16, 22, 220, 16, 20, 198, 69, 220, 21, 220, 16, 24, 220, 23, 198, 69, 220, 21, 220, 16, 21, 220, 16, 24, 198, 69, 220, 22, 220, 23, 220, 24, 19, 198, 69, 220, 22, 220, 17, 15, 220, 16, 22, 198, 69, 220, 22, 220, 24, 18, 220, 17, 15, 198, 69, 220, 22, 220, 24, 19, 220, 24, 18, 198, 69, 220, 23, 220, 16, 24, 220, 17, 17, 198, 69, 220, 23, 220, 17, 17, 220, 24, 15, 198, 69, 220, 23, 220, 24, 15, 220, 24, 19, 198, 69, 220, 24, 220, 16, 17, 220, 16, 16, 198, 69, 220, 24, 220, 22, 18, 220, 16, 16, 198, 69, 220, 24, 220, 16, 16, 220, 22, 20, 198, 69, 220, 24, 220, 22, 20, 220, 22, 18, 198, 69, 220, 16, 15, 220, 19, 15, 220, 16, 18, 198, 69, 220, 16, 15, 220, 16, 18, 220, 19, 15, 198, 69, 220, 16, 15, 220, 19, 24, 220, 19, 15, 198, 69, 220, 16, 15, 220, 19, 15, 220, 19, 24, 198, 69, 220, 16, 15, 220, 21, 19, 220, 19, 24, 198, 69, 220, 16, 15, 220, 19, 24, 220, 21, 19, 198, 69, 220, 16, 15, 220, 22, 22, 220, 21, 19, 198, 69, 220, 16, 15, 220, 21, 19, 220, 22, 22, 198, 69, 220, 16, 15, 220, 22, 19, 220, 22, 22, 198, 69, 220, 16, 15, 220, 22, 22, 220, 22, 19, 198, 69, 220, 16, 16, 220, 16, 17, 220, 16, 19, 198, 69, 220, 16, 16, 220, 22, 18, 220, 22, 20, 198, 69, 220, 16, 17, 220, 16, 20, 220, 16, 19, 198, 69, 220, 16, 17, 220, 16, 19, 220, 18, 21, 198, 69, 220, 16, 17, 220, 18, 24, 220, 16, 19, 198, 69, 220, 16, 17, 220, 16, 20, 220, 18, 22, 198, 69, 220, 16, 17, 220, 18, 21, 220, 18, 20, 198, 69, 220, 16, 17, 220, 18, 22, 220, 18, 20, 198, 69, 220, 16, 17, 220, 18, 20, 220, 18, 24, 198, 69, 220, 16, 19, 220, 16, 20, 220, 16, 21, 198, 69, 220, 16, 19, 220, 18, 23, 220, 16, 21, 198, 69, 220, 16, 19, 220, 18, 21, 220, 18, 23, 198, 69, 220, 16, 19, 220, 19, 16, 220, 18, 21, 198, 69, 220, 16, 19, 220, 18, 24, 220, 19, 16, 198, 69, 220, 16, 20, 220, 18, 22, 220, 16, 21, 198, 69, 220, 16, 20, 220, 16, 22, 220, 18, 22, 198, 69, 220, 16, 21, 220, 18, 23, 220, 16, 24, 198, 69, 220, 16, 21, 220, 18, 22, 220, 18, 23, 198, 69, 220, 16, 22, 220, 17, 15, 220, 16, 24, 198, 69, 220, 16, 22, 220, 16, 24, 220, 19, 22, 198, 69, 220, 16, 22, 220, 19, 20, 220, 18, 22, 198, 69, 220, 16, 22, 220, 19, 22, 220, 19, 20, 198, 69, 220, 16, 23, 220, 19, 21, 220, 17, 16, 198, 69, 220, 16, 23, 220, 17, 16, 220, 19, 21, 198, 69, 220, 16, 24, 220, 17, 15, 220, 17, 17, 198, 69, 220, 16, 24, 220, 18, 23, 220, 19, 22, 198, 69, 220, 17, 15, 220, 23, 23, 220, 17, 17, 198, 69, 220, 17, 15, 220, 24, 18, 220, 23, 23, 198, 69, 220, 17, 16, 220, 19, 21, 220, 20, 17, 198, 69, 220, 17, 16, 220, 20, 17, 220, 19, 21, 198, 69, 220, 17, 16, 220, 20, 17, 220, 21, 16, 198, 69, 220, 17, 16, 220, 21, 16, 220, 20, 17, 198, 69, 220, 17, 16, 220, 21, 16, 220, 23, 21, 198, 69, 220, 17, 16, 220, 23, 21, 220, 21, 16, 198, 69, 220, 17, 16, 220, 23, 21, 220, 23, 24, 198, 69, 220, 17, 16, 220, 23, 24, 220, 23, 21, 198, 69, 220, 17, 17, 220, 23, 23, 220, 24, 15, 198, 69, 220, 17, 18, 220, 17, 19, 220, 17, 21, 198, 69, 220, 17, 18, 220, 17, 21, 220, 17, 19, 198, 69, 220, 17, 18, 220, 17, 21, 220, 17, 20, 198, 69, 220, 17, 18, 220, 17, 20, 220, 17, 21, 198, 69, 220, 17, 22, 220, 17, 23, 220, 18, 15, 198, 69, 220, 17, 22, 220, 18, 15, 220, 17, 23, 198, 69, 220, 17, 22, 220, 18, 15, 220, 17, 24, 198, 69, 220, 17, 22, 220, 17, 24, 220, 18, 15, 198, 69, 220, 18, 16, 220, 18, 17, 220, 18, 19, 198, 69, 220, 18, 16, 220, 18, 19, 220, 18, 17, 198, 69, 220, 18, 16, 220, 18, 19, 220, 18, 18, 198, 69, 220, 18, 16, 220, 18, 18, 220, 18, 19, 198, 69, 220, 18, 20, 220, 18, 21, 220, 18, 23, 198, 69, 220, 18, 20, 220, 18, 23, 220, 18, 22, 198, 69, 220, 18, 20, 220, 18, 22, 220, 19, 17, 198, 69, 220, 18, 20, 220, 19, 17, 220, 18, 24, 198, 69, 220, 18, 21, 220, 19, 19, 220, 18, 23, 198, 69, 220, 18, 21, 220, 19, 16, 220, 19, 19, 198, 69, 220, 18, 22, 220, 19, 20, 220, 19, 17, 198, 69, 220, 18, 23, 220, 19, 19, 220, 19, 22, 198, 69, 220, 18, 24, 220, 19, 17, 220, 19, 16, 198, 69, 220, 18, 24, 220, 19, 23, 220, 19, 16, 198, 69, 220, 18, 24, 220, 19, 16, 220, 20, 15, 198, 69, 220, 18, 24, 220, 20, 15, 220, 19, 23, 198, 69, 220, 19, 15, 220, 19, 18, 220, 19, 24, 198, 69, 220, 19, 15, 220, 19, 24, 220, 19, 18, 198, 69, 220, 19, 16, 220, 19, 17, 220, 19, 19, 198, 69, 220, 19, 16, 220, 19, 23, 220, 20, 15, 198, 69, 220, 19, 17, 220, 20, 19, 220, 19, 19, 198, 69, 220, 19, 17, 220, 19, 20, 220, 20, 19, 198, 69, 220, 19, 18, 220, 20, 20, 220, 19, 24, 198, 69, 220, 19, 18, 220, 19, 24, 220, 20, 20, 198, 69, 220, 19, 19, 220, 20, 21, 220, 19, 22, 198, 69, 220, 19, 19, 220, 20, 19, 220, 20, 21, 198, 69, 220, 19, 20, 220, 19, 22, 220, 20, 18, 198, 69, 220, 19, 20, 220, 20, 18, 220, 20, 16, 198, 69, 220, 19, 20, 220, 20, 16, 220, 20, 19, 198, 69, 220, 19, 22, 220, 20, 21, 220, 20, 18, 198, 69, 220, 19, 23, 220, 21, 18, 220, 20, 15, 198, 69, 220, 19, 23, 220, 20, 15, 220, 22, 15, 198, 69, 220, 19, 23, 220, 21, 23, 220, 21, 18, 198, 69, 220, 19, 23, 220, 22, 15, 220, 21, 23, 198, 69, 220, 19, 24, 220, 20, 20, 220, 21, 24, 198, 69, 220, 19, 24, 220, 21, 24, 220, 20, 20, 198, 69, 220, 20, 15, 220, 21, 18, 220, 21, 20, 198, 69, 220, 20, 15, 220, 21, 20, 220, 22, 15, 198, 69, 220, 20, 16, 220, 20, 18, 220, 21, 17, 198, 69, 220, 20, 16, 220, 20, 22, 220, 20, 19, 198, 69, 220, 20, 16, 220, 21, 15, 220, 20, 22, 198, 69, 220, 20, 16, 220, 21, 17, 220, 21, 15, 198, 69, 220, 20, 18, 220, 20, 21, 220, 20, 24, 198, 69, 220, 20, 18, 220, 20, 24, 220, 21, 17, 198, 69, 220, 20, 19, 220, 20, 22, 220, 20, 21, 198, 69, 220, 20, 20, 220, 20, 23, 220, 23, 15, 198, 69, 220, 20, 20, 220, 23, 15, 220, 20, 23, 198, 69, 220, 20, 20, 220, 23, 15, 220, 21, 24, 198, 69, 220, 20, 20, 220, 21, 24, 220, 23, 15, 198, 69, 220, 20, 21, 220, 20, 22, 220, 20, 24, 198, 69, 220, 20, 22, 220, 23, 17, 220, 20, 24, 198, 69, 220, 20, 22, 220, 21, 15, 220, 23, 17, 198, 69, 220, 20, 23, 220, 23, 18, 220, 23, 15, 198, 69, 220, 20, 23, 220, 23, 15, 220, 23, 18, 198, 69, 220, 20, 24, 220, 23, 19, 220, 21, 17, 198, 69, 220, 20, 24, 220, 23, 17, 220, 23, 19, 198, 69, 220, 21, 15, 220, 21, 17, 220, 23, 20, 198, 69, 220, 21, 15, 220, 23, 20, 220, 23, 17, 198, 69, 220, 21, 17, 220, 23, 19, 220, 23, 22, 198, 69, 220, 21, 17, 220, 23, 22, 220, 23, 20, 198, 69, 220, 21, 18, 220, 22, 21, 220, 21, 20, 198, 69, 220, 21, 18, 220, 21, 23, 220, 22, 21, 198, 69, 220, 21, 20, 220, 22, 23, 220, 22, 15, 198, 69, 220, 21, 20, 220, 22, 21, 220, 22, 23, 198, 69, 220, 21, 21, 220, 21, 22, 220, 22, 17, 198, 69, 220, 21, 21, 220, 22, 17, 220, 21, 22, 198, 69, 220, 21, 21, 220, 22, 17, 220, 22, 16, 198, 69, 220, 21, 21, 220, 22, 16, 220, 22, 17, 198, 69, 220, 21, 23, 220, 22, 15, 220, 23, 16, 198, 69, 220, 21, 23, 220, 22, 24, 220, 22, 21, 198, 69, 220, 21, 23, 220, 23, 16, 220, 22, 24, 198, 69, 220, 22, 15, 220, 22, 23, 220, 23, 16, 198, 69, 220, 22, 18, 220, 22, 20, 220, 22, 23, 198, 69, 220, 22, 18, 220, 24, 17, 220, 22, 20, 198, 69, 220, 22, 18, 220, 22, 23, 220, 22, 21, 198, 69, 220, 22, 18, 220, 22, 21, 220, 24, 16, 198, 69, 220, 22, 18, 220, 24, 16, 220, 24, 17, 198, 69, 220, 22, 20, 220, 24, 17, 220, 22, 23, 198, 69, 220, 22, 21, 220, 22, 24, 220, 24, 16, 198, 69, 220, 22, 23, 220, 24, 17, 220, 23, 16, 198, 69, 220, 22, 24, 220, 23, 16, 220, 23, 19, 198, 69, 220, 22, 24, 220, 23, 19, 220, 23, 17, 198, 69, 220, 22, 24, 220, 23, 17, 220, 24, 16, 198, 69, 220, 23, 16, 220, 24, 19, 220, 23, 19, 198, 69, 220, 23, 16, 220, 24, 17, 220, 24, 19, 198, 69, 220, 23, 17, 220, 23, 20, 220, 24, 18, 198, 69, 220, 23, 17, 220, 24, 18, 220, 24, 16, 198, 69, 220, 23, 19, 220, 24, 19, 220, 23, 22, 198, 69, 220, 23, 20, 220, 23, 22, 220, 24, 15, 198, 69, 220, 23, 20, 220, 24, 15, 220, 23, 23, 198, 69, 220, 23, 20, 220, 23, 23, 220, 24, 18, 198, 69, 220, 23, 22, 220, 24, 19, 220, 24, 15, 198, 69, 220, 24, 16, 220, 24, 18, 220, 24, 17, 198, 69, 220, 24, 17, 220, 24, 18, 220, 24, 19, 198, 151665, 151645]\n",
      "inputs:\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Craft a 3D structure of a metal gate/cage with a blue logo on the door.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "v 30 11 7\n",
      "v 32 11 7\n",
      "v 30 27 7\n",
      "v 32 27 7\n",
      "v 30 38 7\n",
      "v 32 38 7\n",
      "v 30 51 7\n",
      "v 32 51 7\n",
      "v 30 11 8\n",
      "v 31 11 8\n",
      "v 32 11 8\n",
      "v 30 27 8\n",
      "v 31 27 8\n",
      "v 32 27 8\n",
      "v 30 38 8\n",
      "v 32 38 8\n",
      "v 30 39 8\n",
      "v 31 39 8\n",
      "v 32 39 8\n",
      "v 30 51 8\n",
      "v 31 51 8\n",
      "v 32 51 8\n",
      "v 31 16 12\n",
      "v 31 17 12\n",
      "v 32 16 14\n",
      "v 32 17 14\n",
      "v 31 24 15\n",
      "v 31 25 15\n",
      "v 32 24 17\n",
      "v 32 25 17\n",
      "v 31 13 22\n",
      "v 31 14 22\n",
      "v 31 13 24\n",
      "v 31 14 24\n",
      "v 30 27 29\n",
      "v 32 27 29\n",
      "v 30 38 29\n",
      "v 32 38 29\n",
      "v 30 27 30\n",
      "v 31 27 30\n",
      "v 32 27 30\n",
      "v 30 38 30\n",
      "v 31 38 30\n",
      "v 32 38 30\n",
      "v 30 39 30\n",
      "v 31 39 30\n",
      "v 32 39 30\n",
      "v 30 25 40\n",
      "v 31 25 40\n",
      "v 32 25 40\n",
      "v 30 41 40\n",
      "v 31 41 40\n",
      "v 32 41 40\n",
      "v 30 41 41\n",
      "v 31 41 41\n",
      "v 32 41 41\n",
      "v 30 49 44\n",
      "v 31 49 44\n",
      "v 32 49 44\n",
      "v 30 50 44\n",
      "v 31 50 44\n",
      "v 32 50 44\n",
      "v 30 12 47\n",
      "v 31 12 47\n",
      "v 32 12 47\n",
      "v 31 43 47\n",
      "v 31 44 47\n",
      "v 30 13 48\n",
      "v 31 13 48\n",
      "v 32 13 48\n",
      "v 32 43 49\n",
      "v 32 44 49\n",
      "v 30 11 54\n",
      "v 31 11 54\n",
      "v 32 11 54\n",
      "v 30 12 54\n",
      "v 31 12 54\n",
      "v 32 12 54\n",
      "v 30 13 54\n",
      "v 31 13 54\n",
      "v 32 13 54\n",
      "v 30 49 54\n",
      "v 31 49 54\n",
      "v 32 49 54\n",
      "v 30 50 54\n",
      "v 31 50 54\n",
      "v 32 50 54\n",
      "v 30 51 54\n",
      "v 31 51 54\n",
      "v 32 51 54\n",
      "v 30 11 55\n",
      "v 32 11 55\n",
      "v 30 51 55\n",
      "v 32 51 55\n",
      "f 1 2 4\n",
      "f 1 11 2\n",
      "f 1 4 3\n",
      "f 1 3 12\n",
      "f 1 9 11\n",
      "f 1 12 9\n",
      "f 2 14 4\n",
      "f 2 11 14\n",
      "f 3 4 6\n",
      "f 3 6 5\n",
      "f 3 5 12\n",
      "f 4 16 6\n",
      "f 4 14 16\n",
      "f 5 6 8\n",
      "f 5 8 7\n",
      "f 5 7 17\n",
      "f 5 15 12\n",
      "f 5 17 15\n",
      "f 6 19 8\n",
      "f 6 16 19\n",
      "f 7 8 94\n",
      "f 7 20 17\n",
      "f 7 93 20\n",
      "f 7 94 93\n",
      "f 8 19 22\n",
      "f 8 22 90\n",
      "f 8 90 94\n",
      "f 9 12 11\n",
      "f 9 73 11\n",
      "f 9 11 75\n",
      "f 9 75 73\n",
      "f 10 40 13\n",
      "f 10 13 40\n",
      "f 10 49 40\n",
      "f 10 40 49\n",
      "f 10 64 49\n",
      "f 10 49 64\n",
      "f 10 77 64\n",
      "f 10 64 77\n",
      "f 10 74 77\n",
      "f 10 77 74\n",
      "f 11 12 14\n",
      "f 11 73 75\n",
      "f 12 15 14\n",
      "f 12 14 36\n",
      "f 12 39 14\n",
      "f 12 15 37\n",
      "f 12 36 35\n",
      "f 12 37 35\n",
      "f 12 35 39\n",
      "f 14 15 16\n",
      "f 14 38 16\n",
      "f 14 36 38\n",
      "f 14 41 36\n",
      "f 14 39 41\n",
      "f 15 37 16\n",
      "f 15 17 37\n",
      "f 16 38 19\n",
      "f 16 37 38\n",
      "f 17 20 19\n",
      "f 17 19 47\n",
      "f 17 45 37\n",
      "f 17 47 45\n",
      "f 18 46 21\n",
      "f 18 21 46\n",
      "f 19 20 22\n",
      "f 19 38 47\n",
      "f 20 88 22\n",
      "f 20 93 88\n",
      "f 21 46 52\n",
      "f 21 52 46\n",
      "f 21 52 61\n",
      "f 21 61 52\n",
      "f 21 61 86\n",
      "f 21 86 61\n",
      "f 21 86 89\n",
      "f 21 89 86\n",
      "f 22 88 90\n",
      "f 23 24 26\n",
      "f 23 26 24\n",
      "f 23 26 25\n",
      "f 23 25 26\n",
      "f 27 28 30\n",
      "f 27 30 28\n",
      "f 27 30 29\n",
      "f 27 29 30\n",
      "f 31 32 34\n",
      "f 31 34 32\n",
      "f 31 34 33\n",
      "f 31 33 34\n",
      "f 35 36 38\n",
      "f 35 38 37\n",
      "f 35 37 42\n",
      "f 35 42 39\n",
      "f 36 44 38\n",
      "f 36 41 44\n",
      "f 37 45 42\n",
      "f 38 44 47\n",
      "f 39 42 41\n",
      "f 39 48 41\n",
      "f 39 41 50\n",
      "f 39 50 48\n",
      "f 40 43 49\n",
      "f 40 49 43\n",
      "f 41 42 44\n",
      "f 41 48 50\n",
      "f 42 54 44\n",
      "f 42 45 54\n",
      "f 43 55 49\n",
      "f 43 49 55\n",
      "f 44 56 47\n",
      "f 44 54 56\n",
      "f 45 47 53\n",
      "f 45 53 51\n",
      "f 45 51 54\n",
      "f 47 56 53\n",
      "f 48 63 50\n",
      "f 48 50 70\n",
      "f 48 68 63\n",
      "f 48 70 68\n",
      "f 49 55 69\n",
      "f 49 69 55\n",
      "f 50 63 65\n",
      "f 50 65 70\n",
      "f 51 53 62\n",
      "f 51 57 54\n",
      "f 51 60 57\n",
      "f 51 62 60\n",
      "f 53 56 59\n",
      "f 53 59 62\n",
      "f 54 57 56\n",
      "f 55 58 80\n",
      "f 55 80 58\n",
      "f 55 80 69\n",
      "f 55 69 80\n",
      "f 56 57 59\n",
      "f 57 82 59\n",
      "f 57 60 82\n",
      "f 58 83 80\n",
      "f 58 80 83\n",
      "f 59 84 62\n",
      "f 59 82 84\n",
      "f 60 62 85\n",
      "f 60 85 82\n",
      "f 62 84 87\n",
      "f 62 87 85\n",
      "f 63 76 65\n",
      "f 63 68 76\n",
      "f 65 78 70\n",
      "f 65 76 78\n",
      "f 66 67 72\n",
      "f 66 72 67\n",
      "f 66 72 71\n",
      "f 66 71 72\n",
      "f 68 70 81\n",
      "f 68 79 76\n",
      "f 68 81 79\n",
      "f 70 78 81\n",
      "f 73 75 78\n",
      "f 73 92 75\n",
      "f 73 78 76\n",
      "f 73 76 91\n",
      "f 73 91 92\n",
      "f 75 92 78\n",
      "f 76 79 91\n",
      "f 78 92 81\n",
      "f 79 81 84\n",
      "f 79 84 82\n",
      "f 79 82 91\n",
      "f 81 94 84\n",
      "f 81 92 94\n",
      "f 82 85 93\n",
      "f 82 93 91\n",
      "f 84 94 87\n",
      "f 85 87 90\n",
      "f 85 90 88\n",
      "f 85 88 93\n",
      "f 87 94 90\n",
      "f 91 93 92\n",
      "f 92 93 94\n",
      "<|eot_id|><|im_end|>\n",
      "label_ids:\n",
      "[151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 85, 220, 18, 15, 220, 16, 16, 220, 22, 198, 85, 220, 18, 17, 220, 16, 16, 220, 22, 198, 85, 220, 18, 15, 220, 17, 22, 220, 22, 198, 85, 220, 18, 17, 220, 17, 22, 220, 22, 198, 85, 220, 18, 15, 220, 18, 23, 220, 22, 198, 85, 220, 18, 17, 220, 18, 23, 220, 22, 198, 85, 220, 18, 15, 220, 20, 16, 220, 22, 198, 85, 220, 18, 17, 220, 20, 16, 220, 22, 198, 85, 220, 18, 15, 220, 16, 16, 220, 23, 198, 85, 220, 18, 16, 220, 16, 16, 220, 23, 198, 85, 220, 18, 17, 220, 16, 16, 220, 23, 198, 85, 220, 18, 15, 220, 17, 22, 220, 23, 198, 85, 220, 18, 16, 220, 17, 22, 220, 23, 198, 85, 220, 18, 17, 220, 17, 22, 220, 23, 198, 85, 220, 18, 15, 220, 18, 23, 220, 23, 198, 85, 220, 18, 17, 220, 18, 23, 220, 23, 198, 85, 220, 18, 15, 220, 18, 24, 220, 23, 198, 85, 220, 18, 16, 220, 18, 24, 220, 23, 198, 85, 220, 18, 17, 220, 18, 24, 220, 23, 198, 85, 220, 18, 15, 220, 20, 16, 220, 23, 198, 85, 220, 18, 16, 220, 20, 16, 220, 23, 198, 85, 220, 18, 17, 220, 20, 16, 220, 23, 198, 85, 220, 18, 16, 220, 16, 21, 220, 16, 17, 198, 85, 220, 18, 16, 220, 16, 22, 220, 16, 17, 198, 85, 220, 18, 17, 220, 16, 21, 220, 16, 19, 198, 85, 220, 18, 17, 220, 16, 22, 220, 16, 19, 198, 85, 220, 18, 16, 220, 17, 19, 220, 16, 20, 198, 85, 220, 18, 16, 220, 17, 20, 220, 16, 20, 198, 85, 220, 18, 17, 220, 17, 19, 220, 16, 22, 198, 85, 220, 18, 17, 220, 17, 20, 220, 16, 22, 198, 85, 220, 18, 16, 220, 16, 18, 220, 17, 17, 198, 85, 220, 18, 16, 220, 16, 19, 220, 17, 17, 198, 85, 220, 18, 16, 220, 16, 18, 220, 17, 19, 198, 85, 220, 18, 16, 220, 16, 19, 220, 17, 19, 198, 85, 220, 18, 15, 220, 17, 22, 220, 17, 24, 198, 85, 220, 18, 17, 220, 17, 22, 220, 17, 24, 198, 85, 220, 18, 15, 220, 18, 23, 220, 17, 24, 198, 85, 220, 18, 17, 220, 18, 23, 220, 17, 24, 198, 85, 220, 18, 15, 220, 17, 22, 220, 18, 15, 198, 85, 220, 18, 16, 220, 17, 22, 220, 18, 15, 198, 85, 220, 18, 17, 220, 17, 22, 220, 18, 15, 198, 85, 220, 18, 15, 220, 18, 23, 220, 18, 15, 198, 85, 220, 18, 16, 220, 18, 23, 220, 18, 15, 198, 85, 220, 18, 17, 220, 18, 23, 220, 18, 15, 198, 85, 220, 18, 15, 220, 18, 24, 220, 18, 15, 198, 85, 220, 18, 16, 220, 18, 24, 220, 18, 15, 198, 85, 220, 18, 17, 220, 18, 24, 220, 18, 15, 198, 85, 220, 18, 15, 220, 17, 20, 220, 19, 15, 198, 85, 220, 18, 16, 220, 17, 20, 220, 19, 15, 198, 85, 220, 18, 17, 220, 17, 20, 220, 19, 15, 198, 85, 220, 18, 15, 220, 19, 16, 220, 19, 15, 198, 85, 220, 18, 16, 220, 19, 16, 220, 19, 15, 198, 85, 220, 18, 17, 220, 19, 16, 220, 19, 15, 198, 85, 220, 18, 15, 220, 19, 16, 220, 19, 16, 198, 85, 220, 18, 16, 220, 19, 16, 220, 19, 16, 198, 85, 220, 18, 17, 220, 19, 16, 220, 19, 16, 198, 85, 220, 18, 15, 220, 19, 24, 220, 19, 19, 198, 85, 220, 18, 16, 220, 19, 24, 220, 19, 19, 198, 85, 220, 18, 17, 220, 19, 24, 220, 19, 19, 198, 85, 220, 18, 15, 220, 20, 15, 220, 19, 19, 198, 85, 220, 18, 16, 220, 20, 15, 220, 19, 19, 198, 85, 220, 18, 17, 220, 20, 15, 220, 19, 19, 198, 85, 220, 18, 15, 220, 16, 17, 220, 19, 22, 198, 85, 220, 18, 16, 220, 16, 17, 220, 19, 22, 198, 85, 220, 18, 17, 220, 16, 17, 220, 19, 22, 198, 85, 220, 18, 16, 220, 19, 18, 220, 19, 22, 198, 85, 220, 18, 16, 220, 19, 19, 220, 19, 22, 198, 85, 220, 18, 15, 220, 16, 18, 220, 19, 23, 198, 85, 220, 18, 16, 220, 16, 18, 220, 19, 23, 198, 85, 220, 18, 17, 220, 16, 18, 220, 19, 23, 198, 85, 220, 18, 17, 220, 19, 18, 220, 19, 24, 198, 85, 220, 18, 17, 220, 19, 19, 220, 19, 24, 198, 85, 220, 18, 15, 220, 16, 16, 220, 20, 19, 198, 85, 220, 18, 16, 220, 16, 16, 220, 20, 19, 198, 85, 220, 18, 17, 220, 16, 16, 220, 20, 19, 198, 85, 220, 18, 15, 220, 16, 17, 220, 20, 19, 198, 85, 220, 18, 16, 220, 16, 17, 220, 20, 19, 198, 85, 220, 18, 17, 220, 16, 17, 220, 20, 19, 198, 85, 220, 18, 15, 220, 16, 18, 220, 20, 19, 198, 85, 220, 18, 16, 220, 16, 18, 220, 20, 19, 198, 85, 220, 18, 17, 220, 16, 18, 220, 20, 19, 198, 85, 220, 18, 15, 220, 19, 24, 220, 20, 19, 198, 85, 220, 18, 16, 220, 19, 24, 220, 20, 19, 198, 85, 220, 18, 17, 220, 19, 24, 220, 20, 19, 198, 85, 220, 18, 15, 220, 20, 15, 220, 20, 19, 198, 85, 220, 18, 16, 220, 20, 15, 220, 20, 19, 198, 85, 220, 18, 17, 220, 20, 15, 220, 20, 19, 198, 85, 220, 18, 15, 220, 20, 16, 220, 20, 19, 198, 85, 220, 18, 16, 220, 20, 16, 220, 20, 19, 198, 85, 220, 18, 17, 220, 20, 16, 220, 20, 19, 198, 85, 220, 18, 15, 220, 16, 16, 220, 20, 20, 198, 85, 220, 18, 17, 220, 16, 16, 220, 20, 20, 198, 85, 220, 18, 15, 220, 20, 16, 220, 20, 20, 198, 85, 220, 18, 17, 220, 20, 16, 220, 20, 20, 198, 69, 220, 16, 220, 17, 220, 19, 198, 69, 220, 16, 220, 16, 16, 220, 17, 198, 69, 220, 16, 220, 19, 220, 18, 198, 69, 220, 16, 220, 18, 220, 16, 17, 198, 69, 220, 16, 220, 24, 220, 16, 16, 198, 69, 220, 16, 220, 16, 17, 220, 24, 198, 69, 220, 17, 220, 16, 19, 220, 19, 198, 69, 220, 17, 220, 16, 16, 220, 16, 19, 198, 69, 220, 18, 220, 19, 220, 21, 198, 69, 220, 18, 220, 21, 220, 20, 198, 69, 220, 18, 220, 20, 220, 16, 17, 198, 69, 220, 19, 220, 16, 21, 220, 21, 198, 69, 220, 19, 220, 16, 19, 220, 16, 21, 198, 69, 220, 20, 220, 21, 220, 23, 198, 69, 220, 20, 220, 23, 220, 22, 198, 69, 220, 20, 220, 22, 220, 16, 22, 198, 69, 220, 20, 220, 16, 20, 220, 16, 17, 198, 69, 220, 20, 220, 16, 22, 220, 16, 20, 198, 69, 220, 21, 220, 16, 24, 220, 23, 198, 69, 220, 21, 220, 16, 21, 220, 16, 24, 198, 69, 220, 22, 220, 23, 220, 24, 19, 198, 69, 220, 22, 220, 17, 15, 220, 16, 22, 198, 69, 220, 22, 220, 24, 18, 220, 17, 15, 198, 69, 220, 22, 220, 24, 19, 220, 24, 18, 198, 69, 220, 23, 220, 16, 24, 220, 17, 17, 198, 69, 220, 23, 220, 17, 17, 220, 24, 15, 198, 69, 220, 23, 220, 24, 15, 220, 24, 19, 198, 69, 220, 24, 220, 16, 17, 220, 16, 16, 198, 69, 220, 24, 220, 22, 18, 220, 16, 16, 198, 69, 220, 24, 220, 16, 16, 220, 22, 20, 198, 69, 220, 24, 220, 22, 20, 220, 22, 18, 198, 69, 220, 16, 15, 220, 19, 15, 220, 16, 18, 198, 69, 220, 16, 15, 220, 16, 18, 220, 19, 15, 198, 69, 220, 16, 15, 220, 19, 24, 220, 19, 15, 198, 69, 220, 16, 15, 220, 19, 15, 220, 19, 24, 198, 69, 220, 16, 15, 220, 21, 19, 220, 19, 24, 198, 69, 220, 16, 15, 220, 19, 24, 220, 21, 19, 198, 69, 220, 16, 15, 220, 22, 22, 220, 21, 19, 198, 69, 220, 16, 15, 220, 21, 19, 220, 22, 22, 198, 69, 220, 16, 15, 220, 22, 19, 220, 22, 22, 198, 69, 220, 16, 15, 220, 22, 22, 220, 22, 19, 198, 69, 220, 16, 16, 220, 16, 17, 220, 16, 19, 198, 69, 220, 16, 16, 220, 22, 18, 220, 22, 20, 198, 69, 220, 16, 17, 220, 16, 20, 220, 16, 19, 198, 69, 220, 16, 17, 220, 16, 19, 220, 18, 21, 198, 69, 220, 16, 17, 220, 18, 24, 220, 16, 19, 198, 69, 220, 16, 17, 220, 16, 20, 220, 18, 22, 198, 69, 220, 16, 17, 220, 18, 21, 220, 18, 20, 198, 69, 220, 16, 17, 220, 18, 22, 220, 18, 20, 198, 69, 220, 16, 17, 220, 18, 20, 220, 18, 24, 198, 69, 220, 16, 19, 220, 16, 20, 220, 16, 21, 198, 69, 220, 16, 19, 220, 18, 23, 220, 16, 21, 198, 69, 220, 16, 19, 220, 18, 21, 220, 18, 23, 198, 69, 220, 16, 19, 220, 19, 16, 220, 18, 21, 198, 69, 220, 16, 19, 220, 18, 24, 220, 19, 16, 198, 69, 220, 16, 20, 220, 18, 22, 220, 16, 21, 198, 69, 220, 16, 20, 220, 16, 22, 220, 18, 22, 198, 69, 220, 16, 21, 220, 18, 23, 220, 16, 24, 198, 69, 220, 16, 21, 220, 18, 22, 220, 18, 23, 198, 69, 220, 16, 22, 220, 17, 15, 220, 16, 24, 198, 69, 220, 16, 22, 220, 16, 24, 220, 19, 22, 198, 69, 220, 16, 22, 220, 19, 20, 220, 18, 22, 198, 69, 220, 16, 22, 220, 19, 22, 220, 19, 20, 198, 69, 220, 16, 23, 220, 19, 21, 220, 17, 16, 198, 69, 220, 16, 23, 220, 17, 16, 220, 19, 21, 198, 69, 220, 16, 24, 220, 17, 15, 220, 17, 17, 198, 69, 220, 16, 24, 220, 18, 23, 220, 19, 22, 198, 69, 220, 17, 15, 220, 23, 23, 220, 17, 17, 198, 69, 220, 17, 15, 220, 24, 18, 220, 23, 23, 198, 69, 220, 17, 16, 220, 19, 21, 220, 20, 17, 198, 69, 220, 17, 16, 220, 20, 17, 220, 19, 21, 198, 69, 220, 17, 16, 220, 20, 17, 220, 21, 16, 198, 69, 220, 17, 16, 220, 21, 16, 220, 20, 17, 198, 69, 220, 17, 16, 220, 21, 16, 220, 23, 21, 198, 69, 220, 17, 16, 220, 23, 21, 220, 21, 16, 198, 69, 220, 17, 16, 220, 23, 21, 220, 23, 24, 198, 69, 220, 17, 16, 220, 23, 24, 220, 23, 21, 198, 69, 220, 17, 17, 220, 23, 23, 220, 24, 15, 198, 69, 220, 17, 18, 220, 17, 19, 220, 17, 21, 198, 69, 220, 17, 18, 220, 17, 21, 220, 17, 19, 198, 69, 220, 17, 18, 220, 17, 21, 220, 17, 20, 198, 69, 220, 17, 18, 220, 17, 20, 220, 17, 21, 198, 69, 220, 17, 22, 220, 17, 23, 220, 18, 15, 198, 69, 220, 17, 22, 220, 18, 15, 220, 17, 23, 198, 69, 220, 17, 22, 220, 18, 15, 220, 17, 24, 198, 69, 220, 17, 22, 220, 17, 24, 220, 18, 15, 198, 69, 220, 18, 16, 220, 18, 17, 220, 18, 19, 198, 69, 220, 18, 16, 220, 18, 19, 220, 18, 17, 198, 69, 220, 18, 16, 220, 18, 19, 220, 18, 18, 198, 69, 220, 18, 16, 220, 18, 18, 220, 18, 19, 198, 69, 220, 18, 20, 220, 18, 21, 220, 18, 23, 198, 69, 220, 18, 20, 220, 18, 23, 220, 18, 22, 198, 69, 220, 18, 20, 220, 18, 22, 220, 19, 17, 198, 69, 220, 18, 20, 220, 19, 17, 220, 18, 24, 198, 69, 220, 18, 21, 220, 19, 19, 220, 18, 23, 198, 69, 220, 18, 21, 220, 19, 16, 220, 19, 19, 198, 69, 220, 18, 22, 220, 19, 20, 220, 19, 17, 198, 69, 220, 18, 23, 220, 19, 19, 220, 19, 22, 198, 69, 220, 18, 24, 220, 19, 17, 220, 19, 16, 198, 69, 220, 18, 24, 220, 19, 23, 220, 19, 16, 198, 69, 220, 18, 24, 220, 19, 16, 220, 20, 15, 198, 69, 220, 18, 24, 220, 20, 15, 220, 19, 23, 198, 69, 220, 19, 15, 220, 19, 18, 220, 19, 24, 198, 69, 220, 19, 15, 220, 19, 24, 220, 19, 18, 198, 69, 220, 19, 16, 220, 19, 17, 220, 19, 19, 198, 69, 220, 19, 16, 220, 19, 23, 220, 20, 15, 198, 69, 220, 19, 17, 220, 20, 19, 220, 19, 19, 198, 69, 220, 19, 17, 220, 19, 20, 220, 20, 19, 198, 69, 220, 19, 18, 220, 20, 20, 220, 19, 24, 198, 69, 220, 19, 18, 220, 19, 24, 220, 20, 20, 198, 69, 220, 19, 19, 220, 20, 21, 220, 19, 22, 198, 69, 220, 19, 19, 220, 20, 19, 220, 20, 21, 198, 69, 220, 19, 20, 220, 19, 22, 220, 20, 18, 198, 69, 220, 19, 20, 220, 20, 18, 220, 20, 16, 198, 69, 220, 19, 20, 220, 20, 16, 220, 20, 19, 198, 69, 220, 19, 22, 220, 20, 21, 220, 20, 18, 198, 69, 220, 19, 23, 220, 21, 18, 220, 20, 15, 198, 69, 220, 19, 23, 220, 20, 15, 220, 22, 15, 198, 69, 220, 19, 23, 220, 21, 23, 220, 21, 18, 198, 69, 220, 19, 23, 220, 22, 15, 220, 21, 23, 198, 69, 220, 19, 24, 220, 20, 20, 220, 21, 24, 198, 69, 220, 19, 24, 220, 21, 24, 220, 20, 20, 198, 69, 220, 20, 15, 220, 21, 18, 220, 21, 20, 198, 69, 220, 20, 15, 220, 21, 20, 220, 22, 15, 198, 69, 220, 20, 16, 220, 20, 18, 220, 21, 17, 198, 69, 220, 20, 16, 220, 20, 22, 220, 20, 19, 198, 69, 220, 20, 16, 220, 21, 15, 220, 20, 22, 198, 69, 220, 20, 16, 220, 21, 17, 220, 21, 15, 198, 69, 220, 20, 18, 220, 20, 21, 220, 20, 24, 198, 69, 220, 20, 18, 220, 20, 24, 220, 21, 17, 198, 69, 220, 20, 19, 220, 20, 22, 220, 20, 21, 198, 69, 220, 20, 20, 220, 20, 23, 220, 23, 15, 198, 69, 220, 20, 20, 220, 23, 15, 220, 20, 23, 198, 69, 220, 20, 20, 220, 23, 15, 220, 21, 24, 198, 69, 220, 20, 20, 220, 21, 24, 220, 23, 15, 198, 69, 220, 20, 21, 220, 20, 22, 220, 20, 24, 198, 69, 220, 20, 22, 220, 23, 17, 220, 20, 24, 198, 69, 220, 20, 22, 220, 21, 15, 220, 23, 17, 198, 69, 220, 20, 23, 220, 23, 18, 220, 23, 15, 198, 69, 220, 20, 23, 220, 23, 15, 220, 23, 18, 198, 69, 220, 20, 24, 220, 23, 19, 220, 21, 17, 198, 69, 220, 20, 24, 220, 23, 17, 220, 23, 19, 198, 69, 220, 21, 15, 220, 21, 17, 220, 23, 20, 198, 69, 220, 21, 15, 220, 23, 20, 220, 23, 17, 198, 69, 220, 21, 17, 220, 23, 19, 220, 23, 22, 198, 69, 220, 21, 17, 220, 23, 22, 220, 23, 20, 198, 69, 220, 21, 18, 220, 22, 21, 220, 21, 20, 198, 69, 220, 21, 18, 220, 21, 23, 220, 22, 21, 198, 69, 220, 21, 20, 220, 22, 23, 220, 22, 15, 198, 69, 220, 21, 20, 220, 22, 21, 220, 22, 23, 198, 69, 220, 21, 21, 220, 21, 22, 220, 22, 17, 198, 69, 220, 21, 21, 220, 22, 17, 220, 21, 22, 198, 69, 220, 21, 21, 220, 22, 17, 220, 22, 16, 198, 69, 220, 21, 21, 220, 22, 16, 220, 22, 17, 198, 69, 220, 21, 23, 220, 22, 15, 220, 23, 16, 198, 69, 220, 21, 23, 220, 22, 24, 220, 22, 21, 198, 69, 220, 21, 23, 220, 23, 16, 220, 22, 24, 198, 69, 220, 22, 15, 220, 22, 23, 220, 23, 16, 198, 69, 220, 22, 18, 220, 22, 20, 220, 22, 23, 198, 69, 220, 22, 18, 220, 24, 17, 220, 22, 20, 198, 69, 220, 22, 18, 220, 22, 23, 220, 22, 21, 198, 69, 220, 22, 18, 220, 22, 21, 220, 24, 16, 198, 69, 220, 22, 18, 220, 24, 16, 220, 24, 17, 198, 69, 220, 22, 20, 220, 24, 17, 220, 22, 23, 198, 69, 220, 22, 21, 220, 22, 24, 220, 24, 16, 198, 69, 220, 22, 23, 220, 24, 17, 220, 23, 16, 198, 69, 220, 22, 24, 220, 23, 16, 220, 23, 19, 198, 69, 220, 22, 24, 220, 23, 19, 220, 23, 17, 198, 69, 220, 22, 24, 220, 23, 17, 220, 24, 16, 198, 69, 220, 23, 16, 220, 24, 19, 220, 23, 19, 198, 69, 220, 23, 16, 220, 24, 17, 220, 24, 19, 198, 69, 220, 23, 17, 220, 23, 20, 220, 24, 18, 198, 69, 220, 23, 17, 220, 24, 18, 220, 24, 16, 198, 69, 220, 23, 19, 220, 24, 19, 220, 23, 22, 198, 69, 220, 23, 20, 220, 23, 22, 220, 24, 15, 198, 69, 220, 23, 20, 220, 24, 15, 220, 23, 23, 198, 69, 220, 23, 20, 220, 23, 23, 220, 24, 18, 198, 69, 220, 23, 22, 220, 24, 19, 220, 24, 15, 198, 69, 220, 24, 16, 220, 24, 18, 220, 24, 17, 198, 69, 220, 24, 17, 220, 24, 18, 220, 24, 19, 198, 151665, 151645]\n",
      "labels:\n",
      "<|im_end|>v 30 11 7\n",
      "v 32 11 7\n",
      "v 30 27 7\n",
      "v 32 27 7\n",
      "v 30 38 7\n",
      "v 32 38 7\n",
      "v 30 51 7\n",
      "v 32 51 7\n",
      "v 30 11 8\n",
      "v 31 11 8\n",
      "v 32 11 8\n",
      "v 30 27 8\n",
      "v 31 27 8\n",
      "v 32 27 8\n",
      "v 30 38 8\n",
      "v 32 38 8\n",
      "v 30 39 8\n",
      "v 31 39 8\n",
      "v 32 39 8\n",
      "v 30 51 8\n",
      "v 31 51 8\n",
      "v 32 51 8\n",
      "v 31 16 12\n",
      "v 31 17 12\n",
      "v 32 16 14\n",
      "v 32 17 14\n",
      "v 31 24 15\n",
      "v 31 25 15\n",
      "v 32 24 17\n",
      "v 32 25 17\n",
      "v 31 13 22\n",
      "v 31 14 22\n",
      "v 31 13 24\n",
      "v 31 14 24\n",
      "v 30 27 29\n",
      "v 32 27 29\n",
      "v 30 38 29\n",
      "v 32 38 29\n",
      "v 30 27 30\n",
      "v 31 27 30\n",
      "v 32 27 30\n",
      "v 30 38 30\n",
      "v 31 38 30\n",
      "v 32 38 30\n",
      "v 30 39 30\n",
      "v 31 39 30\n",
      "v 32 39 30\n",
      "v 30 25 40\n",
      "v 31 25 40\n",
      "v 32 25 40\n",
      "v 30 41 40\n",
      "v 31 41 40\n",
      "v 32 41 40\n",
      "v 30 41 41\n",
      "v 31 41 41\n",
      "v 32 41 41\n",
      "v 30 49 44\n",
      "v 31 49 44\n",
      "v 32 49 44\n",
      "v 30 50 44\n",
      "v 31 50 44\n",
      "v 32 50 44\n",
      "v 30 12 47\n",
      "v 31 12 47\n",
      "v 32 12 47\n",
      "v 31 43 47\n",
      "v 31 44 47\n",
      "v 30 13 48\n",
      "v 31 13 48\n",
      "v 32 13 48\n",
      "v 32 43 49\n",
      "v 32 44 49\n",
      "v 30 11 54\n",
      "v 31 11 54\n",
      "v 32 11 54\n",
      "v 30 12 54\n",
      "v 31 12 54\n",
      "v 32 12 54\n",
      "v 30 13 54\n",
      "v 31 13 54\n",
      "v 32 13 54\n",
      "v 30 49 54\n",
      "v 31 49 54\n",
      "v 32 49 54\n",
      "v 30 50 54\n",
      "v 31 50 54\n",
      "v 32 50 54\n",
      "v 30 51 54\n",
      "v 31 51 54\n",
      "v 32 51 54\n",
      "v 30 11 55\n",
      "v 32 11 55\n",
      "v 30 51 55\n",
      "v 32 51 55\n",
      "f 1 2 4\n",
      "f 1 11 2\n",
      "f 1 4 3\n",
      "f 1 3 12\n",
      "f 1 9 11\n",
      "f 1 12 9\n",
      "f 2 14 4\n",
      "f 2 11 14\n",
      "f 3 4 6\n",
      "f 3 6 5\n",
      "f 3 5 12\n",
      "f 4 16 6\n",
      "f 4 14 16\n",
      "f 5 6 8\n",
      "f 5 8 7\n",
      "f 5 7 17\n",
      "f 5 15 12\n",
      "f 5 17 15\n",
      "f 6 19 8\n",
      "f 6 16 19\n",
      "f 7 8 94\n",
      "f 7 20 17\n",
      "f 7 93 20\n",
      "f 7 94 93\n",
      "f 8 19 22\n",
      "f 8 22 90\n",
      "f 8 90 94\n",
      "f 9 12 11\n",
      "f 9 73 11\n",
      "f 9 11 75\n",
      "f 9 75 73\n",
      "f 10 40 13\n",
      "f 10 13 40\n",
      "f 10 49 40\n",
      "f 10 40 49\n",
      "f 10 64 49\n",
      "f 10 49 64\n",
      "f 10 77 64\n",
      "f 10 64 77\n",
      "f 10 74 77\n",
      "f 10 77 74\n",
      "f 11 12 14\n",
      "f 11 73 75\n",
      "f 12 15 14\n",
      "f 12 14 36\n",
      "f 12 39 14\n",
      "f 12 15 37\n",
      "f 12 36 35\n",
      "f 12 37 35\n",
      "f 12 35 39\n",
      "f 14 15 16\n",
      "f 14 38 16\n",
      "f 14 36 38\n",
      "f 14 41 36\n",
      "f 14 39 41\n",
      "f 15 37 16\n",
      "f 15 17 37\n",
      "f 16 38 19\n",
      "f 16 37 38\n",
      "f 17 20 19\n",
      "f 17 19 47\n",
      "f 17 45 37\n",
      "f 17 47 45\n",
      "f 18 46 21\n",
      "f 18 21 46\n",
      "f 19 20 22\n",
      "f 19 38 47\n",
      "f 20 88 22\n",
      "f 20 93 88\n",
      "f 21 46 52\n",
      "f 21 52 46\n",
      "f 21 52 61\n",
      "f 21 61 52\n",
      "f 21 61 86\n",
      "f 21 86 61\n",
      "f 21 86 89\n",
      "f 21 89 86\n",
      "f 22 88 90\n",
      "f 23 24 26\n",
      "f 23 26 24\n",
      "f 23 26 25\n",
      "f 23 25 26\n",
      "f 27 28 30\n",
      "f 27 30 28\n",
      "f 27 30 29\n",
      "f 27 29 30\n",
      "f 31 32 34\n",
      "f 31 34 32\n",
      "f 31 34 33\n",
      "f 31 33 34\n",
      "f 35 36 38\n",
      "f 35 38 37\n",
      "f 35 37 42\n",
      "f 35 42 39\n",
      "f 36 44 38\n",
      "f 36 41 44\n",
      "f 37 45 42\n",
      "f 38 44 47\n",
      "f 39 42 41\n",
      "f 39 48 41\n",
      "f 39 41 50\n",
      "f 39 50 48\n",
      "f 40 43 49\n",
      "f 40 49 43\n",
      "f 41 42 44\n",
      "f 41 48 50\n",
      "f 42 54 44\n",
      "f 42 45 54\n",
      "f 43 55 49\n",
      "f 43 49 55\n",
      "f 44 56 47\n",
      "f 44 54 56\n",
      "f 45 47 53\n",
      "f 45 53 51\n",
      "f 45 51 54\n",
      "f 47 56 53\n",
      "f 48 63 50\n",
      "f 48 50 70\n",
      "f 48 68 63\n",
      "f 48 70 68\n",
      "f 49 55 69\n",
      "f 49 69 55\n",
      "f 50 63 65\n",
      "f 50 65 70\n",
      "f 51 53 62\n",
      "f 51 57 54\n",
      "f 51 60 57\n",
      "f 51 62 60\n",
      "f 53 56 59\n",
      "f 53 59 62\n",
      "f 54 57 56\n",
      "f 55 58 80\n",
      "f 55 80 58\n",
      "f 55 80 69\n",
      "f 55 69 80\n",
      "f 56 57 59\n",
      "f 57 82 59\n",
      "f 57 60 82\n",
      "f 58 83 80\n",
      "f 58 80 83\n",
      "f 59 84 62\n",
      "f 59 82 84\n",
      "f 60 62 85\n",
      "f 60 85 82\n",
      "f 62 84 87\n",
      "f 62 87 85\n",
      "f 63 76 65\n",
      "f 63 68 76\n",
      "f 65 78 70\n",
      "f 65 76 78\n",
      "f 66 67 72\n",
      "f 66 72 67\n",
      "f 66 72 71\n",
      "f 66 71 72\n",
      "f 68 70 81\n",
      "f 68 79 76\n",
      "f 68 81 79\n",
      "f 70 78 81\n",
      "f 73 75 78\n",
      "f 73 92 75\n",
      "f 73 78 76\n",
      "f 73 76 91\n",
      "f 73 91 92\n",
      "f 75 92 78\n",
      "f 76 79 91\n",
      "f 78 92 81\n",
      "f 79 81 84\n",
      "f 79 84 82\n",
      "f 79 82 91\n",
      "f 81 94 84\n",
      "f 81 92 94\n",
      "f 82 85 93\n",
      "f 82 93 91\n",
      "f 84 94 87\n",
      "f 85 87 90\n",
      "f 85 90 88\n",
      "f 85 88 93\n",
      "f 87 94 90\n",
      "f 91 93 92\n",
      "f 92 93 94\n",
      "<|eot_id|><|im_end|>\n",
      "[INFO|configuration_utils.py:694] 2025-04-30 01:00:22,922 >> loading configuration file /home/featurize/work/LLaMA-Factory-4-2/saves/Qwen2.5-1.5B-Instruct/full/221_v3/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-04-30 01:00:22,922 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/home/featurize/work/LLaMA-Factory-4-2/saves/Qwen2.5-1.5B-Instruct/full/221_v3\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3901] 2025-04-30 01:00:22,956 >> loading weights file /home/featurize/work/LLaMA-Factory-4-2/saves/Qwen2.5-1.5B-Instruct/full/221_v3/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1582] 2025-04-30 01:00:22,957 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1140] 2025-04-30 01:00:22,959 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.29it/s]\n",
      "[INFO|modeling_utils.py:4888] 2025-04-30 01:00:24,563 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4896] 2025-04-30 01:00:24,563 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /home/featurize/work/LLaMA-Factory-4-2/saves/Qwen2.5-1.5B-Instruct/full/221_v3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1093] 2025-04-30 01:00:24,597 >> loading configuration file /home/featurize/work/LLaMA-Factory-4-2/saves/Qwen2.5-1.5B-Instruct/full/221_v3/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-04-30 01:00:24,598 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO|2025-04-30 01:00:24] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-04-30 01:00:24] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-04-30 01:00:24] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-04-30 01:00:24] llamafactory.model.adapter:157 >> Fine-tuning method: Full\n",
      "[INFO|2025-04-30 01:00:24] llamafactory.model.loader:157 >> trainable params: 1,543,714,304 || all params: 1,543,714,304 || trainable%: 100.0000\n",
      "[INFO|trainer.py:741] 2025-04-30 01:00:25,065 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2369] 2025-04-30 01:00:25,386 >> ***** Running training *****\n",
      "[INFO|trainer.py:2370] 2025-04-30 01:00:25,387 >>   Num examples = 16,881\n",
      "[INFO|trainer.py:2371] 2025-04-30 01:00:25,387 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2372] 2025-04-30 01:00:25,387 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2375] 2025-04-30 01:00:25,387 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2376] 2025-04-30 01:00:25,387 >>   Gradient Accumulation steps = 32\n",
      "[INFO|trainer.py:2377] 2025-04-30 01:00:25,387 >>   Total optimization steps = 527\n",
      "[INFO|trainer.py:2378] 2025-04-30 01:00:25,388 >>   Number of trainable parameters = 1,543,714,304\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.5.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/home/featurize/work/LLaMA-Factory-4-2/swanlog/run-20250430_010026-a3b1799d\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 👋 Hi \u001b[1m\u001b[39mlixidong\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33mfull\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@lixidong/Qwen2.5-1.5B-Instruct\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@lixidong/Qwen2.5-1.5B-Instruct/runs/z532eo7jyuafko13kfpqp\u001b[0m\u001b[0m\n",
      "{'loss': 0.1293, 'grad_norm': 0.32604673504829407, 'learning_rate': 1.8518518518518519e-06, 'epoch': 0.01}\n",
      "{'loss': 0.1239, 'grad_norm': 0.3816700875759125, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.02}\n",
      "{'loss': 0.138, 'grad_norm': 0.4236868619918823, 'learning_rate': 5.555555555555557e-06, 'epoch': 0.03}\n",
      "{'loss': 0.1364, 'grad_norm': 0.4147684872150421, 'learning_rate': 7.4074074074074075e-06, 'epoch': 0.04}\n",
      "{'loss': 0.1346, 'grad_norm': 0.37655794620513916, 'learning_rate': 9.25925925925926e-06, 'epoch': 0.05}\n",
      "{'loss': 0.1307, 'grad_norm': 0.40510544180870056, 'learning_rate': 9.999111761904046e-06, 'epoch': 0.06}\n",
      "{'loss': 0.1423, 'grad_norm': 0.3974587917327881, 'learning_rate': 9.99368478303009e-06, 'epoch': 0.07}\n",
      "{'loss': 0.1444, 'grad_norm': 0.42158830165863037, 'learning_rate': 9.98332964017015e-06, 'epoch': 0.08}\n",
      "{'loss': 0.1429, 'grad_norm': 0.36706623435020447, 'learning_rate': 9.968056552600043e-06, 'epoch': 0.09}\n",
      "{'loss': 0.1203, 'grad_norm': 0.4901354908943176, 'learning_rate': 9.947880593013256e-06, 'epoch': 0.09}\n",
      "{'loss': 0.1344, 'grad_norm': 0.4952380657196045, 'learning_rate': 9.922821672646028e-06, 'epoch': 0.1}\n",
      "{'loss': 0.1343, 'grad_norm': 0.5073279142379761, 'learning_rate': 9.89290452162736e-06, 'epoch': 0.11}\n",
      "{'loss': 0.1368, 'grad_norm': 0.3845756947994232, 'learning_rate': 9.85815866457337e-06, 'epoch': 0.12}\n",
      "{'loss': 0.1551, 'grad_norm': 0.4228627681732178, 'learning_rate': 9.81861839145005e-06, 'epoch': 0.13}\n",
      "{'loss': 0.1331, 'grad_norm': 0.4956315755844116, 'learning_rate': 9.774322723733216e-06, 'epoch': 0.14}\n",
      "{'loss': 0.1366, 'grad_norm': 0.48863717913627625, 'learning_rate': 9.725315375899025e-06, 'epoch': 0.15}\n",
      "{'loss': 0.1423, 'grad_norm': 0.4956026077270508, 'learning_rate': 9.671644712283061e-06, 'epoch': 0.16}\n",
      "{'loss': 0.1349, 'grad_norm': 0.40997380018234253, 'learning_rate': 9.613363699350575e-06, 'epoch': 0.17}\n",
      "{'loss': 0.1486, 'grad_norm': 0.38651081919670105, 'learning_rate': 9.550529853424979e-06, 'epoch': 0.18}\n",
      "{'loss': 0.1292, 'grad_norm': 0.38065552711486816, 'learning_rate': 9.48320518392618e-06, 'epoch': 0.19}\n",
      "{'loss': 0.136, 'grad_norm': 0.43590131402015686, 'learning_rate': 9.411456132174768e-06, 'epoch': 0.2}\n",
      "{'loss': 0.136, 'grad_norm': 0.3713003993034363, 'learning_rate': 9.33535350582245e-06, 'epoch': 0.21}\n",
      "{'loss': 0.1383, 'grad_norm': 0.3326137363910675, 'learning_rate': 9.25497240897346e-06, 'epoch': 0.22}\n",
      "{'loss': 0.1412, 'grad_norm': 0.4213034212589264, 'learning_rate': 9.170392168065858e-06, 'epoch': 0.23}\n",
      "{'loss': 0.1413, 'grad_norm': 0.43367332220077515, 'learning_rate': 9.08169625358592e-06, 'epoch': 0.24}\n",
      "{'loss': 0.1354, 'grad_norm': 0.3868035078048706, 'learning_rate': 8.988972197692857e-06, 'epoch': 0.25}\n",
      "{'loss': 0.13, 'grad_norm': 0.5222122669219971, 'learning_rate': 8.892311507835118e-06, 'epoch': 0.26}\n",
      "{'loss': 0.1417, 'grad_norm': 0.3731640875339508, 'learning_rate': 8.791809576443611e-06, 'epoch': 0.27}\n",
      "{'loss': 0.1219, 'grad_norm': 0.4411626160144806, 'learning_rate': 8.68756558679087e-06, 'epoch': 0.27}\n",
      "{'loss': 0.1419, 'grad_norm': 0.3566397726535797, 'learning_rate': 8.579682415109156e-06, 'epoch': 0.28}\n",
      "{'loss': 0.1427, 'grad_norm': 0.38834959268569946, 'learning_rate': 8.468266529064025e-06, 'epoch': 0.29}\n",
      "{'loss': 0.1407, 'grad_norm': 0.36476749181747437, 'learning_rate': 8.353427882683601e-06, 'epoch': 0.3}\n",
      "{'loss': 0.1282, 'grad_norm': 0.3793874979019165, 'learning_rate': 8.235279807847223e-06, 'epoch': 0.31}\n",
      "{'loss': 0.1197, 'grad_norm': 0.32136714458465576, 'learning_rate': 8.113938902440563e-06, 'epoch': 0.32}\n",
      "{'loss': 0.1274, 'grad_norm': 0.33313289284706116, 'learning_rate': 7.989524915287595e-06, 'epoch': 0.33}\n",
      "{'loss': 0.1301, 'grad_norm': 0.34908103942871094, 'learning_rate': 7.862160627972956e-06, 'epoch': 0.34}\n",
      "{'loss': 0.1438, 'grad_norm': 0.3231621980667114, 'learning_rate': 7.731971733671347e-06, 'epoch': 0.35}\n",
      "{'loss': 0.1276, 'grad_norm': 0.3581680953502655, 'learning_rate': 7.5990867131035474e-06, 'epoch': 0.36}\n",
      "{'loss': 0.129, 'grad_norm': 0.4209238290786743, 'learning_rate': 7.463636707741458e-06, 'epoch': 0.37}\n",
      "{'loss': 0.1366, 'grad_norm': 0.3666099011898041, 'learning_rate': 7.325755390387293e-06, 'epoch': 0.38}\n",
      "{'loss': 0.1366, 'grad_norm': 0.3747852146625519, 'learning_rate': 7.185578833254665e-06, 'epoch': 0.39}\n",
      "{'loss': 0.1329, 'grad_norm': 0.3179681897163391, 'learning_rate': 7.043245373681746e-06, 'epoch': 0.4}\n",
      "{'loss': 0.1424, 'grad_norm': 0.31524401903152466, 'learning_rate': 6.898895477609007e-06, 'epoch': 0.41}\n",
      "{'loss': 0.1289, 'grad_norm': 0.37928447127342224, 'learning_rate': 6.752671600956295e-06, 'epoch': 0.42}\n",
      "{'loss': 0.1429, 'grad_norm': 0.34050294756889343, 'learning_rate': 6.604718049036047e-06, 'epoch': 0.43}\n",
      "{'loss': 0.1361, 'grad_norm': 0.3640602231025696, 'learning_rate': 6.455180834141359e-06, 'epoch': 0.44}\n",
      "{'loss': 0.1296, 'grad_norm': 0.3964642584323883, 'learning_rate': 6.304207531449486e-06, 'epoch': 0.45}\n",
      "{'loss': 0.1391, 'grad_norm': 0.4495700001716614, 'learning_rate': 6.151947133382954e-06, 'epoch': 0.45}\n",
      "{'loss': 0.1351, 'grad_norm': 0.43443867564201355, 'learning_rate': 5.9985499025720354e-06, 'epoch': 0.46}\n",
      "{'loss': 0.1417, 'grad_norm': 0.41361287236213684, 'learning_rate': 5.844167223563669e-06, 'epoch': 0.47}\n",
      "{'loss': 0.1372, 'grad_norm': 0.2826406955718994, 'learning_rate': 5.68895145342319e-06, 'epoch': 0.48}\n",
      "{'loss': 0.1218, 'grad_norm': 0.4516359269618988, 'learning_rate': 5.5330557713763e-06, 'epoch': 0.49}\n",
      "{'loss': 0.1338, 'grad_norm': 0.3406745493412018, 'learning_rate': 5.376634027639664e-06, 'epoch': 0.5}\n",
      "{'loss': 0.1326, 'grad_norm': 0.360660195350647, 'learning_rate': 5.219840591589325e-06, 'epoch': 0.51}\n",
      "{'loss': 0.1397, 'grad_norm': 0.3436932861804962, 'learning_rate': 5.062830199416764e-06, 'epoch': 0.52}\n",
      "{'loss': 0.135, 'grad_norm': 0.35662782192230225, 'learning_rate': 4.90575780142296e-06, 'epoch': 0.53}\n",
      "{'loss': 0.1431, 'grad_norm': 0.321939080953598, 'learning_rate': 4.748778409101153e-06, 'epoch': 0.54}\n",
      "{'loss': 0.132, 'grad_norm': 0.3181364834308624, 'learning_rate': 4.592046942159213e-06, 'epoch': 0.55}\n",
      "{'loss': 0.1484, 'grad_norm': 0.3511112332344055, 'learning_rate': 4.4357180756325915e-06, 'epoch': 0.56}\n",
      "{'loss': 0.1289, 'grad_norm': 0.37773945927619934, 'learning_rate': 4.279946087238739e-06, 'epoch': 0.57}\n",
      "{'loss': 0.1333, 'grad_norm': 0.3850390613079071, 'learning_rate': 4.124884705123619e-06, 'epoch': 0.58}\n",
      "{'loss': 0.1522, 'grad_norm': 0.3476615250110626, 'learning_rate': 3.970686956150595e-06, 'epoch': 0.59}\n",
      "{'loss': 0.131, 'grad_norm': 0.34449273347854614, 'learning_rate': 3.817505014881378e-06, 'epoch': 0.6}\n",
      "{'loss': 0.1376, 'grad_norm': 0.37684574723243713, 'learning_rate': 3.6654900533981234e-06, 'epoch': 0.61}\n",
      "{'loss': 0.142, 'grad_norm': 0.3162505626678467, 'learning_rate': 3.5147920921148267e-06, 'epoch': 0.62}\n",
      "{'loss': 0.1338, 'grad_norm': 0.3602861762046814, 'learning_rate': 3.3655598517252886e-06, 'epoch': 0.63}\n",
      "{'loss': 0.1209, 'grad_norm': 0.308415025472641, 'learning_rate': 3.217940606433747e-06, 'epoch': 0.64}\n",
      "{'loss': 0.1204, 'grad_norm': 0.2963212728500366, 'learning_rate': 3.0720800386130176e-06, 'epoch': 0.64}\n",
      "{'loss': 0.1487, 'grad_norm': 0.3303315043449402, 'learning_rate': 2.92812209503358e-06, 'epoch': 0.65}\n",
      "{'loss': 0.1362, 'grad_norm': 0.3488100469112396, 'learning_rate': 2.7862088448054936e-06, 'epoch': 0.66}\n",
      "{'loss': 0.1361, 'grad_norm': 0.3819531798362732, 'learning_rate': 2.646480339173337e-06, 'epoch': 0.67}\n",
      "{'loss': 0.1303, 'grad_norm': 0.3475766181945801, 'learning_rate': 2.509074473302546e-06, 'epoch': 0.68}\n",
      "{'loss': 0.1168, 'grad_norm': 0.35029295086860657, 'learning_rate': 2.3741268501935212e-06, 'epoch': 0.69}\n",
      "{'loss': 0.1296, 'grad_norm': 0.32170286774635315, 'learning_rate': 2.2417706468578495e-06, 'epoch': 0.7}\n",
      "{'loss': 0.1356, 'grad_norm': 0.2858441174030304, 'learning_rate': 2.112136482888663e-06, 'epoch': 0.71}\n",
      "{'loss': 0.1252, 'grad_norm': 0.32066625356674194, 'learning_rate': 1.9853522915548777e-06, 'epoch': 0.72}\n",
      "{'loss': 0.1401, 'grad_norm': 0.3723418712615967, 'learning_rate': 1.8615431935464984e-06, 'epoch': 0.73}\n",
      "{'loss': 0.1304, 'grad_norm': 0.34843525290489197, 'learning_rate': 1.7408313734956074e-06, 'epoch': 0.74}\n",
      "{'loss': 0.1271, 'grad_norm': 0.33469080924987793, 'learning_rate': 1.6233359593948777e-06, 'epoch': 0.75}\n",
      "{'loss': 0.1418, 'grad_norm': 0.3469831943511963, 'learning_rate': 1.5091729050326376e-06, 'epoch': 0.76}\n",
      "{'loss': 0.1243, 'grad_norm': 0.3505042791366577, 'learning_rate': 1.3984548755604655e-06, 'epoch': 0.77}\n",
      "{'loss': 0.1325, 'grad_norm': 0.36484310030937195, 'learning_rate': 1.2912911363063048e-06, 'epoch': 0.78}\n",
      "{'loss': 0.138, 'grad_norm': 0.30892619490623474, 'learning_rate': 1.18778744494276e-06, 'epoch': 0.79}\n",
      "{'loss': 0.129, 'grad_norm': 0.37776777148246765, 'learning_rate': 1.0880459471170597e-06, 'epoch': 0.8}\n",
      "{'loss': 0.1428, 'grad_norm': 0.369875431060791, 'learning_rate': 9.921650756456164e-07, 'epoch': 0.81}\n",
      "{'loss': 0.1214, 'grad_norm': 0.27667999267578125, 'learning_rate': 9.002394533727382e-07, 'epoch': 0.82}\n",
      "{'loss': 0.1229, 'grad_norm': 0.2878720760345459, 'learning_rate': 8.123597997892918e-07, 'epoch': 0.82}\n",
      "{'loss': 0.1278, 'grad_norm': 0.28480684757232666, 'learning_rate': 7.286128415035249e-07, 'epoch': 0.83}\n",
      "{'loss': 0.1348, 'grad_norm': 0.330055832862854, 'learning_rate': 6.490812266523716e-07, 'epoch': 0.84}\n",
      "{'loss': 0.1275, 'grad_norm': 0.3081509470939636, 'learning_rate': 5.738434433377244e-07, 'epoch': 0.85}\n",
      "{'loss': 0.1156, 'grad_norm': 0.2973465323448181, 'learning_rate': 5.029737421681446e-07, 'epoch': 0.86}\n",
      "{'loss': 0.1386, 'grad_norm': 0.33245643973350525, 'learning_rate': 4.3654206298248625e-07, 'epoch': 0.87}\n",
      "{'loss': 0.141, 'grad_norm': 0.2997063398361206, 'learning_rate': 3.7461396582771035e-07, 'epoch': 0.88}\n",
      "{'loss': 0.12, 'grad_norm': 0.3095458447933197, 'learning_rate': 3.172505662590386e-07, 'epoch': 0.89}\n",
      "{'loss': 0.1266, 'grad_norm': 0.36714231967926025, 'learning_rate': 2.6450847502627883e-07, 'epoch': 0.9}\n",
      "{'loss': 0.1304, 'grad_norm': 0.2992863357067108, 'learning_rate': 2.1643974220584729e-07, 'epoch': 0.91}\n",
      "{'loss': 0.129, 'grad_norm': 0.31637346744537354, 'learning_rate': 1.7309180583363062e-07, 'epoch': 0.92}\n",
      "{'loss': 0.1395, 'grad_norm': 0.3030172884464264, 'learning_rate': 1.3450744508936687e-07, 'epoch': 0.93}\n",
      "{'loss': 0.1292, 'grad_norm': 0.2986043393611908, 'learning_rate': 1.007247380787657e-07, 'epoch': 0.94}\n",
      "{'loss': 0.1519, 'grad_norm': 0.3098960518836975, 'learning_rate': 7.177702425500977e-08, 'epoch': 0.95}\n",
      "{'loss': 0.1333, 'grad_norm': 0.30745601654052734, 'learning_rate': 4.769287151674407e-08, 'epoch': 0.96}\n",
      "{'loss': 0.1228, 'grad_norm': 0.2971368432044983, 'learning_rate': 2.8496048015005385e-08, 'epoch': 0.97}\n",
      "{'loss': 0.1254, 'grad_norm': 0.30024540424346924, 'learning_rate': 1.4205498696930332e-08, 'epoch': 0.98}\n",
      "{'loss': 0.1281, 'grad_norm': 0.2888960838317871, 'learning_rate': 4.835326609376468e-09, 'epoch': 0.99}\n",
      "{'loss': 0.1329, 'grad_norm': 0.3792575001716614, 'learning_rate': 3.9477898091944135e-10, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████| 527/527 [3:34:11<00:00, 21.25s/it][INFO|trainer.py:3910] 2025-04-30 04:34:39,433 >> Saving model checkpoint to saves/Qwen2.5-1.5B-Instruct/full/430_v2/checkpoint-527\n",
      "[INFO|configuration_utils.py:420] 2025-04-30 04:34:39,436 >> Configuration saved in saves/Qwen2.5-1.5B-Instruct/full/430_v2/checkpoint-527/config.json\n",
      "[INFO|configuration_utils.py:909] 2025-04-30 04:34:39,438 >> Configuration saved in saves/Qwen2.5-1.5B-Instruct/full/430_v2/checkpoint-527/generation_config.json\n",
      "[INFO|modeling_utils.py:2996] 2025-04-30 04:38:41,613 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at saves/Qwen2.5-1.5B-Instruct/full/430_v2/checkpoint-527/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-04-30 04:38:41,616 >> tokenizer config file saved in saves/Qwen2.5-1.5B-Instruct/full/430_v2/checkpoint-527/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-04-30 04:38:41,617 >> Special tokens file saved in saves/Qwen2.5-1.5B-Instruct/full/430_v2/checkpoint-527/special_tokens_map.json\n",
      "[INFO|trainer.py:2643] 2025-04-30 04:43:12,014 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 13366.645, 'train_samples_per_second': 1.263, 'train_steps_per_second': 0.039, 'train_loss': 0.13401951746877275, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████| 527/527 [3:42:44<00:00, 25.36s/it]\n",
      "[INFO|trainer.py:3910] 2025-04-30 04:43:12,048 >> Saving model checkpoint to saves/Qwen2.5-1.5B-Instruct/full/430_v2\n",
      "[INFO|configuration_utils.py:420] 2025-04-30 04:43:12,052 >> Configuration saved in saves/Qwen2.5-1.5B-Instruct/full/430_v2/config.json\n",
      "[INFO|configuration_utils.py:909] 2025-04-30 04:43:12,054 >> Configuration saved in saves/Qwen2.5-1.5B-Instruct/full/430_v2/generation_config.json\n",
      "[INFO|modeling_utils.py:2996] 2025-04-30 04:47:14,730 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at saves/Qwen2.5-1.5B-Instruct/full/430_v2/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-04-30 04:47:14,735 >> tokenizer config file saved in saves/Qwen2.5-1.5B-Instruct/full/430_v2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-04-30 04:47:14,737 >> Special tokens file saved in saves/Qwen2.5-1.5B-Instruct/full/430_v2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       0.999\n",
      "  total_flos               = 251053970GF\n",
      "  train_loss               =       0.134\n",
      "  train_runtime            =  3:42:46.64\n",
      "  train_samples_per_second =       1.263\n",
      "  train_steps_per_second   =       0.039\n",
      "Figure saved at: saves/Qwen2.5-1.5B-Instruct/full/430_v2/training_loss.png\n",
      "[WARNING|2025-04-30 04:47:15] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-04-30 04:47:15] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:449] 2025-04-30 04:47:15,853 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Experiment \u001b[33mfull\u001b[0m has completed\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@lixidong/Qwen2.5-1.5B-Instruct\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@lixidong/Qwen2.5-1.5B-Instruct/runs/z532eo7jyuafko13kfpqp\u001b[0m\u001b[0m\n",
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "!PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True llamafactory-cli train ./config/Qwen2.5-1.5B-221-v1/Qwen2.5-1.5B-Instruct-full.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5d43f4-789e-41cd-a81a-9941d01e99a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-07 19:08:09.504540: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746644889.529957    4837 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746644889.537875    4837 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746644889.561136    4837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746644889.561179    4837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746644889.561190    4837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746644889.561198    4837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-07 19:08:09.567379: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[INFO|configuration_utils.py:694] 2025-05-07 19:08:17,855 >> loading configuration file saves/Qwen2.5-1.5B-Instruct/full/221_v3/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-07 19:08:17,856 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"saves/Qwen2.5-1.5B-Instruct/full/221_v3\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:17,859 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:17,859 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:17,859 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:17,859 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:17,859 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:17,860 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:17,860 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-05-07 19:08:18,400 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:694] 2025-05-07 19:08:18,402 >> loading configuration file saves/Qwen2.5-1.5B-Instruct/full/221_v3/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-07 19:08:18,403 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"saves/Qwen2.5-1.5B-Instruct/full/221_v3\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:18,404 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:18,405 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:18,405 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:18,405 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:18,405 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:18,405 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2032] 2025-05-07 19:08:18,405 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2304] 2025-05-07 19:08:18,923 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-07 19:08:18] llamafactory.data.template:157 >> Add <|eot_id|> to stop words.\n",
      "[INFO|configuration_utils.py:694] 2025-05-07 19:08:18,968 >> loading configuration file saves/Qwen2.5-1.5B-Instruct/full/221_v3/config.json\n",
      "[INFO|configuration_utils.py:768] 2025-05-07 19:08:18,970 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"saves/Qwen2.5-1.5B-Instruct/full/221_v3\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|2025-05-07 19:08:18] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3901] 2025-05-07 19:08:19,018 >> loading weights file saves/Qwen2.5-1.5B-Instruct/full/221_v3/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1582] 2025-05-07 19:08:19,038 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1140] 2025-05-07 19:08:19,040 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:47<00:00, 23.59s/it]\n",
      "[INFO|modeling_utils.py:4888] 2025-05-07 19:09:06,287 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4896] 2025-05-07 19:09:06,287 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at saves/Qwen2.5-1.5B-Instruct/full/221_v3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1093] 2025-05-07 19:09:06,322 >> loading configuration file saves/Qwen2.5-1.5B-Instruct/full/221_v3/generation_config.json\n",
      "[INFO|configuration_utils.py:1140] 2025-05-07 19:09:06,323 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "[INFO|2025-05-07 19:09:06] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-07 19:09:06] llamafactory.model.loader:157 >> all params: 1,543,714,304\n",
      "[INFO|2025-05-07 19:09:07] llamafactory.train.tuner:157 >> Convert model dtype to: torch.float32.\n",
      "[INFO|configuration_utils.py:420] 2025-05-07 19:09:07,678 >> Configuration saved in A_myModels/Qwen2.5-1.5B-Instruct_221/config.json\n",
      "[INFO|configuration_utils.py:909] 2025-05-07 19:09:07,681 >> Configuration saved in A_myModels/Qwen2.5-1.5B-Instruct_221/generation_config.json\n",
      "[INFO|modeling_utils.py:2996] 2025-05-07 19:13:16,155 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at A_myModels/Qwen2.5-1.5B-Instruct_221/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2491] 2025-05-07 19:13:16,158 >> tokenizer config file saved in A_myModels/Qwen2.5-1.5B-Instruct_221/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2500] 2025-05-07 19:13:16,160 >> Special tokens file saved in A_myModels/Qwen2.5-1.5B-Instruct_221/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli export ./config/Qwen2.5-1.5B-221-v1/Qwen2.5-1.5B-Instruct-full-export.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51554163-02e9-4103-8663-4d44651ade06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3fa14c-c5c8-4a53-a620-4ecb1b32280b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 20:24:11.142709: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-29 20:24:11.154452: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745958251.168007   13601 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745958251.172074   13601 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745958251.182963   13601 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745958251.182979   13601 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745958251.182981   13601 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745958251.182982   13601 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-29 20:24:11.187114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7a3fc513a140fca481411669ce5487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已成功合并并保存至 /home/featurize/work/LLaMA-Factory-4-2/A_myModels/llamameshchange\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# 定义路径\n",
    "base_model_path = \"/home/featurize/work/LLaMA-Mesh-main/model/models--Zhengyi--LLaMA-Mesh/snapshots/8744c58e8cbbaa0be1e9111adf2f4962e5d8ff2c\"  # 原始模型路径\n",
    "lora_adapter_path = \"/home/featurize/work/LLaMA-Factory-4-2/saves/Llama-3.1-8B-Instruct/lora/sft/429/checkpoint-600\"  # LoRA适配器路径\n",
    "save_path = \"/home/featurize/work/LLaMA-Factory-4-2/A_myModels/llamameshchange\"  # 合并后保存路径\n",
    "\n",
    "# 加载原始模型和分词器\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# 加载LoRA适配器\n",
    "model = PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "\n",
    "# 合并模型并卸载适配器\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# 保存合并后的模型和分词器\n",
    "merged_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"模型已成功合并并保存至 {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e25c2-58af-4ec8-a564-af244e1f6e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
