{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504acc6d-ac07-4d86-96e9-ab17bd6a2d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk wordcloud matplotlib\n",
    "!pip install spacy\n",
    "!pip install \"numpy<2\" --force-reinstall\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9dcb4c-1405-4dae-9962-8bd2d0a62b17",
   "metadata": {},
   "source": [
    "### 用于提取名词并保存到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6778a6b-4b5a-460b-a1e0-aa01d773ea23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# 加载英语语言模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 读取 JSON 文件\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# 提取 user 的 content\n",
    "def extract_user_content(json_data):\n",
    "    user_contents = []\n",
    "    if isinstance(json_data, list):\n",
    "        for item in json_data:\n",
    "            if \"messages\" in item:\n",
    "                for message in item[\"messages\"]:\n",
    "                    if message[\"role\"] == \"user\":\n",
    "                        user_contents.append(message[\"content\"])\n",
    "    elif isinstance(json_data, dict):\n",
    "        if \"messages\" in json_data:\n",
    "            for message in json_data[\"messages\"]:\n",
    "                if message[\"role\"] == \"user\":\n",
    "                    user_contents.append(message[\"content\"])\n",
    "    return \" \".join(user_contents)\n",
    "\n",
    "# 提取词，根据 extract_all_words 决定提取所有词还是只提取名词\n",
    "def extract_words(text, extract_all_words, custom_filter=None):\n",
    "    chunk_size = 1000000\n",
    "    all_words = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        doc = nlp(chunk)\n",
    "        words = []\n",
    "        for token in doc:\n",
    "            if extract_all_words:\n",
    "                if not token.is_punct and (custom_filter is None or token.text.lower() not in custom_filter):\n",
    "                    words.append(token.text)\n",
    "                    # print(f\"提取词: {token.text}\")\n",
    "            else:\n",
    "                if token.pos_ == 'NOUN' and (custom_filter is None or token.text.lower() not in custom_filter):\n",
    "                    words.append(token.text)\n",
    "                    # print(f\"提取名词: {token.text}\")\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n",
    "# 将词保存到文件\n",
    "def save_words_to_file(words, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for word in words:\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 读取 JSON 文件部分\n",
    "    json_file_path = './data_ok/data_sharegpt/data_sharegpt_8k.json'  # 替换为你的 JSON 文件路径\n",
    "    # json_file_path = './data_test/test1.json'  # 替换为你的 JSON 文件路径\n",
    "    json_data = read_json_file(json_file_path)\n",
    "    user_text = extract_user_content(json_data)\n",
    "    print(\"用户内容提取成功，内容长度:\", len(user_text))\n",
    "    # print(\"user_text:\", user_text)\n",
    "\n",
    "    # 判断变量，True 表示提取所有词，False 表示只提取名词\n",
    "    extract_all_words = False\n",
    "\n",
    "    # 可以添加自定义的过滤词列表\n",
    "    # custom_filter = ['obj', 'file', 'object', 'model']\n",
    "    custom_filter = []\n",
    "    words = extract_words(user_text, extract_all_words, custom_filter)\n",
    "    word_file_path = 'A_all_words8k.txt' if extract_all_words else 'A_nouns8k.txt'\n",
    "    save_words_to_file(words, word_file_path)\n",
    "    print(f\"{'词' if extract_all_words else '名词'}提取并保存成功，数量: {len(words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05559b6-1f79-49fd-9b8e-0cf5d9645062",
   "metadata": {},
   "source": [
    "### 手动删除一些词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a1315-e335-4800-861a-e91f38f130b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_word_from_file(file_path, words_to_remove):\n",
    "    try:\n",
    "        new_lines = []\n",
    "        # 打开文件并按行读取内容\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                words = line.strip().split()  # 去除行首尾空白并分割为单词列表\n",
    "                # 过滤掉在 words_to_remove 列表中的单词\n",
    "                new_words = [word for word in words if word not in words_to_remove]\n",
    "                if new_words:  # 检查处理后的单词列表是否为空\n",
    "                    new_line = ' '.join(new_words)  # 重新组合单词为字符串\n",
    "                    new_lines.append(new_line + '\\n')  # 添加换行符并保存到新行列表\n",
    "\n",
    "        # 将处理后的内容写回文件\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.writelines(new_lines)\n",
    "\n",
    "        print(f\"已成功从文件 {file_path} 中移除所有 '{words_to_remove}'\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"未找到文件 {file_path}。\")\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件时出现错误: {e}\")\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "file_path = '.txt'  \n",
    "words_to_remove = []  \n",
    "remove_word_from_file(file_path, words_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f850c-baaa-4013-8c36-1faf29b29f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 统计频率\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_word_frequency(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            words = [line.strip() for line in file.readlines()]\n",
    "        word_counts = Counter(words)\n",
    "        top_100 = word_counts.most_common(100)\n",
    "        return top_100\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：文件 {file_path} 未找到。\")\n",
    "    except Exception as e:\n",
    "        print(f\"错误：发生了未知错误 {e}。\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'A_all_words8k.txt'\n",
    "    result = count_word_frequency(file_path)\n",
    "    if result:\n",
    "        formatted_output = ','.join([f\"'{word}'\" for word, _ in result])\n",
    "        print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc80b8-b178-4d0e-9470-22fb183e9c91",
   "metadata": {},
   "source": [
    "### 用于生成词云并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31dc0a-4046-49d9-8fdc-ffd0608280f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成词云并保存到指定位置\n",
    "def generate_wordcloud_from_file(noun_file_path, wordcloud_image_path):\n",
    "    with open(noun_file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    wordcloud.to_file(wordcloud_image_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    noun_file_path = 'A_all_words8k.txt'  # 存储名词的文件路径\n",
    "    wordcloud_image_path = 'A_all_words8k.png'  # 存储词云图片的路径\n",
    "    generate_wordcloud_from_file(noun_file_path, wordcloud_image_path)\n",
    "    print(\"词云生成并保存成功\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
