{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea3f3a-0d59-4e4b-a2c1-fc57bf84d35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8c47f8-0dc7-4d81-9500-f75d0f5ec753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A package for downloading and processing Objaverse.\"\"\"\n",
    "\"\"\"Thanks Objaverse\"\"\"\n",
    "\"\"\"\n",
    "Paper:\n",
    "Deitke M, Schwenk D, Salvador J, et al. Objaverse: A universe of annotated 3d objects[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023: 13142-13153.\n",
    "\"\"\"\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import urllib.request\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_PATH = os.path.join(os.path.expanduser(\"./\"), \"myObjaverse\")\n",
    "\n",
    "__version__ = \"<REPLACE_WITH_VERSION>\"\n",
    "_VERSIONED_PATH = os.path.join(BASE_PATH, \"hf-objaverse-v1\")\n",
    "\n",
    "\n",
    "def load_annotations(uids: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Load the full metadata of all objects in the dataset.\n",
    "\n",
    "    Args:\n",
    "        uids: A list of uids with which to load metadata. If None, it loads\n",
    "        the metadata for all uids.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping the uid to the metadata.\n",
    "    \"\"\"\n",
    "    metadata_path = os.path.join(_VERSIONED_PATH, \"metadata\")\n",
    "    object_paths = _load_object_paths()\n",
    "    dir_ids = (\n",
    "        set(object_paths[uid].split(\"/\")[1] for uid in uids)\n",
    "        if uids is not None\n",
    "        else [f\"{i // 1000:03d}-{i % 1000:03d}\" for i in range(160)]\n",
    "    )\n",
    "    if len(dir_ids) > 10:\n",
    "        dir_ids = tqdm(dir_ids)\n",
    "    out = {}\n",
    "    for i_id in dir_ids:\n",
    "        json_file = f\"{i_id}.json.gz\"\n",
    "        local_path = os.path.join(metadata_path, json_file)\n",
    "        if not os.path.exists(local_path):\n",
    "            hf_url = f\"https://huggingface.co/datasets/allenai/objaverse/resolve/main/metadata/{i_id}.json.gz\"\n",
    "            # wget the file and put it in local_path\n",
    "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "            urllib.request.urlretrieve(hf_url, local_path)\n",
    "        with gzip.open(local_path, \"rb\") as f:\n",
    "            data = json.load(f)\n",
    "        if uids is not None:\n",
    "            data = {uid: data[uid] for uid in uids if uid in data}\n",
    "        out.update(data)\n",
    "        if uids is not None and len(out) == len(uids):\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "def _load_object_paths() -> Dict[str, str]:\n",
    "    \"\"\"Load the object paths from the dataset.\n",
    "\n",
    "    The object paths specify the location of where the object is located\n",
    "    in the Hugging Face repo.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping the uid to the object path.\n",
    "    \"\"\"\n",
    "    object_paths_file = \"object-paths.json.gz\"\n",
    "    local_path = os.path.join(_VERSIONED_PATH, object_paths_file)\n",
    "    if not os.path.exists(local_path):\n",
    "        hf_url = f\"https://huggingface.co/datasets/allenai/objaverse/resolve/main/{object_paths_file}\"\n",
    "        # wget the file and put it in local_path\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        urllib.request.urlretrieve(hf_url, local_path)\n",
    "    with gzip.open(local_path, \"rb\") as f:\n",
    "        object_paths = json.load(f)\n",
    "    return object_paths\n",
    "\n",
    "\n",
    "def load_uids() -> List[str]:\n",
    "    \"\"\"Load the uids from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        A list of uids.\n",
    "    \"\"\"\n",
    "    return list(_load_object_paths().keys())\n",
    "\n",
    "\n",
    "def _download_object(\n",
    "    uid: str,\n",
    "    object_path: str,\n",
    "    total_downloads: float,\n",
    "    start_file_count: int,\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"Download the object for the given uid.\n",
    "\n",
    "    Args:\n",
    "        uid: The uid of the object to load.\n",
    "        object_path: The path to the object in the Hugging Face repo.\n",
    "\n",
    "    Returns:\n",
    "        The local path of where the object was downloaded.\n",
    "    \"\"\"\n",
    "    # print(f\"downloading {uid}\")\n",
    "    local_path = os.path.join(_VERSIONED_PATH, object_path)\n",
    "    tmp_local_path = os.path.join(_VERSIONED_PATH, object_path + \".tmp\")\n",
    "    hf_url = (\n",
    "        f\"https://huggingface.co/datasets/allenai/objaverse/resolve/main/{object_path}\"\n",
    "    )\n",
    "    # wget the file and put it in local_path\n",
    "    os.makedirs(os.path.dirname(tmp_local_path), exist_ok=True)\n",
    "    urllib.request.urlretrieve(hf_url, tmp_local_path)\n",
    "\n",
    "    os.rename(tmp_local_path, local_path)\n",
    "\n",
    "    files = glob.glob(os.path.join(_VERSIONED_PATH, \"glbs\", \"*\", \"*.glb\"))\n",
    "    print(\n",
    "        \"Downloaded\",\n",
    "        len(files) - start_file_count,\n",
    "        \"/\",\n",
    "        total_downloads,\n",
    "        \"objects\",\n",
    "    )\n",
    "\n",
    "    return uid, local_path\n",
    "\n",
    "\n",
    "def load_objects(uids: List[str], download_processes: int = 1) -> Dict[str, str]:\n",
    "    \"\"\"Return the path to the object files for the given uids.\n",
    "\n",
    "    If the object is not already downloaded, it will be downloaded.\n",
    "\n",
    "    Args:\n",
    "        uids: A list of uids.\n",
    "        download_processes: The number of processes to use to download the objects.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping the object uid to the local path of where the object\n",
    "        downloaded.\n",
    "    \"\"\"\n",
    "    object_paths = _load_object_paths()\n",
    "    out = {}\n",
    "    if download_processes == 1:\n",
    "        uids_to_download = []\n",
    "        for uid in uids:\n",
    "            if uid.endswith(\".glb\"):\n",
    "                uid = uid[:-4]\n",
    "            if uid not in object_paths:\n",
    "                warnings.warn(f\"Could not find object with uid {uid}. Skipping it.\")\n",
    "                continue\n",
    "            object_path = object_paths[uid]\n",
    "            local_path = os.path.join(_VERSIONED_PATH, object_path)\n",
    "            if os.path.exists(local_path):\n",
    "                out[uid] = local_path\n",
    "                continue\n",
    "            uids_to_download.append((uid, object_path))\n",
    "        if len(uids_to_download) == 0:\n",
    "            return out\n",
    "        start_file_count = len(\n",
    "            glob.glob(os.path.join(_VERSIONED_PATH, \"glbs\", \"*\", \"*.glb\"))\n",
    "        )\n",
    "        for uid, object_path in uids_to_download:\n",
    "            uid, local_path = _download_object(\n",
    "                uid, object_path, len(uids_to_download), start_file_count\n",
    "            )\n",
    "            out[uid] = local_path\n",
    "    else:\n",
    "        args = []\n",
    "        for uid in uids:\n",
    "            if uid.endswith(\".glb\"):\n",
    "                uid = uid[:-4]\n",
    "            if uid not in object_paths:\n",
    "                warnings.warn(f\"Could not find object with uid {uid}. Skipping it.\")\n",
    "                continue\n",
    "            object_path = object_paths[uid]\n",
    "            local_path = os.path.join(_VERSIONED_PATH, object_path)\n",
    "            if not os.path.exists(local_path):\n",
    "                args.append((uid, object_paths[uid]))\n",
    "            else:\n",
    "                out[uid] = local_path\n",
    "        if len(args) == 0:\n",
    "            return out\n",
    "        print(\n",
    "            f\"starting download of {len(args)} objects with {download_processes} processes\"\n",
    "        )\n",
    "        start_file_count = len(\n",
    "            glob.glob(os.path.join(_VERSIONED_PATH, \"glbs\", \"*\", \"*.glb\"))\n",
    "        )\n",
    "        args_list = [(*arg, len(args), start_file_count) for arg in args]\n",
    "        with multiprocessing.Pool(download_processes) as pool:\n",
    "            r = pool.starmap(_download_object, args_list)\n",
    "            for uid, local_path in r:\n",
    "                out[uid] = local_path\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_lvis_annotations() -> Dict[str, List[str]]:\n",
    "    \"\"\"Load the LVIS annotations.\n",
    "\n",
    "    If the annotations are not already downloaded, they will be downloaded.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping the LVIS category to the list of uids in that category.\n",
    "    \"\"\"\n",
    "    hf_url = \"https://huggingface.co/datasets/allenai/objaverse/resolve/main/lvis-annotations.json.gz\"\n",
    "    local_path = os.path.join(_VERSIONED_PATH, \"lvis-annotations.json.gz\")\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    if not os.path.exists(local_path):\n",
    "        urllib.request.urlretrieve(hf_url, local_path)\n",
    "    with gzip.open(local_path, \"rb\") as f:\n",
    "        lvis_annotations = json.load(f)\n",
    "    return lvis_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df97137-10d6-4b69-954a-2c6b8cea5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载UIDS\n",
    "uids = load_uids()\n",
    "len(uids), type(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d28bc-b680-4b09-91c3-da012ea32cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看5个UIDs\n",
    "uids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f569c-ef77-4d4e-8ecf-9cd714324eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotations = load_annotations(uids[:5])\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6325200d-cfdd-4e11-b8fe-63ccd3d1b508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotations[uids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d9026-930b-4c1f-8e0b-0e90a49711b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载所有注释\n",
    "annotations = load_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e63f1e-027e-4b71-9d05-3cb60218f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[uids[1]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7dc038-015e-4451-b8a3-d39a6d6be5e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 筛选出小于x面的\n",
    "objs = []\n",
    "name = []\n",
    "description = []\n",
    "face = []\n",
    "for uid, annotation in annotations.items():\n",
    "    if 1 <= annotation.get(\"faceCount\") <= 700 :\n",
    "        objs.append(uid)\n",
    "        name.append(annotation.get(\"name\"))\n",
    "        description.append(annotation.get(\"description\"))\n",
    "        face.append(annotation.get(\"faceCount\"))\n",
    "objs[:3], name[:3], description[:3], face[:3]\n",
    "len(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29afd1-fe55-4098-b1c6-bd456cf54225",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767d497-6c32-434e-8a46-cba874f15bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看线程\n",
    "import multiprocessing\n",
    "processes = multiprocessing.cpu_count()\n",
    "processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c2766-1e45-4088-8c6e-a5a7d7b6107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# random.seed(42)\n",
    "\n",
    "# object_uids = random.sample(objs, 100)\n",
    "\n",
    "# object_uids\n",
    "\n",
    "object_uids = objs[:]\n",
    "len(object_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155430d5-777c-4c29-bb69-5d0fc184d5f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "objects = load_objects(\n",
    "    uids=object_uids,\n",
    "    download_processes=processes\n",
    ")\n",
    "objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692edce-c815-4c11-a1ad-d6c4f2b88a38",
   "metadata": {},
   "source": [
    "遍历统计文件夹中glb文件数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d440b6-8633-4708-b897-23702a909433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 遍历统计文件夹中glb文件数量\n",
    "def count_glb_files_in_subdirectories(root_folder):\n",
    "    total_count = 0\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        glb_count = 0\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.glb'):\n",
    "                glb_count += 1\n",
    "                total_count += 1\n",
    "        # if glb_count > 0:\n",
    "            # print(f\"子目录 {root} 中 .glb 文件的数量: {glb_count}\")\n",
    "    print(f\"所有子目录中 .glb 文件的总数: {total_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 指定要遍历的根文件夹路径\n",
    "    root_folder = ''\n",
    "    count_glb_files_in_subdirectories(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a50a6-509c-4c3a-a3b0-7da16105478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = object_uids\n",
    "len(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f51f77-1912-4dea-aff1-22e64325e729",
   "metadata": {},
   "source": [
    "查看Cap3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577a815-6dd0-454a-bf7a-89eb6908b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "captions = pd.read_csv('./Cap3D_automated_Objaverse_full.csv', header=None)\n",
    "text = captions[captions[0] == objs[1]][1].values[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8b891-4570-44bd-ac8e-d6e627527931",
   "metadata": {},
   "source": [
    "创建exported_data文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4431ced-7eb6-4f45-8e91-9d281287a73f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "export_folder = \"\"\n",
    "\n",
    "if not os.path.exists(export_folder):\n",
    "    os.makedirs(export_folder)\n",
    "\n",
    "export_file_path = os.path.join(export_folder, \"exported_data.json\")\n",
    "\n",
    "captions = pd.read_csv('./Cap3D_automated_Objaverse_full.csv', header=None)\n",
    "\n",
    "exported_data = []\n",
    "error_count = 1  \n",
    "for i in range(len(num)):\n",
    "    try:\n",
    "        text = captions[captions[0] == objs[i]][1].values[0]\n",
    "    except IndexError:\n",
    "        print(f\"[{error_count}] +++00000 IndexError occurred at index {i}, skipping this data.\")\n",
    "        error_count += 1  \n",
    "        continue  \n",
    "\n",
    "    print(str(i) + \" --- \" + text)\n",
    "    exported_data.append({\n",
    "        \"Object ID\": objs[i],\n",
    "        \"Name\": name[i],\n",
    "        \"Description\": description[i],\n",
    "        \"text\": text\n",
    "    })\n",
    "\n",
    "with open(export_file_path, 'w') as jsonfile:\n",
    "    json.dump(exported_data, jsonfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
