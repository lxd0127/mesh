{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd3597f-9fc7-43eb-90e4-132bdd1107c4",
   "metadata": {},
   "source": [
    "统计每条数据的token，找到最大token，计算每条数据平均token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277df615-de7e-4979-b931-2d72c7bdf9fb",
   "metadata": {},
   "source": [
    "### 统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da6215-2122-4b76-8c1c-d80d887e8ce8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 安装依赖包\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196388a-711b-48e4-9614-c33beecca5ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "def load_json_data(file_path: str) -> List[dict]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def calculate_field_tokens(\n",
    "    data: List[dict],\n",
    "    target_fields: Optional[List[str]] = None,\n",
    "    role_filter: Optional[List[str]] = None\n",
    ") -> Tuple[List[Dict[str, int]], List[dict]]:\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    results = []\n",
    "\n",
    "    for idx, entry in enumerate(data):\n",
    "        field_stats = {}\n",
    "        total_tokens = 0\n",
    "\n",
    "        # 处理目标字段中的普通字段（如uid、Name等）\n",
    "        for field_path in (target_fields or []):\n",
    "            values = _get_nested_values(entry, field_path)\n",
    "            for value in values:\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    text = json.dumps(value, ensure_ascii=False)\n",
    "                else:\n",
    "                    text = str(value)\n",
    "                tokens = enc.encode(text)\n",
    "                field_stats[field_path] = field_stats.get(field_path, 0) + len(tokens)\n",
    "                total_tokens += len(tokens)\n",
    "\n",
    "        # 显式处理messages数组中的角色内容\n",
    "        if \"messages\" in entry:\n",
    "            for msg in entry[\"messages\"]:\n",
    "                role = msg.get(\"role\")\n",
    "                # 应用角色过滤\n",
    "                if role_filter and role not in role_filter:\n",
    "                    continue\n",
    "                content = msg.get(\"content\", \"\")\n",
    "                tokens_content = enc.encode(content)\n",
    "                # 构造按角色分类的字段名\n",
    "                role_content_field = f\"messages.{role}.content\"\n",
    "                field_stats[role_content_field] = field_stats.get(role_content_field, 0) + len(tokens_content)\n",
    "                total_tokens += len(tokens_content)\n",
    "\n",
    "        # 记录结果\n",
    "        results.append({\n",
    "            \"index\": idx,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"field_breakdown\": field_stats\n",
    "        })\n",
    "        # print(f\"条目 {idx} 的token统计：{json.dumps(field_stats, ensure_ascii=False)}\")\n",
    "\n",
    "    return results, data\n",
    "\n",
    "def _get_nested_values(obj: dict, path: str) -> list:\n",
    "    parts = path.split('.')\n",
    "    results = [obj]\n",
    "    for part in parts:\n",
    "        new_results = []\n",
    "        for r in results:\n",
    "            if part == '*':\n",
    "                if isinstance(r, list):\n",
    "                    new_results.extend(r)\n",
    "            elif isinstance(r, dict):\n",
    "                if part in r:\n",
    "                    new_results.append(r[part])\n",
    "            elif isinstance(r, list):\n",
    "                for item in r:\n",
    "                    if isinstance(item, dict) and part in item:\n",
    "                        new_results.append(item[part])\n",
    "        results = new_results\n",
    "    return [item for item in results if item is not None]\n",
    "\n",
    "def analyze_results(results: List[Dict[str, int]], data: List[dict]):\n",
    "    # 汇总统计\n",
    "    summary = {\n",
    "        \"total\": {\n",
    "            \"max\": max(r[\"total_tokens\"] for r in results),\n",
    "            \"min\": min(r[\"total_tokens\"] for r in results),\n",
    "            \"avg\": sum(r[\"total_tokens\"] for r in results) / len(results)\n",
    "        },\n",
    "        \"fields\": {}\n",
    "    }\n",
    "\n",
    "    # 字段级统计\n",
    "    field_stats = {}\n",
    "    for r in results:\n",
    "        for field, count in r[\"field_breakdown\"].items():\n",
    "            if field not in field_stats:\n",
    "                field_stats[field] = []\n",
    "            field_stats[field].append(count)\n",
    "\n",
    "    for field, counts in field_stats.items():\n",
    "        summary[\"fields\"][field] = {\n",
    "            \"max\": max(counts),\n",
    "            \"min\": min(counts),\n",
    "            \"avg\": sum(counts) / len(counts)\n",
    "        }\n",
    "\n",
    "    # 输出结果\n",
    "    print(\"\\n全局统计：\")\n",
    "    print(f\"总token数 | 最大: {summary['total']['max']} 最小: {summary['total']['min']} 平均: {summary['total']['avg']:.1f}\")\n",
    "\n",
    "    print(\"\\n字段级统计：\")\n",
    "    for field, stats in summary[\"fields\"].items():\n",
    "        print(f\"{field.ljust(20)} | 最大: {stats['max']} 最小: {stats['min']} 平均: {stats['avg']:.1f}\")\n",
    "\n",
    "    # 输出最大条目详情\n",
    "    max_entry = max(results, key=lambda x: x[\"total_tokens\"])\n",
    "    print(f\"\\n最大token条目（索引 {max_entry['index']}，共 {max_entry['total_tokens']} tokens）\")\n",
    "    # print(json.dumps(data[max_entry[\"index\"]], indent=4, ensure_ascii=False))\n",
    "\n",
    "def main():\n",
    "    FILE_PATH = ''\n",
    "    TARGET_FIELDS = [\"uid\", \"Object_ID\", \"Name\"]  # 不再包含messages路径\n",
    "    # TARGET_FIELDS = []\n",
    "    ROLE_FILTER = [\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "    data = load_json_data(FILE_PATH)\n",
    "    results, data = calculate_field_tokens(\n",
    "        data,\n",
    "        target_fields=TARGET_FIELDS,\n",
    "        role_filter=ROLE_FILTER\n",
    "    )\n",
    "    analyze_results(results, data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45474fee-eb2f-4335-a1a9-7644a601e6ce",
   "metadata": {},
   "source": [
    "某一条数据的token长度超过给定的值，则删掉这条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d213cbf-180f-4a1d-9458-1db5c7a64904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "def load_json_data(file_path: str) -> List[dict]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json_data(data: List[dict], file_path: str):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def filter_entries_by_token_limit(\n",
    "    data: List[dict],\n",
    "    role_filter: Optional[List[str]] = None,\n",
    "    token_limit: int = 500\n",
    ") -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    过滤数据条目：删除指定角色 content 超过 token 限制的条目\n",
    "    返回 (保留的条目列表, 被删除的条目列表)\n",
    "    \"\"\"\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    kept_data = []\n",
    "    removed_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        should_remove = False\n",
    "\n",
    "        # 检查消息内容\n",
    "        if \"messages\" in entry:\n",
    "            for msg in entry[\"messages\"]:\n",
    "                role = msg.get(\"role\")\n",
    "                content = msg.get(\"content\", \"\")\n",
    "                \n",
    "                # 只处理指定角色\n",
    "                if role_filter and role not in role_filter:\n",
    "                    continue\n",
    "                \n",
    "                # 计算 token\n",
    "                tokens = enc.encode(content)\n",
    "                if len(tokens) > token_limit:\n",
    "                    should_remove = True\n",
    "                    break  # 发现超限条目，立即终止检查\n",
    "\n",
    "        # 分类存储结果\n",
    "        if should_remove:\n",
    "            removed_data.append(entry)\n",
    "        else:\n",
    "            kept_data.append(entry)\n",
    "\n",
    "    return kept_data, removed_data\n",
    "\n",
    "def calculate_field_tokens(\n",
    "    data: List[dict],\n",
    "    target_fields: Optional[List[str]] = None\n",
    ") -> List[Dict[str, int]]:\n",
    "    # 计算字段 token 数量\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    results = []\n",
    "\n",
    "    for idx, entry in enumerate(data):\n",
    "        field_stats = {}\n",
    "        total_tokens = 0\n",
    "\n",
    "        # 处理普通字段\n",
    "        for field_path in (target_fields or []):\n",
    "            values = _get_nested_values(entry, field_path)\n",
    "            for value in values:\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    text = json.dumps(value, ensure_ascii=False)\n",
    "                else:\n",
    "                    text = str(value)\n",
    "                tokens = enc.encode(text)\n",
    "                field_stats[field_path] = field_stats.get(field_path, 0) + len(tokens)\n",
    "                total_tokens += len(tokens)\n",
    "\n",
    "        # 处理消息内容\n",
    "        if \"messages\" in entry:\n",
    "            for msg in entry[\"messages\"]:\n",
    "                content = msg.get(\"content\", \"\")\n",
    "                tokens = enc.encode(content)\n",
    "                role = msg.get(\"role\")\n",
    "                role_field = f\"messages.{role}.content\"\n",
    "                field_stats[role_field] = field_stats.get(role_field, 0) + len(tokens)\n",
    "                total_tokens += len(tokens)\n",
    "\n",
    "        results.append({\n",
    "            \"index\": idx,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"field_breakdown\": field_stats\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "def _get_nested_values(obj: dict, path: str) -> list:\n",
    "    \"\"\"\n",
    "    使用点号语法获取嵌套字段值\n",
    "    支持通配符 * 表示遍历数组\n",
    "    \"\"\"\n",
    "    parts = path.split('.')\n",
    "    results = [obj]\n",
    "    \n",
    "    for part in parts:\n",
    "        new_results = []\n",
    "        for r in results:\n",
    "            if part == '*':\n",
    "                if isinstance(r, list):\n",
    "                    new_results.extend(r)\n",
    "            elif isinstance(r, dict):\n",
    "                if part in r:\n",
    "                    new_results.append(r[part])\n",
    "            elif isinstance(r, list):\n",
    "                for item in r:\n",
    "                    if isinstance(item, dict) and part in item:\n",
    "                        new_results.append(item[part])\n",
    "        results = new_results\n",
    "    \n",
    "    return [item for item in results if item is not None]\n",
    "\n",
    "def analyze_results(results: List[Dict[str, int]], data: List[dict]):\n",
    "    \"\"\"分析并输出详细统计结果\"\"\"\n",
    "    # 汇总统计\n",
    "    summary = {\n",
    "        \"total\": {\n",
    "            \"max\": max(r[\"total_tokens\"] for r in results),\n",
    "            \"min\": min(r[\"total_tokens\"] for r in results),\n",
    "            \"avg\": sum(r[\"total_tokens\"] for r in results) / len(results)\n",
    "        },\n",
    "        \"fields\": {}\n",
    "    }\n",
    "\n",
    "    # 字段级统计\n",
    "    field_stats = {}\n",
    "    for r in results:\n",
    "        for field, count in r[\"field_breakdown\"].items():\n",
    "            if field not in field_stats:\n",
    "                field_stats[field] = []\n",
    "            field_stats[field].append(count)\n",
    "\n",
    "    for field, counts in field_stats.items():\n",
    "        summary[\"fields\"][field] = {\n",
    "            \"max\": max(counts),\n",
    "            \"min\": min(counts),\n",
    "            \"avg\": sum(counts) / len(counts)\n",
    "        }\n",
    "\n",
    "    # 输出结果\n",
    "    print(\"\\n全局统计：\")\n",
    "    print(f\"总token数 | 最大: {summary['total']['max']} 最小: {summary['total']['min']} 平均: {summary['total']['avg']:.1f}\")\n",
    "\n",
    "    print(\"\\n字段级统计：\")\n",
    "    for field, stats in summary[\"fields\"].items():\n",
    "        print(f\"{field.ljust(20)} | 最大: {stats['max']} 最小: {stats['min']} 平均: {stats['avg']:.1f}\")\n",
    "\n",
    "    # 输出最大条目详情\n",
    "    if data:\n",
    "        max_entry = max(results, key=lambda x: x[\"total_tokens\"])\n",
    "        print(f\"\\n最大token条目（索引 {max_entry['index']}，共 {max_entry['total_tokens']} tokens）\")\n",
    "        # print(json.dumps(data[max_entry[\"index\"]], indent=4, ensure_ascii=False))\n",
    "\n",
    "def main():\n",
    "    # 配置参数\n",
    "    INPUT_FILE = ''\n",
    "    OUTPUT_FILE = ''\n",
    "    TARGET_FIELDS = [\"uid\", \"Object_ID\", \"Name\"]\n",
    "    ROLE_FILTER = [\"assistant\"]\n",
    "    TOKEN_LIMIT = 2000\n",
    "\n",
    "    # 处理流程\n",
    "    # 1. 加载原始数据\n",
    "    original_data = load_json_data(INPUT_FILE)\n",
    "    \n",
    "    # 2. 执行数据过滤\n",
    "    filtered_data, removed_data = filter_entries_by_token_limit(\n",
    "        original_data,\n",
    "        role_filter=ROLE_FILTER,\n",
    "        token_limit=TOKEN_LIMIT\n",
    "    )\n",
    "    \n",
    "    # 3. 保存过滤后的数据\n",
    "    save_json_data(filtered_data, OUTPUT_FILE)\n",
    "    \n",
    "    # 4. 输出过滤结果\n",
    "    print(f\"\\n=== 过滤结果 ===\")\n",
    "    print(f\"原始条目数: {len(original_data)}\")\n",
    "    print(f\"保留条目数: {len(filtered_data)}\")\n",
    "    print(f\"删除条目数: {len(removed_data)}\")\n",
    "    print(f\"已保存过滤后数据到: {OUTPUT_FILE}\")\n",
    "\n",
    "    # 5. 分析过滤后数据\n",
    "    if filtered_data:\n",
    "        print(\"\\n=== 过滤后数据统计 ===\")\n",
    "        results = calculate_field_tokens(filtered_data, TARGET_FIELDS)\n",
    "        analyze_results(results, filtered_data)\n",
    "    else:\n",
    "        print(\"\\n警告：过滤后数据为空！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
